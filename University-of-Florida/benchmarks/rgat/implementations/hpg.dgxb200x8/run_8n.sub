#!/bin/bash
#SBATCH --job-name=rgat-8n
#SBATCH --nodes=8
#SBATCH --gres=gpu:b200:8         # one task per GPU
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=14
#SBATCH --time=02:00:00
#SBATCH --mem=0
#SBATCH --exclusive

set -euo pipefail

# -----------------------------
# Apptainer
# -----------------------------
module purge || true
module load apptainer || true
APPTAINER_BIN="$(command -v apptainer || true)"
if [[ -z "${APPTAINER_BIN}" ]]; then
  echo "ERROR: apptainer not found in PATH." >&2
  exit 127
fi

# -----------------------------
# Paths (EDIT ME if needed)
# -----------------------------
: "${CONT:=./rgat-amd.sif}"

: "${SCRATCH:=./rgat}"
: "${DATA_PARENT:=${SCRATCH}/converted/full}"       # contains float8/ float16/
: "${GRAPH_PARENT:=${SCRATCH}/graph/full}"          # CSC arrays

: "${CONTAINER_DATA_DIR:=/data}"
: "${CONTAINER_GRAPH_DIR:=/graph}"

: "${WORK_DIR:=/workspace/gnn}"
: "${HOST_GNN_DIR:=./rgat/gnn}"

: "${LOGDIR:=${PWD}/results}"
: "${NEXP:=1}"
: "${MLPERF_RULESET:=5.1.0}"
: "${CHECK_COMPLIANCE:=1}"
: "${DATESTAMP:=$(date +'%y%m%d%H%M%S%N')}"
: "${CLEAR_CACHES:=1}"
: "${SEED_BASE:=${SEED-$RANDOM}}"
: "${DROPCACHE_CMD:=__auto__}"

umask 0002
mkdir -p "${LOGDIR}"
srun -N1 -n1 /bin/mkdir -p "${LOGDIR}"

# -----------------------------
# Cluster / dist derived
# -----------------------------
export MASTER_PORT=${MASTER_PORT:-29500}
export MASTER_ADDR="$(scontrol show hostnames "${SLURM_JOB_NODELIST}" | head -n1)"
export DGXNNODES="${SLURM_JOB_NUM_NODES:-1}"
export DGXNGPU=8
export MODEL_NAME="graph_neural_network"
export MODEL_FRAMEWORK="pytorch"
export SPREFIX="${MODEL_NAME}_${MODEL_FRAMEWORK}_${DGXNNODES}x${DGXNGPU}_${DATESTAMP}"
LOGBASE="${DATESTAMP}"

echo "PROLOG_START $(date +%s)"
echo "Using MASTER_ADDR=${MASTER_ADDR} MASTER_PORT=${MASTER_PORT}"

# -----------------------------
# Quick topology print (host view)
# -----------------------------
srun -N1 -n1 --export=ALL "${APPTAINER_BIN}" exec --nv \
  -B "${DATA_PARENT}:${CONTAINER_DATA_DIR}" \
  -B "${GRAPH_PARENT}:${CONTAINER_GRAPH_DIR}" \
  -B "${LOGDIR}:/results" \
  "${CONT}" /bin/bash -lc 'nvidia-smi topo -m || true'

echo "PROLOG_STOP $(date +%s)"

# ============================
# Experiments loop
# ============================
for _experiment_index in $(seq -w 1 "${NEXP}"); do
  echo "EXPERIMENT_START $(date +%s)"
  LOG_FILE_BASE="${LOGDIR}/${LOGBASE}"

  (
    echo "Beginning trial ${_experiment_index} of ${NEXP}"
    srun -N1 -n1 /bin/bash -lc 'echo -n "Clearing cache on " && hostname && sync || true'
    sleep 5
    
    ########################################
    # Per-trial seed + optional cache clear
    ########################################
    SEED=$(( SEED_BASE - 1 + 10#$_experiment_index ))
    export SEED

    if [[ "${CLEAR_CACHES}" -eq 1 ]]; then
      srun --ntasks-per-node=1 bash -lc 'echo -n "Clearing host page cache on "; hostname; sync; '"${DROPCACHE_CMD}"' || true'
      # MLPerf cache_clear event (inside container)
      "${APPTAINER_BIN}" exec --nv -B "${LOGDIR}:/results" \
        "${CONT}" /bin/bash -lc '
python - <<PY
from utility.logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)
PY
        '
    fi

    echo "RUNANDTIME_START $(date +%s)"

    # ------------------------------------------------------------------
    srun -l --mpi="${SLURM_MPI_TYPE:-pmix}" \
      --ntasks-per-node="${DGXNGPU}" \
      --gpus-per-node="${DGXNGPU}" \
      --export=ALL,CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
      "${APPTAINER_BIN}" exec --nv \
        -B "${DATA_PARENT}:${CONTAINER_DATA_DIR}" \
        -B "${GRAPH_PARENT}:${CONTAINER_GRAPH_DIR}" \
        -B "${HOST_GNN_DIR}:${WORK_DIR}" \
        -B "${LOGDIR}:/results" \
        --pwd "${WORK_DIR}" \
        "${CONT}" /bin/bash -lc '
          set -euo pipefail
          export SHELL=/bin/bash

          # ----- Distributed addrs -----
          export MASTER_ADDR='"${MASTER_ADDR}"'
          export MASTER_PORT='"${MASTER_PORT}"'
          export DGXNGPU='"${DGXNGPU}"'

          # ----- NCCL (IB only, per sanity run) -----
          # Keep logs concise by default; bump to INFO if debugging.
          export NCCL_DEBUG=${NCCL_DEBUG:-WARN}
          export NCCL_DEBUG_SUBSYS=${NCCL_DEBUG_SUBSYS:-INIT,ENV,GRAPH,NET}

          # Prefer IPoIB/HSN interfaces, not lo/docker/etc.
          export NCCL_SOCKET_IFNAME="${NCCL_SOCKET_IFNAME:-ib,hsn}"
          export NCCL_IB_DISABLE=0
          # IB typically uses GID index 0. (RoCEv2 would be 3; do NOT mix.)
          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-0}
          # Prevent cross-NIC merges that bit us before.
          export NCCL_NET_MERGE_LEVEL=${NCCL_NET_MERGE_LEVEL:-LOC}
          # Often helps on modern PCIe
          export NCCL_IB_PCI_RELAXED_ORDERING=${NCCL_IB_PCI_RELAXED_ORDERING:-1}
          # If you want to allow using multiple NICs on a node:
          export NCCL_CROSS_NIC=${NCCL_CROSS_NIC:-1}

          # Auto-pick ONLY InfiniBand HCAs (avoid mixing with RoCE)
          HCAS=""
          for h in /sys/class/infiniband/*; do
            [[ -d "$h" ]] || continue
            ll="$(cat "$h/ports/1/link_layer" 2>/dev/null || echo "")"
            name="$(basename "$h")"
            if [[ "$ll" == "InfiniBand" ]]; then
              HCAS+="${name},"
            fi
          done
          HCAS="${HCAS%,}"
          if [[ -n "${HCAS}" ]]; then
            export NCCL_IB_HCA="${HCAS}"
          else
            echo "WARN: No InfiniBand HCAs found by link_layer; leaving NCCL_IB_HCA unset."
          fi

          # ----- CUDA lib compatibility (container specific) -----
          export TRITON_LIBCUDA_PATH=/usr/local/cuda/compat/lib/libcuda.so.1
          export LD_LIBRARY_PATH=/usr/local/cuda/compat/lib:${LD_LIBRARY_PATH:-}

          # ----- RGAT training config -----
          cd "'"${WORK_DIR}"'"
          # Your tuned config (kept as-is)
          source config_DGXB200_8x8x1024.sh

          # (Optional) enforce bash for any invoked subscripts
          export BASH_ENV=/dev/null

          # MLPerf wrapper
          slurm2pytorch ./run_and_time.sh
        '

    echo "RUNANDTIME_STOP $(date +%s)"
  ) |& tee "${LOG_FILE_BASE}_${_experiment_index}.log"

  if [[ "${CHECK_COMPLIANCE}" -eq 1 ]]; then
    "${APPTAINER_BIN}" exec --nv -B "${LOGDIR}:/results" \
      "${CONT}" /bin/bash -lc '
        python3 -m mlperf_logging.compliance_checker --usage training \
          --ruleset '"${MLPERF_RULESET}"' \
          --log_output /results/compliance_'"${DATESTAMP}"'.out \
          /results/'"${LOGBASE}"'_'${_experiment_index}'.log || true
      '
    echo "Wrote compliance output to ${LOGDIR}/compliance_${DATESTAMP}.out"
  fi

  echo "EXPERIMENT_STOP $(date +%s)"
done

echo "All done. Logs in ${LOGDIR}"

