#!/bin/bash
#SBATCH --job-name=dlrm-1n
#SBATCH --nodes=1
#SBATCH --gres=gpu:b200:8 
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=112
#SBATCH --time=02:30:00

set -euo pipefail

module purge
module load cuda/12.9.1
module load apptainer/1.4.2

APPT="apptainer"
CONT="./dlrm_dcnv2-amd.sif"

WD_HOST="./workspace/dlrm"
DATA_HOST="./scratch/data/criteo_1tb_multihot_raw"
RES_HOST="${WD_HOST}/results"
mkdir -p "${RES_HOST}"

# ---------- Host-side cache drop (default is sudo sysctl; override if needed) ----------
: "${DROPCACHE_CMD:=sudo /sbin/sysctl vm.drop_caches=3}"
echo "HOST cache clear: ${DROPCACHE_CMD}"
( sync || true; eval "${DROPCACHE_CMD}" || echo "WARNING: cache drop command failed or not permitted" ) 2>&1 | sed 's/^/[host-cache]/'

# ---------- optional: stage val set to a fast local path ----------
pick_fast_dir() {
  local candidates=()
  [[ -n "${SLURM_TMPDIR:-}" ]] && candidates+=("${SLURM_TMPDIR}")
  candidates+=("/scratch/local/${USER}" "/scratch/${USER}")
  for d in "${candidates[@]}"; do
    mkdir -p "${d}" 2>/dev/null || true
    if [[ -w "${d}" ]]; then
      local free_gb
      free_gb=$(df -PB1G "${d}" | awk 'NR==2{print $4+0}')
      if [[ "${free_gb}" -ge 80 ]]; then
        echo "${d}"
        return 0
      fi
    fi
  done
  return 1
}

VAL_FAST=""
if fast_dir="$(pick_fast_dir)"; then
  VAL_FAST="${fast_dir}/dlrm_val_${SLURM_JOB_ID:-$$}"
  mkdir -p "${VAL_FAST}" || true
fi

VAL_SRC="${DATA_HOST}/val_data.bin"
if [[ -n "${VAL_FAST}" && -r "${VAL_SRC}" ]]; then
  if [[ ! -r "${VAL_FAST}/val_data.bin" || "$(stat -c%s "${VAL_FAST}/val_data.bin" 2>/dev/null || echo 0)" -ne "$(stat -c%s "${VAL_SRC}")" ]]; then
    echo "Staging val_data.bin to ${VAL_FAST}â€¦"
    cp -f "${VAL_SRC}" "${VAL_FAST}/val_data.bin" || { echo "Warning: staging failed; using Lustre for val_data." >&2; VAL_FAST=""; }
    sync || true
  fi
fi

VAL_MNT_SRC="${VAL_FAST:-${DATA_HOST}}"
echo "VAL_MNT_SRC=${VAL_MNT_SRC}"

BIND_MOUNTS="${DATA_HOST}:/data,${VAL_MNT_SRC}:/data_val,${WD_HOST}:/workspace,${RES_HOST}:/results"

echo "RUNANDTIME_START $(date +%s)"

# ---------- run inside the container ----------
srun -n 1 --mpi=none --export=ALL \
  ${APPT} exec --nv --cleanenv \
  --bind "${BIND_MOUNTS}" \
  --pwd /workspace \
  --env TRAIN_DATA="${TRAIN_DATA:-/data/train_data.bin}" \
  --env VAL_DATA="${VAL_DATA:-/data_val/val_data.bin}" \
  -- "${CONT}" /bin/bash -s <<'INSIDE'
set -euo pipefail

command -v numactl >/dev/null || { echo "ERROR: numactl missing in container"; exit 3; }

nvidia-smi -L || true
nvidia-smi topo -m || true

# ---------- env ----------
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
# remove MPI envs if present
for v in $(env | awk -F= '/^(OMPI_|PMI_|PMIX_)/ {print $1}'); do unset "$v" || true; done
export HUGECTR_USE_MPI=0
export HCTR_USE_MPI=0

# single-node NCCL/UCX
export NCCL_P2P_LEVEL=NVL
export NCCL_NVLS_ENABLE=1
export NCCL_SHM_DISABLE=0
export CUDA_DEVICE_MAX_CONNECTIONS=32
export NCCL_DEBUG=WARN
export NCCL_IB_DISABLE=1
export UCX_TLS=sm,self,cuda_copy,cuda_ipc
export UCX_IB_GPU_DIRECT_RDMA=n
unset UCX_NET_DEVICES
export NCCL_NET=Socket
export NCCL_SOCKET_IFNAME=lo
export NCCL_NET_MERGE_LEVEL=LOC

# Use a launcher wrapper so numactl never runs alone
launch() { numactl --cpunodebind=0-1 --membind=0-1 "$@"; }
echo "DLRM_BIND=numactl --cpunodebind=0-1 --membind=0-1"

export OMP_NUM_THREADS="${OMP_NUM_THREADS:-16}"
export OMP_PROC_BIND=close
export OMP_PLACES=cores

# Hyperparams aligned with your working path
export BATCHSIZE="${BATCHSIZE:-55296}"
export BATCHSIZE_EVAL="${BATCHSIZE_EVAL:-1048576}"
export LEARNING_RATE="${LEARNING_RATE:-0.004}"
export SCALER="${SCALER:-16348}"
export MEM_COMM_BW_RATIO="${MEM_COMM_BW_RATIO:-9}"
export SHARDING_PLAN="${SHARDING_PLAN:-auto}"
export DP_SHARDING_THRESHOLD="${DP_SHARDING_THRESHOLD:-0.008}"
export EVAL_INTERVAL="${EVAL_INTERVAL:-10000}"

# ---------- SEED: distinct per experiment/run (and visible to the logger) ----------
SEED_BASE=${SEED_BASE:-$RANDOM}
SEED=$(( (SEED_BASE + ${SLURM_PROCID:-0} + 10#${_experiment_index:-1}) % 32768 ))
export SEED
export MLPERF_SEED="${SEED}"         # some loggers check this
export PYTHONHASHSEED="${SEED}"      # improves determinism
echo "Using SEED_BASE=${SEED_BASE}  SLURM_PROCID=${SLURM_PROCID:-0}  _experiment_index=${_experiment_index:-1}  -> SEED=${SEED}"

# ---------- lightweight mpi4py shim for single-task runs ----------
mkdir -p /workspace/pyshim/mpi4py
cat >/workspace/pyshim/mpi4py/__init__.py <<'PY'
from typing import Any
class _Comm:
    def Get_size(self)->int: return 1
    def Get_rank(self)->int: return 0
    def allreduce(self, sendobj: Any, op: Any=None)->Any: return sendobj
    def Allreduce(self, sendbuf: Any, recvbuf: Any, op: Any=None)->None:
        try: recvbuf[...] = sendbuf
        except Exception:
            if hasattr(recvbuf, "set"): recvbuf.set(sendbuf)
            elif hasattr(sendbuf, "tobytes") and hasattr(recvbuf, "frombytes"): recvbuf.frombytes(sendbuf.tobytes())
    def Barrier(self)->None: return None
    def bcast(self, obj: Any, root: int=0)->Any: return obj
    def Bcast(self, buf: Any, root: int=0)->None: return None
class _MPI: COMM_WORLD = _Comm()
MPI = _MPI()
__all__ = ["MPI"]
PY
export PYTHONPATH="/workspace/pyshim:/usr/local/hugectr/python:/usr/local/hugectr/lib:${PYTHONPATH:-}"
export PYTHONNOUSERSITE=1

# ---------- run & capture (force train.py to avoid config drift) ----------
export DGXNGPU=8
export MODEL_NAME=DLRM_DCNv2
export FRAMEWORK=HugeCTR
export DGXSYSTEM=B200-8x
export RUN_SCRIPT=/workspace/train.py

RUN_STAMP="$(date +%y%m%d%H%M%S)"
RUN_JID="${SLURM_JOB_ID:-$$}"
RUN_ID="${RUN_STAMP}_${RUN_JID}"

# Keep a raw capture only to feed the compliance checker; training also streams to Slurm log via tee
export RAW_CAPTURE="/results/${RUN_ID}.log"

# Emit required MLPerf cache_clear event BEFORE training so compliance finds it
CACHE_LINE="$(python3 - <<'PY'
import json, time
print(":::MLLOG " + json.dumps({
  "namespace": "", "time_ms": int(time.time()*1000),
  "event_type": "POINT_IN_TIME", "key": "cache_clear",
  "value": True, "metadata": {}
}))
PY
)"
echo "${CACHE_LINE}" | tee -a "${RAW_CAPTURE}"

echo "START $(date +%F' '%T)"

# Direct train.py launch (now passing the seed explicitly)
launch python3 /workspace/train.py \
  --batchsize "${BATCHSIZE}" \
  --batchsize_eval "${BATCHSIZE_EVAL}" \
  --lr "${LEARNING_RATE}" \
  --use_mixed_precision \
  --scaler "${SCALER}" \
  --gen_loss_summary \
  --sharding_plan "${SHARDING_PLAN}" \
  --dp_sharding_threshold "${DP_SHARDING_THRESHOLD}" \
  --num_gpus_per_node 8 \
  --mem_comm_bw_ratio "${MEM_COMM_BW_RATIO}" \
  --eval_interval "${EVAL_INTERVAL}" \
  --train_data "${TRAIN_DATA}" \
  --val_data "${VAL_DATA}" \
  --seed "${SEED}" | tee -a "${RAW_CAPTURE}"

echo "STOP  $(date +%F' '%T)"

# ---------- Compliance checker -> STDOUT (lands in slurm-<jobid>.out) ----------
if python3 -c "import mlperf_logging" >/dev/null 2>&1; then
  echo
  echo "===== MLPerf Compliance Checker (ruleset ${MLPERF_RULESET:-5.0.0}) ====="
  python3 -m mlperf_logging.compliance_checker --usage training \
    --ruleset "${MLPERF_RULESET:-5.1.0}" \
    "${RAW_CAPTURE}" || true
  echo "===== End Compliance ====="
else
  echo "WARNING: mlperf_logging not found in container; skipping compliance."
fi
INSIDE

