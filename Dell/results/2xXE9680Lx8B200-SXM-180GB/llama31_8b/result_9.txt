+ echo 'Beginning trial 10 of 10'
Beginning trial 10 of 10
+ echo ':::DLPAL dockerd://mlperf-dell:llama31_8b 43 2 xe9680lx8b200-sxm-[6-7] '\''unknown'\'' 2xXE9680Lx8B200-SXM-280GB_2x8x2xtp1pp1cp1_8b'
:::DLPAL dockerd://mlperf-dell:llama31_8b 43 2 xe9680lx8b200-sxm-[6-7] 'unknown' 2xXE9680Lx8B200-SXM-280GB_2x8x2xtp1pp1cp1_8b
++ srun -N1 -n1 --container-name=llama31_8b_43 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"Dell","division":"closed","status":"Available on-premise","system_name":"2xXE9680Lx8B200-SXM-180GB","number_of_nodes":"2","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8562Y+","host_processor_core_count":"32","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"2.27.7","cublas_version":"13.0.2.14","cudnn_version":"9.13.1.26","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-153-generic","nvidia_kernel_driver":"570.172.08"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Dell","division":"closed","status":"Available on-premise","system_name":"2xXE9680Lx8B200-SXM-180GB","number_of_nodes":"2","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8562Y+","host_processor_core_count":"32","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"2.27.7","cublas_version":"13.0.2.14","cudnn_version":"9.13.1.26","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-153-generic","nvidia_kernel_driver":"570.172.08"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=llama31_8b_43 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID  
+ export SEED=24089
+ SEED=24089
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c '
                 host=$(hostname)
                 echo "$host sync_start"
                 sync && echo "$host sync_done"
                 cache_before=$(awk "/^Cached:/ {print \$2}" /proc/meminfo)
                 sudo /sbin/sysctl vm.drop_caches=3
                 cache_after=$(awk "/^Cached:/ {print \$2}" /proc/meminfo)
                 echo "$host cache_cleared ${cache_before}kB to ${cache_after}kB"
            '
xe9680lx8b200-sxm-7 sync_start
xe9680lx8b200-sxm-7 sync_done
xe9680lx8b200-sxm-6 sync_start
xe9680lx8b200-sxm-6 sync_done
vm.drop_caches = 3
xe9680lx8b200-sxm-7 cache_cleared 92916828kB to 2127768kB
vm.drop_caches = 3
xe9680lx8b200-sxm-6 cache_cleared 95030316kB to 3928536kB
+ srun --ntasks-per-node=1 --container-name=llama31_8b_43 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_common.callbacks import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
:::MLLOG {"namespace": "", "time_ms": 1760020900821, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760020901508, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1760020902'
RUNANDTIME_START 1760020902
+ SLURM_HOSTFILE=/results_llama2-70b/hostfile.43.EaBD
+ NV_MLPERF_DEBUG=1
+ srun -l --mpi=pmix --ntasks-per-node=8 --distribution=arbitrary --time=140 --container-name=llama31_8b_43 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/results_llama2-70b:/results,/results_llama2-70b/251009045106045321477_npy_index:/npy_index,/results_llama2-70b/mem_dump:/mem_dump,/training_datasets_v5.1/training_datasets_v4.1/llama3//8b/tokenizer:/workspace/llm/nemo_tokenizer:ro,/training_datasets_v5.1/training_datasets_v4.1/llama3//8b:/preproc_data:ro --container-workdir=/workspace/llm --container-env=MASTER_PORT,MASTER_ADDR,NCCL_SHARP_GROUP_SIZE_THRESH,NCCL_NVLS_ENABLE slurm2pytorch ./run_and_time.sh
 9: LOAD_CHECKPOINT=
14: LOAD_CHECKPOINT=
12: LOAD_CHECKPOINT=
11: LOAD_CHECKPOINT=
 8: LOAD_CHECKPOINT=
 8: Hello from: xe9680lx8b200-sxm-7
10: LOAD_CHECKPOINT=
15: LOAD_CHECKPOINT=
13: LOAD_CHECKPOINT=
 0: slurm2pytorch: MASTER_ADDR=xe9680lx8b200-sxm-6 MASTER_PORT=29500 WORLD_SIZE=16
 6: LOAD_CHECKPOINT=
 7: LOAD_CHECKPOINT=
 5: LOAD_CHECKPOINT=
 4: LOAD_CHECKPOINT=
 2: LOAD_CHECKPOINT=
 3: LOAD_CHECKPOINT=
 0: LOAD_CHECKPOINT=
 1: LOAD_CHECKPOINT=
 0: Hello from: xe9680lx8b200-sxm-6
 0: running LLM benchmark
 0: Extra args:  exp_manager.explicit_log_dir="/results/251009045106045321477"
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
14: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
14:   import pynvml  # type: ignore[import]
 9: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 9:   import pynvml  # type: ignore[import]
11: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
11:   import pynvml  # type: ignore[import]
 8: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 8:   import pynvml  # type: ignore[import]
13: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
13:   import pynvml  # type: ignore[import]
10: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
10:   import pynvml  # type: ignore[import]
12: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
12:   import pynvml  # type: ignore[import]
15: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
15:   import pynvml  # type: ignore[import]
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 6: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 6:   import pynvml  # type: ignore[import]
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=32
 4: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 4:   import pynvml  # type: ignore[import]
 0: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 0:   import pynvml  # type: ignore[import]
 1: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 1:   import pynvml  # type: ignore[import]
 3: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 3:   import pynvml  # type: ignore[import]
 2: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 2:   import pynvml  # type: ignore[import]
 5: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 5:   import pynvml  # type: ignore[import]
 7: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 7:   import pynvml  # type: ignore[import]
 6: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 0: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 4: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 7: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 5: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 2: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 1: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 3: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
13: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
15: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 8: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 9: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
11: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
14: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
10: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
12: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 0: :::MLLOG {"namespace": "", "time_ms": 1760020924676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 750}}
 0: [NeMo I 2025-10-09 14:42:04 nemo_logging:393] 
 0:     
 0:     **************** Experiment configuration ****************
 0: [NeMo I 2025-10-09 14:42:04 nemo_logging:393] 
 0:     model:
 0:       data:
 0:         data_prefix:
 0:           train:
 0:           - 0.5
 0:           - /preproc_data/c4_en_6_c4_spm_text_document
 0:           - 0.5
 0:           - /preproc_data/c4_en_7_c4_spm_text_document
 0:           validation:
 0:           - /preproc_data/c4_en_validation_subset_c4_spm_text_document
 0:           test:
 0:           - /preproc_data/c4_en_validation_subset_c4_spm_text_document
 0:         index_mapping_dir: /npy_index
 0:         splits_string: null
 0:         validation_drop_last: false
 0:         pad_samples_to_global_batch_size: true
 0:         shuffle_documents: false
 0:         legacy_dataset: true
 0:         delay_data_init: true
 0:         delay_data_mmap: true
 0:         no_seqlen_plus_one_input_tokens: true
 0:         exchange_indices_distributed: true
 0:         mock_dataset: false
 0:         mock_tokenizer_vocab_size: 32000
 0:       mcore_gpt: true
 0:       name: megatron_gpt_full_te_layer_autocast
 0:       micro_batch_size: 2
 0:       tensor_model_parallel_size: 1
 0:       pipeline_model_parallel_size: 1
 0:       virtual_pipeline_model_parallel_size: null
 0:       context_parallel_size: 1
 0:       expert_model_parallel_size: 1
 0:       global_batch_size: 32
 0:       use_tp_pp_dp_mapping: true
 0:       base_config: 8b
 0:       overwritten_attributes:
 0:         num_layers: 32
 0:         enable_cuda_graph: 1
 0:         cuda_graph_scope: full_iteration
 0:       encoder_seq_length: 8192
 0:       overlap_p2p_comm: true
 0:       batch_p2p_comm: false
 0:       account_for_embedding_in_pipeline_split: false
 0:       account_for_loss_in_pipeline_split: false
 0:       external_cuda_graph: false
 0:       defer_embedding_wgrad_compute: false
 0:       wgrad_deferral_limit: 50
 0:       tokenizer:
 0:         model: /workspace/llm/nemo_tokenizer
 0:       gradient_accumulation_fusion: true
 0:       fused_single_qkv_rope: true
 0:       cross_entropy_loss_fusion: true
 0:       deterministic_mode: false
 0:       seed: 24089
 0:       resume_from_checkpoint: null
 0:       dist_ckpt_format: torch_dist
 0:       dist_ckpt_parallel_load: true
 0:       sync_batch_comm: false
 0:       activations_checkpoint_granularity: null
 0:       activations_checkpoint_method: null
 0:       activations_checkpoint_num_layers: null
 0:       sequence_parallel: false
 0:       transformer_engine: true
 0:       fp8: true
 0:       fp8_hybrid: true
 0:       fp8_recipe: tensorwise
 0:       fp8_amax_history_len: 1
 0:       fp8_amax_compute_algo: most_recent
 0:       fp4: false
 0:       fp4_recipe: nvfp4
 0:       reduce_amax: true
 0:       tp_only_amax_red: true
 0:       first_last_layers_bf16: false
 0:       num_layers_at_start_in_bf16: 0
 0:       num_layers_at_end_in_bf16: 0
 0:       fp8_dot_product_attention: false
 0:       use_te_rng_tracker: true
 0:       use_transformer_engine_op_fuser: true
 0:       cross_entropy_fusion_impl: te
 0:       ub_tp_comm_overlap: false
 0:       tp_comm_overlap_ag: true
 0:       tp_comm_overlap_rs: true
 0:       nccl_communicator_config_path: /workspace/llm/conf/nccl/custom_communicator_cta.yaml
 0:       sharp: false
 0:       optim:
 0:         overlap_grad_reduce: true
 0:         overlap_param_gather: true
 0:         align_param_gather: false
 0:         use_distributed_optimizer: true
 0:         bucket_size: 768000000
 0:         fp8_param_gather: true
 0:         overlap_param_gather_with_optim_step: false
 0:         lr: 0.0004
 0:         sched:
 0:           min_lr: 4.0e-05
 0:           warmup_steps: 16
 0:           max_steps_for_lr_sched: 43200000.0
 0:         lock_timeout: null
 0:       gc_interval_train: 10000
 0:       gc_interval_valid: 10000
 0:       nsys_profile:
 0:         enabled: false
 0:         start_step: 10
 0:         end_step: 10
 0:         ranks:
 0:         - 0
 0:         gen_shape: false
 0:         nvtx_ranges: false
 0:       custom:
 0:         log_metrics: NEMO
 0:         init_global_step: 0
 0:         target_log_ppl: 3.3
 0:         use_distributed_checkpointing: 1
 0:         run_warmup_on_synth_data: 1
 0:         reset_fp8_stats_after_warmup: 1
 0:         pre_validate: 0
 0:         override_zero_consumed_samples: 1
 0:         force_success_status: 0
 0:         warmup_train_steps: 2
 0:         warmup_validation_steps: 2
 0:         extend_run_evals: 0
 0:         disable_nemo_logs: true
 0:     proxy_gbs: 32
 0:     is_proxy_run: false
 0:     skip_evals: 12
 0:     default_val_check_interval: 384
 0:     trainer:
 0:       devices: 8
 0:       num_nodes: 2
 0:       precision: bf16
 0:       max_steps: 1200000
 0:       max_epochs: 1
 0:       log_every_n_steps: 32
 0:       val_check_interval: 768
 0:       limit_val_batches: 32
 0:       limit_test_batches: 1
 0:       limit_train_batches: null
 0:       enable_progress_bar: false
 0:       num_sanity_val_steps: 0
 0:     exp_manager:
 0:       explicit_log_dir: /results/251009045106045321477
 0:       resume_if_exists: 0
 0:       create_checkpoint_callback: 0
 0:       checkpoint_callback_params:
 0:         save_top_k: 1
 0:         mode: max
 0:         every_n_epochs: 0
 0:         save_last: true
 0:       log_step_timing: true
 0:       create_tensorboard_logger: false
 0:       log_global_rank_0_only: true
 0:     misc:
 0:       print_config: false
 0:       memory_profiler:
 0:         enable: false
 0:         file_prefix: memdump
 0:         max_entries: 1000000
 0:         rank_0_only: true
 0:         start_location: init
 0:         end_location: train_start
 0:         force_oom_before_stop: false
 0:         possible_oom: false
 0:     
 0: [NeMo I 2025-10-09 14:42:04 nemo_logging:393] 
 0:     TP: 1; PP: 1; VP: None; CP: 1
 0: [NeMo I 2025-10-09 14:42:04 nemo_logging:393] ======== Benchmarked setups ========
 6: [W1009 14:42:05.452938875 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.453457686 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.453779249 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.454123346 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.454431377 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.454766818 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.455079472 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.455398588 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.455704806 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 6: [W1009 14:42:05.456025831 socket.cpp:755] [c10d] The client socket has failed to connect to [xe9680lx8b200-sxm-6]:29500 (errno: 22 - Invalid argument).
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: HPU available: False, using: 0 HPUs
 0: `Trainer(limit_test_batches=1)` was configured so 1 batch will be used.
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama31_8b", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Dell", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xXE9680Lx8B200-SXM-180GB", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "seed", "value": 24089, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 32, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925515, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1.0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 8192, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 1024, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1574207408, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "init_checkpoint_step", "value": 0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0004, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925516, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.95, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-05, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.1, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 1.0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_end_learning_rate", "value": 4e-05, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 16, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925517, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 1199984, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925518, "event_type": "POINT_IN_TIME", "key": "max_steps", "value": 1200000, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925518, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_schedule", "value": "cosine with linear warmup", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760020925518, "event_type": "POINT_IN_TIME", "key": "target_accuracy", "value": 3.3, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] ======== Benchmarked fit ========
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Experiments will be logged at /workspace/llm/nemo_experiments/default/2025-10-09_14-42-05
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has context parallel group: [0]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has model parallel group: [0]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has embedding group: [0]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]]
 0: [NeMo I 2025-10-09 14:42:05 nemo_logging:393] Rank 0 has embedding rank: 0
 0: [AUX I 2025-10-09 14:42:05 megatron_strategy:607] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/16
 4: [W1009 14:42:05.790479613 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 7: [W1009 14:42:05.000800698 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 5: [W1009 14:42:05.010845605 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 2: [W1009 14:42:05.092241902 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 1: [W1009 14:42:05.142833423 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 6: [W1009 14:42:05.181737437 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 3: [W1009 14:42:05.191721831 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
13: [W1009 14:42:06.281973646 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
15: [W1009 14:42:06.281976390 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
14: [W1009 14:42:06.284025682 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
12: [W1009 14:42:06.286446275 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 8: [W1009 14:42:06.290675094 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
11: [W1009 14:42:06.291827092 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 9: [W1009 14:42:06.292467023 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
10: [W1009 14:42:06.297414656 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 0: [W1009 14:42:06.405393892 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 16 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: [Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 1: [Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 2: [Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 5: [Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 7: [Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 3: [Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 4: [Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 6: [Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
10: [Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 8: [Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 9: [Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
12: [Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
11: [Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
13: [Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
15: [Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
14: [Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 2: [Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 0: [Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 1: [Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 4: [Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 5: [Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 3: [Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 7: [Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 6: [Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
14: [Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 8: [Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 9: [Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
11: [Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
10: [Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
13: [Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
12: [Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
15: [Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 0: [Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 1: [Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 3: [Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 5: [Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 2: [Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 4: [Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 6: [Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 7: [Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
11: [Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
15: [Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 9: [Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 8: [Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
10: [Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
12: [Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
13: [Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
14: [Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
 0: [NeMo I 2025-10-09 14:42:09 utils:662] Building GPTDataset splits with sizes=[38400000, 1600512, 32] and config=GPTDatasetConfig(random_seed=24089, sequence_length=8192, blend=None, blend_per_split=[(['/preproc_data/c4-train.en_6_text_document'], [50.0]), (['/preproc_data/c4-validation-91205-samples.en_text_document'], None), (['/preproc_data/c4-validation-91205-samples.en_text_document'], None)], multiple_validation_sets=None, full_validation=None, split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache='/npy_index', mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7f2bb92dec30>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
 0: [NeMo I 2025-10-09 14:42:09 utils:662] Load the _IndexReader from /preproc_data/c4-train.en_6_text_document.idx
 0: [NeMo I 2025-10-09 14:42:09 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 14:42:09 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 14:42:09 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 14:42:09 utils:662] > total number of sequences: 45608611
 0: [NeMo I 2025-10-09 14:42:09 utils:662] > total number of documents: 45608611
 0: [NeMo I 2025-10-09 14:42:09 utils:662] Build and save the GPTDataset train indices
 0: [NeMo I 2025-10-09 14:43:22 utils:662] > total number of samples: 38441516
 0: [NeMo I 2025-10-09 14:43:22 utils:662] > total number of epochs: 15
 0: [NeMo I 2025-10-09 14:43:22 utils:662] Load the _IndexReader from /preproc_data/c4-validation-91205-samples.en_text_document.idx
 0: [NeMo I 2025-10-09 14:43:22 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 14:43:22 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 14:43:22 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 14:43:22 utils:662] > total number of sequences: 91205
 0: [NeMo I 2025-10-09 14:43:22 utils:662] > total number of documents: 91205
 0: [NeMo I 2025-10-09 14:43:22 utils:662] Build and save the GPTDataset valid indices
 0: [NeMo I 2025-10-09 14:43:24 utils:662] > total number of samples: 1601227
 0: [NeMo I 2025-10-09 14:43:24 utils:662] > total number of epochs: 315
 0: [NeMo I 2025-10-09 14:43:24 utils:662] Load the _IndexReader from /preproc_data/c4-validation-91205-samples.en_text_document.idx
 0: [NeMo I 2025-10-09 14:43:24 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 14:43:24 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 14:43:24 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 14:43:24 utils:662] > total number of sequences: 91205
 0: [NeMo I 2025-10-09 14:43:24 utils:662] > total number of documents: 91205
 0: [NeMo I 2025-10-09 14:43:24 utils:662] Build and save the GPTDataset test indices
 0: [NeMo I 2025-10-09 14:43:24 utils:662] > total number of samples: 5083
 0: [NeMo I 2025-10-09 14:43:24 utils:662] > total number of epochs: 1
 0: [NeMo W 2025-10-09 14:43:24 nemo_logging:405] Recommend unsetting CUDA_DEVICE_MAX_CONNECTIONS for best performance                         but get 1
 0: [NeMo I 2025-10-09 14:43:24 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-10-09 14:43:31 nemo_logging:393] Apply rope scaling with factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, old_context_len=8192.
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-10-09 14:43:31 nemo_logging:393] Copying Trainer's 'max_steps' (1200000) to LR scheduler's 'max_steps'.
 0: [NeMo I 2025-10-09 14:43:31 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: [NeMo I 2025-10-09 14:43:31 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 8030261248
 0: [NeMo I 2025-10-09 14:43:31 utils:662] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=768000000, pad_buckets_for_high_nccl_busbw=False, average_in_collective=True, fp8_param_gather=True, reuse_grad_buf_for_mxfp8_param_ag=False, use_megatron_fsdp=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache=False, nccl_ub=False, fsdp_double_buffer=False, outer_dp_sharding_strategy='no_shard', disable_symmetric_registration=False, delay_wgrad_compute=False)
 0: [NeMo I 2025-10-09 14:43:31 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 1
 0:     Params for bucket 1 (1050939392 elements, 1050939392 padded size):
 0:     	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
 0:     	module.embedding.word_embeddings.weight
 0:     	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.final_layernorm.weight
 0:     	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
 0:     	module.output_layer.weight
 0:     	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
 0: [NeMo I 2025-10-09 14:43:31 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 9
 0:     Params for bucket 1 (830472192 elements, 830472192 padded size):
 0:     	module.decoder.layers.31.mlp.linear_fc2.weight
 0:     	module.decoder.layers.30.mlp.linear_fc2.weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.31.mlp.linear_fc1.weight
 0:     	module.decoder.layers.30.mlp.linear_fc1.weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.29.mlp.linear_fc2.weight
 0:     	module.decoder.layers.29.mlp.linear_fc1.weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.28.mlp.linear_fc2.weight
 0:     	module.decoder.layers.28.mlp.linear_fc1.weight
 0:     	module.decoder.layers.31.self_attention.linear_proj.weight
 0:     	module.decoder.layers.30.self_attention.linear_proj.weight
 0:     	module.decoder.layers.29.self_attention.linear_proj.weight
 0:     Params for bucket 2 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.28.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.27.mlp.linear_fc1.weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.27.mlp.linear_fc2.weight
 0:     	module.decoder.layers.26.mlp.linear_fc2.weight
 0:     	module.decoder.layers.26.mlp.linear_fc1.weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.25.mlp.linear_fc2.weight
 0:     	module.decoder.layers.25.mlp.linear_fc1.weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.24.mlp.linear_fc2.weight
 0:     	module.decoder.layers.28.self_attention.linear_proj.weight
 0:     	module.decoder.layers.27.self_attention.linear_proj.weight
 0:     	module.decoder.layers.26.self_attention.linear_proj.weight
 0:     	module.decoder.layers.25.self_attention.linear_proj.weight
 0:     	module.decoder.layers.24.mlp.linear_fc1.weight
 0:     Params for bucket 3 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.24.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.23.mlp.linear_fc1.weight
 0:     	module.decoder.layers.23.mlp.linear_fc2.weight
 0:     	module.decoder.layers.22.mlp.linear_fc2.weight
 0:     	module.decoder.layers.22.mlp.linear_fc1.weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.21.mlp.linear_fc2.weight
 0:     	module.decoder.layers.21.mlp.linear_fc1.weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.20.mlp.linear_fc1.weight
 0:     	module.decoder.layers.24.self_attention.linear_proj.weight
 0:     	module.decoder.layers.23.self_attention.linear_proj.weight
 0:     	module.decoder.layers.22.self_attention.linear_proj.weight
 0:     	module.decoder.layers.21.self_attention.linear_proj.weight
 0:     	module.decoder.layers.20.mlp.linear_fc2.weight
 0:     Params for bucket 4 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.20.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.19.mlp.linear_fc1.weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.19.mlp.linear_fc2.weight
 0:     	module.decoder.layers.18.mlp.linear_fc2.weight
 0:     	module.decoder.layers.18.mlp.linear_fc1.weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.17.mlp.linear_fc2.weight
 0:     	module.decoder.layers.17.mlp.linear_fc1.weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.16.mlp.linear_fc2.weight
 0:     	module.decoder.layers.16.mlp.linear_fc1.weight
 0:     	module.decoder.layers.20.self_attention.linear_proj.weight
 0:     	module.decoder.layers.19.self_attention.linear_proj.weight
 0:     	module.decoder.layers.18.self_attention.linear_proj.weight
 0:     	module.decoder.layers.17.self_attention.linear_proj.weight
 0:     Params for bucket 5 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.16.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.15.mlp.linear_fc2.weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.15.mlp.linear_fc1.weight
 0:     	module.decoder.layers.14.mlp.linear_fc2.weight
 0:     	module.decoder.layers.14.mlp.linear_fc1.weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.13.mlp.linear_fc2.weight
 0:     	module.decoder.layers.13.mlp.linear_fc1.weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.16.self_attention.linear_proj.weight
 0:     	module.decoder.layers.15.self_attention.linear_proj.weight
 0:     	module.decoder.layers.14.self_attention.linear_proj.weight
 0:     	module.decoder.layers.13.self_attention.linear_proj.weight
 0:     	module.decoder.layers.12.mlp.linear_fc1.weight
 0:     	module.decoder.layers.12.mlp.linear_fc2.weight
 0:     Params for bucket 6 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.12.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.11.mlp.linear_fc2.weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.11.mlp.linear_fc1.weight
 0:     	module.decoder.layers.10.mlp.linear_fc2.weight
 0:     	module.decoder.layers.10.mlp.linear_fc1.weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.9.mlp.linear_fc2.weight
 0:     	module.decoder.layers.9.mlp.linear_fc1.weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.8.mlp.linear_fc2.weight
 0:     	module.decoder.layers.8.mlp.linear_fc1.weight
 0:     	module.decoder.layers.12.self_attention.linear_proj.weight
 0:     	module.decoder.layers.11.self_attention.linear_proj.weight
 0:     	module.decoder.layers.10.self_attention.linear_proj.weight
 0:     	module.decoder.layers.9.self_attention.linear_proj.weight
 0:     Params for bucket 7 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.8.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.7.mlp.linear_fc1.weight
 0:     	module.decoder.layers.7.mlp.linear_fc2.weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.6.mlp.linear_fc2.weight
 0:     	module.decoder.layers.6.mlp.linear_fc1.weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.5.mlp.linear_fc2.weight
 0:     	module.decoder.layers.5.mlp.linear_fc1.weight
 0:     	module.decoder.layers.4.mlp.linear_fc2.weight
 0:     	module.decoder.layers.8.self_attention.linear_proj.weight
 0:     	module.decoder.layers.7.self_attention.linear_proj.weight
 0:     	module.decoder.layers.6.self_attention.linear_proj.weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.5.self_attention.linear_proj.weight
 0:     	module.decoder.layers.4.mlp.linear_fc1.weight
 0:     Params for bucket 8 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.4.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.3.mlp.linear_fc1.weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.3.mlp.linear_fc2.weight
 0:     	module.decoder.layers.2.mlp.linear_fc2.weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.1.self_attention.linear_proj.weight
 0:     	module.decoder.layers.4.self_attention.linear_proj.weight
 0:     	module.decoder.layers.3.self_attention.linear_proj.weight
 0:     	module.decoder.layers.2.mlp.linear_fc1.weight
 0:     	module.decoder.layers.2.self_attention.linear_proj.weight
 0:     	module.decoder.layers.1.mlp.linear_fc2.weight
 0:     	module.decoder.layers.1.mlp.linear_fc1.weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.0.mlp.linear_fc2.weight
 0:     	module.decoder.layers.0.mlp.linear_fc1.weight
 0:     Params for bucket 9 (41943040 elements, 41943040 padded size):
 0:     	module.decoder.layers.0.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.0.self_attention.linear_proj.weight
 0: [NeMo I 2025-10-09 14:43:31 utils:662] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0004, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='tensorwise', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_ti
 0: me=False, timers=None, config_logger_dir='')
 0: 
 0:   | Name   | Type | Params | Mode 
 0: ----------------------------------------
 0: 0 | module | DDP  | 8.0 B  | train
 0: ----------------------------------------
 0: 8.0 B     Trainable params
 0: 0         Non-trainable params
 0: 8.0 B     Total params
 0: 32,121.045Total estimated model params size (MB)
 0: 651       Modules in train mode
 0: 0         Modules in eval mode
 0: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [AUX I 2025-10-09 14:43:32 data:291] Instantiating MegatronPretrainingSampler with total_samples: 38441516 and consumed_samples: 0
 0: [AUX I 2025-10-09 14:43:33 data:291] Instantiating MegatronPretrainingSampler with total_samples: 1601227 and consumed_samples: 0
 0: GPTModel(
 0:   (embedding): LanguageModelEmbedding(
 0:     (word_embeddings): VocabParallelEmbedding()
 0:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 0:   )
 0:   (rotary_pos_emb): RotaryEmbedding()
 0:   (decoder): TransformerBlock(
 0:     (layers): ModuleList(
 0:       (0-31): 32 x TransformerLayer(
 0:         (input_layernorm): IdentityOp()
 0:         (self_attention): SelfAttention(
 0:           (core_attention): TEDotProductAttention(
 0:             (flash_attention): FlashAttention()
 0:             (fused_attention): FusedAttention()
 0:             (unfused_attention): UnfusedDotProductAttention(
 0:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 0:               (attention_dropout): Dropout(p=0.0, inplace=False)
 0:             )
 0:           )
 0:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 0:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 0:           (q_layernorm): IdentityOp()
 0:           (k_layernorm): IdentityOp()
 0:         )
 0:         (pre_cross_attn_layernorm): IdentityOp()
 0:         (cross_attention): IdentityOp()
 0:         (cross_attn_bda): IdentityFuncOp()
 0:         (pre_mlp_layernorm): IdentityOp()
 0:         (mlp): TEFusedMLP(
 0:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 0:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 0:         )
 0:       )
 0:     )
 0:     (final_layernorm): RMSNorm()
 0:   )
 0:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 7: GPTModel(
 7:   (embedding): LanguageModelEmbedding(
 7:     (word_embeddings): VocabParallelEmbedding()
 7:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 7:   )
 7:   (rotary_pos_emb): RotaryEmbedding()
 7:   (decoder): TransformerBlock(
 7:     (layers): ModuleList(
 7:       (0-31): 32 x TransformerLayer(
 7:         (input_layernorm): IdentityOp()
 7:         (self_attention): SelfAttention(
 7:           (core_attention): TEDotProductAttention(
 7:             (flash_attention): FlashAttention()
 7:             (fused_attention): FusedAttention()
 7:             (unfused_attention): UnfusedDotProductAttention(
 7:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 7:               (attention_dropout): Dropout(p=0.0, inplace=False)
 7:             )
 7:           )
 7:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 7:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 7:           (q_layernorm): IdentityOp()
 7:           (k_layernorm): IdentityOp()
 7:         )
 7:         (pre_cross_attn_layernorm): IdentityOp()
 7:         (cross_attention): IdentityOp()
 7:         (cross_attn_bda): IdentityFuncOp()
 7:         (pre_mlp_layernorm): IdentityOp()
 7:         (mlp): TEFusedMLP(
 7:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 7:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 7:         )
 7:       )
 7:     )
 7:     (final_layernorm): RMSNorm()
 7:   )
 7:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 7: )
 5: GPTModel(
 5:   (embedding): LanguageModelEmbedding(
 5:     (word_embeddings): VocabParallelEmbedding()
 5:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 5:   )
 5:   (rotary_pos_emb): RotaryEmbedding()
 5:   (decoder): TransformerBlock(
 5:     (layers): ModuleList(
 5:       (0-31): 32 x TransformerLayer(
 5:         (input_layernorm): IdentityOp()
 5:         (self_attention): SelfAttention(
 5:           (core_attention): TEDotProductAttention(
 5:             (flash_attention): FlashAttention()
 5:             (fused_attention): FusedAttention()
 5:             (unfused_attention): UnfusedDotProductAttention(
 5:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 5:               (attention_dropout): Dropout(p=0.0, inplace=False)
 5:             )
 5:           )
 5:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 5:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 5:           (q_layernorm): IdentityOp()
 5:           (k_layernorm): IdentityOp()
 5:         )
 5:         (pre_cross_attn_layernorm): IdentityOp()
 5:         (cross_attention): IdentityOp()
 5:         (cross_attn_bda): IdentityFuncOp()
 5:         (pre_mlp_layernorm): IdentityOp()
 5:         (mlp): TEFusedMLP(
 5:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 5:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 5:         )
 5:       )
 5:     )
 5:     (final_layernorm): RMSNorm()
 5:   )
 5:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 2: GPTModel(
 2:   (embedding): LanguageModelEmbedding(
 2:     (word_embeddings): VocabParallelEmbedding()
 2:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 2:   )
 2:   (rotary_pos_emb): RotaryEmbedding()
 2:   (decoder): TransformerBlock(
 2:     (layers): ModuleList(
 2:       (0-31): 32 x TransformerLayer(
 2:         (input_layernorm): IdentityOp()
 2:         (self_attention): SelfAttention(
 2:           (core_attention): TEDotProductAttention(
 2:             (flash_attention): FlashAttention()
 2:             (fused_attention): FusedAttention()
 2:             (unfused_attention): UnfusedDotProductAttention(
 2:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 2:               (attention_dropout): Dropout(p=0.0, inplace=False)
 2:             )
 2:           )
 2:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 2:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 2:           (q_layernorm): IdentityOp()
 2:           (k_layernorm): IdentityOp()
 2:         )
 2:         (pre_cross_attn_layernorm): IdentityOp()
 2:         (cross_attention): IdentityOp()
 2:         (cross_attn_bda): IdentityFuncOp()
 2:         (pre_mlp_layernorm): IdentityOp()
 2:         (mlp): TEFusedMLP(
 2:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 2:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 2:         )
 2:       )
 2:     )
 2:     (final_layernorm): RMSNorm()
 2:   )
 2:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 5: )
 6: GPTModel(
 6:   (embedding): LanguageModelEmbedding(
 6:     (word_embeddings): VocabParallelEmbedding()
 6:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 6:   )
 6:   (rotary_pos_emb): RotaryEmbedding()
 6:   (decoder): TransformerBlock(
 6:     (layers): ModuleList(
 6:       (0-31): 32 x TransformerLayer(
 6:         (input_layernorm): IdentityOp()
 6:         (self_attention): SelfAttention(
 6:           (core_attention): TEDotProductAttention(
 6:             (flash_attention): FlashAttention()
 6:             (fused_attention): FusedAttention()
 6:             (unfused_attention): UnfusedDotProductAttention(
 6:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 6:               (attention_dropout): Dropout(p=0.0, inplace=False)
 6:             )
 6:           )
 6:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 6:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 6:           (q_layernorm): IdentityOp()
 6:           (k_layernorm): IdentityOp()
 6:         )
 6:         (pre_cross_attn_layernorm): IdentityOp()
 6:         (cross_attention): IdentityOp()
 6:         (cross_attn_bda): IdentityFuncOp()
 6:         (pre_mlp_layernorm): IdentityOp()
 6:         (mlp): TEFusedMLP(
 6:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 6:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 6:         )
 6:       )
 6:     )
 6:     (final_layernorm): RMSNorm()
 6:   )
 6:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 6: )
 1: GPTModel(
 1:   (embedding): LanguageModelEmbedding(
 1:     (word_embeddings): VocabParallelEmbedding()
 1:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 1:   )
 1:   (rotary_pos_emb): RotaryEmbedding()
 1:   (decoder): TransformerBlock(
 1:     (layers): ModuleList(
 1:       (0-31): 32 x TransformerLayer(
 1:         (input_layernorm): IdentityOp()
 1:         (self_attention): SelfAttention(
 1:           (core_attention): TEDotProductAttention(
 1:             (flash_attention): FlashAttention()
 1:             (fused_attention): FusedAttention()
 1:             (unfused_attention): UnfusedDotProductAttention(
 1:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 1:               (attention_dropout): Dropout(p=0.0, inplace=False)
 1:             )
 1:           )
 1:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 1:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 1:           (q_layernorm): IdentityOp()
 1:           (k_layernorm): IdentityOp()
 1:         )
 1:         (pre_cross_attn_layernorm): IdentityOp()
 1:         (cross_attention): IdentityOp()
 1:         (cross_attn_bda): IdentityFuncOp()
 1:         (pre_mlp_layernorm): IdentityOp()
 1:         (mlp): TEFusedMLP(
 1:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 1:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 1:         )
 1:       )
 1:     )
 1:     (final_layernorm): RMSNorm()
 1:   )
 1:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 1: )
 2: )
 3: GPTModel(
 3:   (embedding): LanguageModelEmbedding(
 3:     (word_embeddings): VocabParallelEmbedding()
 3:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 3:   )
 3:   (rotary_pos_emb): RotaryEmbedding()
 3:   (decoder): TransformerBlock(
 3:     (layers): ModuleList(
 3:       (0-31): 32 x TransformerLayer(
 3:         (input_layernorm): IdentityOp()
 3:         (self_attention): SelfAttention(
 3:           (core_attention): TEDotProductAttention(
 3:             (flash_attention): FlashAttention()
 3:             (fused_attention): FusedAttention()
 3:             (unfused_attention): UnfusedDotProductAttention(
 3:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 3:               (attention_dropout): Dropout(p=0.0, inplace=False)
 3:             )
 3:           )
 3:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 3:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 3:           (q_layernorm): IdentityOp()
 3:           (k_layernorm): IdentityOp()
 3:         )
 3:         (pre_cross_attn_layernorm): IdentityOp()
 3:         (cross_attention): IdentityOp()
 3:         (cross_attn_bda): IdentityFuncOp()
 3:         (pre_mlp_layernorm): IdentityOp()
 3:         (mlp): TEFusedMLP(
 3:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 3:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 3:         )
 3:       )
 3:     )
 3:     (final_layernorm): RMSNorm()
 3:   )
 3:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 3: )
 0: )
 0: 
 0: MCore config:
 4: GPTModel(
 4:   (embedding): LanguageModelEmbedding(
 4:     (word_embeddings): VocabParallelEmbedding()
 4:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 4:   )
 4:   (rotary_pos_emb): RotaryEmbedding()
 4:   (decoder): TransformerBlock(
 4:     (layers): ModuleList(
 4:       (0-31): 32 x TransformerLayer(
 4:         (input_layernorm): IdentityOp()
 4:         (self_attention): SelfAttention(
 4:           (core_attention): TEDotProductAttention(
 4:             (flash_attention): FlashAttention()
 4:             (fused_attention): FusedAttention()
 4:             (unfused_attention): UnfusedDotProductAttention(
 4:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 4:               (attention_dropout): Dropout(p=0.0, inplace=False)
 4:             )
 4:           )
 4:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 4:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 4:           (q_layernorm): IdentityOp()
 4:           (k_layernorm): IdentityOp()
 4:         )
 4:         (pre_cross_attn_layernorm): IdentityOp()
 4:         (cross_attention): IdentityOp()
 4:         (cross_attn_bda): IdentityFuncOp()
 4:         (pre_mlp_layernorm): IdentityOp()
 4:         (mlp): TEFusedMLP(
 4:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 4:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 4:         )
 4:       )
 4:     )
 4:     (final_layernorm): RMSNorm()
 4:   )
 4:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 4: )
 0: Llama31Config8B(tensor_model_parallel_size=1,
 0:                 pipeline_model_parallel_comm_backend=None,
 0:                 pipeline_model_parallel_size=1,
 0:                 virtual_pipeline_model_parallel_size=None,
 0:                 sequence_parallel=False,
 0:                 context_parallel_size=1,
 0:                 hierarchical_context_parallel_sizes=None,
 0:                 expert_model_parallel_size=1,
 0:                 expert_tensor_parallel_size=1,
 0:                 moe_extended_tp=False,
 0:                 perform_initialization=True,
 0:                 use_cpu_initialization=False,
 0:                 fp16=False,
 0:                 bf16=True,
 0:                 params_dtype=torch.bfloat16,
 0:                 timers=<megatron.core.timers.Timers object at 0x7f2bb92d3050>,
 0:                 finalize_model_grads_func=<function MegatronOptimizerModule.on_fit_start.<locals>.finalize_model_grads_func at 0x7f2b9c1c47c0>,
 0:                 grad_scale_func=None,
 0:                 no_sync_func=<bound method DistributedDataParallel.no_sync of DDP(
 0:   (module): Float16Module(
 0:     (module): GPTModel(
 0:       (embedding): LanguageModelEmbedding(
 0:         (word_embeddings): VocabParallelEmbedding()
 0:         (embedding_dropout): Dropout(p=0.0, inplace=False)
 0:       )
 0:       (rotary_pos_emb): RotaryEmbedding()
 0:       (decoder): TransformerBlock(
 0:         (layers): ModuleList(
 0:           (0-31): 32 x TransformerLayer(
 0:             (input_layernorm): IdentityOp()
 0:             (self_attention): SelfAttention(
 0:               (core_attention): TEDotProductAttention(
 0:                 (flash_attention): FlashAttention()
 0:                 (fused_attention): FusedAttention()
 0:                 (unfused_attention): UnfusedDotProductAttention(
 0:                   (scale_mask_softmax): FusedScaleMaskSoftmax()
 0:                   (attention_dropout): Dropout(p=0.0, inplace=False)
 0:                 )
 0:               )
 0:               (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 0:               (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 0:               (q_layernorm): IdentityOp()
 0:               (k_layernorm): IdentityOp()
 0:             )
 0:             (pre_cross_attn_layernorm): IdentityOp()
 0:             (cross_attention): IdentityOp()
 0:             (cross_attn_bda): IdentityFuncOp()
 0:             (pre_mlp_layernorm): IdentityOp()
 0:             (mlp): TEFusedMLP(
 0:               (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 0:               (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 0:             )
 0:           )
 0:         )
 0:         (final_layernorm): RMSNorm()
 0:       )
 0:       (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 0:     )
 0:   )
10: GPTModel(
10:   (embedding): LanguageModelEmbedding(
10:     (word_embeddings): VocabParallelEmbedding()
10:     (embedding_dropout): Dropout(p=0.0, inplace=False)
10:   )
10:   (rotary_pos_emb): RotaryEmbedding()
10:   (decoder): TransformerBlock(
10:     (layers): ModuleList(
10:       (0-31): 32 x TransformerLayer(
10:         (input_layernorm): IdentityOp()
10:         (self_attention): SelfAttention(
10:           (core_attention): TEDotProductAttention(
10:             (flash_attention): FlashAttention()
10:             (fused_attention): FusedAttention()
10:             (unfused_attention): UnfusedDotProductAttention(
10:               (scale_mask_softmax): FusedScaleMaskSoftmax()
10:               (attention_dropout): Dropout(p=0.0, inplace=False)
10:             )
10:           )
10:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
10:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
10:           (q_layernorm): IdentityOp()
10:           (k_layernorm): IdentityOp()
 0: )>,
10:         )
10:         (pre_cross_attn_layernorm): IdentityOp()
10:         (cross_attention): IdentityOp()
10:         (cross_attn_bda): IdentityFuncOp()
10:         (pre_mlp_layernorm): IdentityOp()
10:         (mlp): TEFusedMLP(
10:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
10:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
10:         )
10:       )
10:     )
10:     (final_layernorm): RMSNorm()
10:   )
10:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 0:                 grad_sync_func=None,
12: GPTModel(
12:   (embedding): LanguageModelEmbedding(
12:     (word_embeddings): VocabParallelEmbedding()
12:     (embedding_dropout): Dropout(p=0.0, inplace=False)
12:   )
12:   (rotary_pos_emb): RotaryEmbedding()
12:   (decoder): TransformerBlock(
12:     (layers): ModuleList(
12:       (0-31): 32 x TransformerLayer(
12:         (input_layernorm): IdentityOp()
12:         (self_attention): SelfAttention(
12:           (core_attention): TEDotProductAttention(
12:             (flash_attention): FlashAttention()
12:             (fused_attention): FusedAttention()
12:             (unfused_attention): UnfusedDotProductAttention(
12:               (scale_mask_softmax): FusedScaleMaskSoftmax()
12:               (attention_dropout): Dropout(p=0.0, inplace=False)
12:             )
12:           )
12:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
12:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
12:           (q_layernorm): IdentityOp()
12:           (k_layernorm): IdentityOp()
 0:                 param_sync_func=None,
12:         )
12:         (pre_cross_attn_layernorm): IdentityOp()
12:         (cross_attention): IdentityOp()
12:         (cross_attn_bda): IdentityFuncOp()
12:         (pre_mlp_layernorm): IdentityOp()
12:         (mlp): TEFusedMLP(
12:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
12:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
12:         )
12:       )
12:     )
12:     (final_layernorm): RMSNorm()
12:   )
12:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
15: GPTModel(
15:   (embedding): LanguageModelEmbedding(
15:     (word_embeddings): VocabParallelEmbedding()
15:     (embedding_dropout): Dropout(p=0.0, inplace=False)
15:   )
15:   (rotary_pos_emb): RotaryEmbedding()
15:   (decoder): TransformerBlock(
15:     (layers): ModuleList(
15:       (0-31): 32 x TransformerLayer(
15:         (input_layernorm): IdentityOp()
15:         (self_attention): SelfAttention(
15:           (core_attention): TEDotProductAttention(
15:             (flash_attention): FlashAttention()
15:             (fused_attention): FusedAttention()
15:             (unfused_attention): UnfusedDotProductAttention(
15:               (scale_mask_softmax): FusedScaleMaskSoftmax()
15:               (attention_dropout): Dropout(p=0.0, inplace=False)
15:             )
15:           )
15:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
15:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
15:           (q_layernorm): IdentityOp()
15:           (k_layernorm): IdentityOp()
15:         )
15:         (pre_cross_attn_layernorm): IdentityOp()
15:         (cross_attention): IdentityOp()
15:         (cross_attn_bda): IdentityFuncOp()
15:         (pre_mlp_layernorm): IdentityOp()
15:         (mlp): TEFusedMLP(
15:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
15:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
15:         )
15:       )
15:     )
15:     (final_layernorm): RMSNorm()
15:   )
15:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
15: )
 8: GPTModel(
 8:   (embedding): LanguageModelEmbedding(
 8:     (word_embeddings): VocabParallelEmbedding()
 8:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 8:   )
 8:   (rotary_pos_emb): RotaryEmbedding()
 8:   (decoder): TransformerBlock(
 8:     (layers): ModuleList(
 8:       (0-31): 32 x TransformerLayer(
 8:         (input_layernorm): IdentityOp()
 8:         (self_attention): SelfAttention(
 8:           (core_attention): TEDotProductAttention(
 8:             (flash_attention): FlashAttention()
 8:             (fused_attention): FusedAttention()
 8:             (unfused_attention): UnfusedDotProductAttention(
 8:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 8:               (attention_dropout): Dropout(p=0.0, inplace=False)
 8:             )
 8:           )
 8:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 8:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 8:           (q_layernorm): IdentityOp()
 8:           (k_layernorm): IdentityOp()
 0:                 deterministic_mode=False,
 8:         )
 8:         (pre_cross_attn_layernorm): IdentityOp()
 8:         (cross_attention): IdentityOp()
 8:         (cross_attn_bda): IdentityFuncOp()
 8:         (pre_mlp_layernorm): IdentityOp()
 8:         (mlp): TEFusedMLP(
 8:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 8:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 8:         )
 8:       )
 8:     )
 8:     (final_layernorm): RMSNorm()
 8:   )
 8:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 9: GPTModel(
 9:   (embedding): LanguageModelEmbedding(
 9:     (word_embeddings): VocabParallelEmbedding()
 9:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 9:   )
 9:   (rotary_pos_emb): RotaryEmbedding()
 9:   (decoder): TransformerBlock(
 9:     (layers): ModuleList(
 9:       (0-31): 32 x TransformerLayer(
 9:         (input_layernorm): IdentityOp()
 9:         (self_attention): SelfAttention(
 9:           (core_attention): TEDotProductAttention(
 9:             (flash_attention): FlashAttention()
 9:             (fused_attention): FusedAttention()
 9:             (unfused_attention): UnfusedDotProductAttention(
 9:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 9:               (attention_dropout): Dropout(p=0.0, inplace=False)
 9:             )
 9:           )
 9:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 9:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 9:           (q_layernorm): IdentityOp()
 9:           (k_layernorm): IdentityOp()
 0:                 enable_autocast=False,
 0:                 autocast_dtype=torch.bfloat16,
 9:         )
 9:         (pre_cross_attn_layernorm): IdentityOp()
 9:         (cross_attention): IdentityOp()
 9:         (cross_attn_bda): IdentityFuncOp()
 9:         (pre_mlp_layernorm): IdentityOp()
 9:         (mlp): TEFusedMLP(
 9:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 9:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 9:         )
 9:       )
 9:     )
 9:     (final_layernorm): RMSNorm()
 9:   )
 9:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 9: )
 0:                 num_microbatches_with_partial_activation_checkpoints=None,
10: )
 0:                 gradient_accumulation_fusion=True,
11: GPTModel(
11:   (embedding): LanguageModelEmbedding(
11:     (word_embeddings): VocabParallelEmbedding()
11:     (embedding_dropout): Dropout(p=0.0, inplace=False)
11:   )
11:   (rotary_pos_emb): RotaryEmbedding()
11:   (decoder): TransformerBlock(
11:     (layers): ModuleList(
11:       (0-31): 32 x TransformerLayer(
11:         (input_layernorm): IdentityOp()
11:         (self_attention): SelfAttention(
11:           (core_attention): TEDotProductAttention(
11:             (flash_attention): FlashAttention()
11:             (fused_attention): FusedAttention()
11:             (unfused_attention): UnfusedDotProductAttention(
11:               (scale_mask_softmax): FusedScaleMaskSoftmax()
11:               (attention_dropout): Dropout(p=0.0, inplace=False)
11:             )
11:           )
11:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
11:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
11:           (q_layernorm): IdentityOp()
11:           (k_layernorm): IdentityOp()
11:         )
11:         (pre_cross_attn_layernorm): IdentityOp()
11:         (cross_attention): IdentityOp()
11:         (cross_attn_bda): IdentityFuncOp()
11:         (pre_mlp_layernorm): IdentityOp()
11:         (mlp): TEFusedMLP(
11:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
11:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
11:         )
11:       )
11:     )
11:     (final_layernorm): RMSNorm()
11:   )
11:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
11: )
13: GPTModel(
13:   (embedding): LanguageModelEmbedding(
13:     (word_embeddings): VocabParallelEmbedding()
13:     (embedding_dropout): Dropout(p=0.0, inplace=False)
13:   )
13:   (rotary_pos_emb): RotaryEmbedding()
13:   (decoder): TransformerBlock(
13:     (layers): ModuleList(
13:       (0-31): 32 x TransformerLayer(
13:         (input_layernorm): IdentityOp()
13:         (self_attention): SelfAttention(
13:           (core_attention): TEDotProductAttention(
13:             (flash_attention): FlashAttention()
13:             (fused_attention): FusedAttention()
13:             (unfused_attention): UnfusedDotProductAttention(
13:               (scale_mask_softmax): FusedScaleMaskSoftmax()
13:               (attention_dropout): Dropout(p=0.0, inplace=False)
13:             )
13:           )
13:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
13:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
13:           (q_layernorm): IdentityOp()
13:           (k_layernorm): IdentityOp()
13:         )
13:         (pre_cross_attn_layernorm): IdentityOp()
13:         (cross_attention): IdentityOp()
13:         (cross_attn_bda): IdentityFuncOp()
13:         (pre_mlp_layernorm): IdentityOp()
13:         (mlp): TEFusedMLP(
13:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
13:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
13:         )
13:       )
13:     )
13:     (final_layernorm): RMSNorm()
13:   )
13:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
13: )
14: GPTModel(
14:   (embedding): LanguageModelEmbedding(
14:     (word_embeddings): VocabParallelEmbedding()
14:     (embedding_dropout): Dropout(p=0.0, inplace=False)
14:   )
14:   (rotary_pos_emb): RotaryEmbedding()
14:   (decoder): TransformerBlock(
14:     (layers): ModuleList(
14:       (0-31): 32 x TransformerLayer(
14:         (input_layernorm): IdentityOp()
14:         (self_attention): SelfAttention(
14:           (core_attention): TEDotProductAttention(
14:             (flash_attention): FlashAttention()
14:             (fused_attention): FusedAttention()
14:             (unfused_attention): UnfusedDotProductAttention(
14:               (scale_mask_softmax): FusedScaleMaskSoftmax()
14:               (attention_dropout): Dropout(p=0.0, inplace=False)
14:             )
14:           )
14:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
14:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
14:           (q_layernorm): IdentityOp()
14:           (k_layernorm): IdentityOp()
 0:                 async_tensor_model_parallel_allreduce=False,
14:         )
14:         (pre_cross_attn_layernorm): IdentityOp()
14:         (cross_attention): IdentityOp()
14:         (cross_attn_bda): IdentityFuncOp()
14:         (pre_mlp_layernorm): IdentityOp()
14:         (mlp): TEFusedMLP(
14:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
14:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
14:         )
14:       )
14:     )
14:     (final_layernorm): RMSNorm()
14:   )
14:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
14: )
 8: )
12: )
 0:                 use_te_rng_tracker=(True,),
 0:                 tp_comm_overlap=False,
 0:                 tp_comm_bulk_wgrad=True,
 0:                 tp_comm_bulk_dgrad=True,
 0:                 tp_comm_overlap_ag=True,
 0:                 tp_comm_overlap_rs=True,
 0:                 tp_comm_overlap_rs_dgrad=False,
 0:                 tp_comm_split_ag=True,
 0:                 tp_comm_atomic_ag=False,
 0:                 tp_comm_split_rs=True,
 0:                 tp_comm_atomic_rs=False,
 0:                 cross_entropy_loss_fusion=True,
 0:                 cross_entropy_fusion_impl='te',
 0:                 tp_comm_overlap_disable_qkv=False,
 0:                 tp_comm_overlap_disable_fc1=False,
 0:                 tp_comm_bootstrap_backend=None,
 0:                 overlap_moe_expert_parallel_comm=False,
 0:                 delay_wgrad_compute=False,
 0:                 pipeline_dtype=torch.bfloat16,
 0:                 variable_seq_lengths=False,
 0:                 overlap_p2p_comm=True,
 0:                 batch_p2p_comm=False,
 0:                 batch_p2p_sync=True,
 0:                 use_ring_exchange_p2p=False,
 0:                 deallocate_pipeline_outputs=True,
 0:                 defer_embedding_wgrad_compute=False,
 0:                 wgrad_deferral_limit=50,
 0:                 overlap_p2p_comm_warmup_flush=False,
 0:                 microbatch_group_size_per_vp_stage=1,
 0:                 cpu_offloading=False,
 0:                 cpu_offloading_num_layers=0,
 0:                 _cpu_offloading_context=None,
 0:                 cpu_offloading_activations=True,
 0:                 cpu_offloading_weights=False,
 0:                 cpu_offloading_double_buffering=False,
 0:                 barrier_with_L1_time=True,
 0:                 num_layers=32,
 0:                 mtp_num_layers=None,
 0:                 mtp_loss_scaling_factor=None,
 0:                 num_layers_in_first_pipeline_stage=None,
 0:                 num_layers_in_last_pipeline_stage=None,
 0:                 pipeline_model_parallel_layout=None,
 0:                 account_for_embedding_in_pipeline_split=False,
 0:                 account_for_loss_in_pipeline_split=False,
 0:                 hidden_size=4096,
 0:                 num_attention_heads=32,
 0:                 attention_backend=<AttnBackend.auto: 5>,
 0:                 softmax_scale=None,
 0:                 softmax_type='vanilla',
 0:                 num_query_groups=8,
 0:                 ffn_hidden_size=14336,
 0:                 kv_channels=128,
 0:                 hidden_dropout=0.0,
 0:                 attention_dropout=0.0,
 0:                 fp32_residual_connection=False,
 0:                 apply_residual_connection_post_layernorm=False,
 0:                 layernorm_epsilon=1e-05,
 0:                 layernorm_zero_centered_gamma=False,
 0:                 add_bias_linear=False,
 0:                 add_qkv_bias=False,
 0:                 gated_linear_unit=True,
 0:                 activation_func=<function silu at 0x7f2eb28f6480>,
 0:                 activation_func_fp8_input_store=False,
 0:                 glu_linear_offset=0.0,
 0:                 activation_func_clamp_value=None,
 0:                 num_moe_experts=None,
 0:                 rotary_interleaved=False,
 0:                 window_size=None,
 0:                 window_attn_skip_freq=None,
 0:                 normalization='RMSNorm',
 0:                 qk_layernorm=False,
 0:                 test_mode=False,
 0:                 calculate_per_token_loss=False,
 0:                 multi_latent_attention=False,
 0:                 no_rope_freq=None,
 0:                 moe_deepep_num_sms=20,
 0:                 init_method=functools.partial(<function normal_ at 0x7f2eb2776de0>, mean=0.0, std=0.02),
 0:                 output_layer_init_method=functools.partial(<function normal_ at 0x7f2eb2776de0>, mean=0.0, std=0.0025),
 0:                 init_method_std=0.02,
 0:                 embedding_init_method=functools.partial(<function normal_ at 0x7f2eb2776de0>, mean=0.0, std=0.02),
 0:                 embedding_init_method_std=0.02,
 0:                 init_model_with_meta_device=False,
 0:                 apply_query_key_layer_scaling=False,
 0:                 attention_softmax_in_fp32=False,
 0:                 disable_bf16_reduced_precision_matmul=False,
 0:                 bias_activation_fusion=True,
 0:                 masked_softmax_fusion=True,
 0:                 persist_layer_norm=True,
 0:                 memory_efficient_layer_norm=False,
 0:                 bias_dropout_fusion=True,
 0:                 apply_rope_fusion=True,
 0:                 use_fused_weighted_squared_relu=False,
 0:                 fused_single_qkv_rope=True,
 0:                 recompute_granularity=None,
 0:                 recompute_method=None,
 0:                 recompute_num_layers=None,
 0:                 distribute_saved_activations=None,
 0:                 recompute_modules=['core_attn'],
 0:                 fp8='hybrid',
 0:                 fp8_recipe='tensorwise',
 0:                 fp8_param=True,
 0:                 fp8_margin=0,
 0:                 fp8_interval=1,
 0:                 fp8_amax_history_len=1,
 0:                 fp8_amax_compute_algo='most_recent',
 0:                 fp8_wgrad=True,
 0:                 fp8_dot_product_attention=False,
 0:                 fp8_multi_head_attention=False,
 0:                 tp_only_amax_red=True,
 0:                 first_last_layers_bf16=False,
 0:                 num_layers_at_start_in_bf16=0,
 0:                 num_layers_at_end_in_bf16=0,
 0:                 use_kitchen=False,
 0:                 fp4=None,
 0:                 fp4_recipe='nvfp4',
 0:                 fp4_param=False,
 0:                 moe_shared_expert_intermediate_size=None,
 0:                 moe_shared_expert_overlap=False,
 0:                 moe_layer_freq=1,
 0:                 moe_ffn_hidden_size=None,
 0:                 moe_router_load_balancing_type='aux_loss',
 0:                 moe_router_topk=2,
 0:                 moe_router_topk_limited_devices=None,
 0:                 moe_router_padding_for_fp8=False,
 0:                 moe_router_num_groups=None,
 0:                 moe_router_group_topk=None,
 0:                 moe_router_pre_softmax=False,
 0:                 moe_router_topk_scaling_factor=None,
 0:                 moe_router_score_function='softmax',
 0:                 moe_router_dtype=None,
 0:                 moe_router_enable_expert_bias=False,
 0:                 moe_router_bias_update_rate=0.001,
 0:                 moe_router_force_load_balancing=False,
 0:                 moe_grouped_gemm=False,
 0:                 moe_use_legacy_grouped_gemm=False,
 0:                 moe_aux_loss_coeff=0.0,
 0:                 moe_z_loss_coeff=None,
 0:                 moe_input_jitter_eps=None,
 0:                 moe_token_dropping=False,
 0:                 moe_token_dispatcher_type='allgather',
 0:                 moe_enable_deepep=False,
 0:                 moe_per_layer_logging=False,
 0:                 moe_expert_capacity_factor=None,
 0:                 moe_pad_expert_input_to_capacity=False,
 0:                 moe_token_drop_policy='probs',
 0:                 moe_layer_recompute=False,
 0:                 moe_permute_fusion=False,
 0:                 moe_router_fusion=False,
 0:                 moe_apply_probs_on_input=False,
 0:                 cp_comm_type=None,
 0:                 enable_cuda_graph=1,
 0:                 cuda_graph_use_single_mempool=False,
 0:                 cuda_graph_retain_backward_graph=False,
 0:                 cuda_graph_warmup_steps=3,
 0:                 external_cuda_graph=False,
 0:                 cuda_graph_scope='full_iteration',
 0:                 clone_scatter_output_in_embedding=True,
 0:                 disable_parameter_transpose_cache=False,
 0:                 config_logger_dir='',
 0:                 flash_decode=False,
 0:                 use_te_activation_func=False,
 0:                 inference_rng_tracker=False,
 0:                 inference_sampling_seed=42,
 0:                 symmetric_ar_type=None,
 0:                 mrope_section=None,
 0:                 is_hybrid_model=False,
 0:                 mamba_state_dim=128,
 0:                 mamba_head_dim=64,
 0:                 mamba_num_groups=8,
 0:                 mamba_num_heads=None,
 0:                 use_mamba_mem_eff_path=True,
 0:                 mlp_chunks_for_prefill=1,
 0:                 heterogeneous_block_specs=False,
 0:                 hetereogenous_dist_checkpoint=False,
 0:                 quant_recipe=None,
 0:                 transformer_impl='transformer_engine',
 0:                 fp16_lm_cross_entropy=False,
 0:                 parallel_output=True,
 0:                 share_embeddings_and_output_weights=False,
 0:                 make_vocab_size_divisible_by=128,
 0:                 position_embedding_type='rope',
 0:                 rotary_base=500000,
 0:                 rotary_percent=1.0,
 0:                 seq_len_interpolation_factor=None,
 0:                 seq_length=8192,
 0:                 scatter_embedding_sequence_parallel=True,
 0:                 use_transformer_engine_full_layer_spec=False,
 0:                 transformer_layer_spec=<function default_layer_spec at 0x7f2bb9fda660>,
 0:                 forward_step_fn=<function gpt_forward_step at 0x7f2bb9fd8ea0>,
 0:                 data_step_fn=<function gpt_data_step at 0x7f2bb9fd8c20>,
 0:                 generation_config=None,
 0:                 vocab_size=None,
 0:                 tp_comm_overlap_cfg=None,
 0:                 use_transformer_engine_op_fuser=True,
 0:                 scale_factor=8.0,
 0:                 low_freq_factor=1.0,
 0:                 high_freq_factor=4.0,
 0:                 old_context_len=8192)
 0: [AUX I 2025-10-09 14:43:33 data:291] Instantiating MegatronPretrainingSampler with total_samples: 10000000 and consumed_samples: 0
 1: [rank1]:[W1009 14:43:33.075217639 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 56 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 2: [rank2]:[W1009 14:43:33.097154044 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 59 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
13: [rank13]:[W1009 14:43:33.010365658 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 92 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 3: [rank3]:[W1009 14:43:33.124763034 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 62 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 9: [rank9]:[W1009 14:43:33.029648252 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 80 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
11: [rank11]:[W1009 14:43:33.028312346 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 86 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
14: [rank14]:[W1009 14:43:33.024394839 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 95 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
15: [rank15]:[W1009 14:43:33.028245749 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 98 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 5: [rank5]:[W1009 14:43:33.132395569 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 68 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 6: [rank6]:[W1009 14:43:33.148389188 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 71 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 7: [rank7]:[W1009 14:43:33.148809429 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 74 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
12: [rank12]:[W1009 14:43:33.054384317 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 89 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 0: [rank0]:[W1009 14:43:33.158288088 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 53 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
10: [rank10]:[W1009 14:43:33.062446656 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 83 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 8: [rank8]:[W1009 14:43:33.105733231 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 77 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 4: [rank4]:[W1009 14:43:34.325238415 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 65 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 0: [AUX I 2025-10-09 14:43:35 custom_callbacks:129] Starting training warmup
 0: [AUX I 2025-10-09 14:43:35 custom_callbacks:133]     Starting warmup step 0
 0: [AUX I 2025-10-09 14:43:44 custom_callbacks:150]     Finished warmup step 0, takes 9.415629148483276 s
 0: [AUX I 2025-10-09 14:43:44 custom_callbacks:133]     Starting warmup step 1
 0: [NeMo I 2025-10-09 14:43:44 full_cuda_graph:164] Capture CUDA graph for training!!!
 0: [NeMo I 2025-10-09 14:43:45 full_cuda_graph:182] CUDA graph capture done!!!
 0: [AUX I 2025-10-09 14:43:45 custom_callbacks:150]     Finished warmup step 1, takes 1.0239722728729248 s
 0: [AUX I 2025-10-09 14:43:45 custom_callbacks:157] Finished training warmup: 10.444520711898804 s. 
 0: [AUX I 2025-10-09 14:43:45 custom_callbacks:176] Starting validation warmups
 0: [NeMo I 2025-10-09 14:43:45 full_cuda_graph:164] Capture CUDA graph for validation!!!
 0: [NeMo I 2025-10-09 14:43:46 full_cuda_graph:182] CUDA graph capture done!!!
 0: [AUX I 2025-10-09 14:43:46 custom_callbacks:195] Finished validation warmup: 0.8115866184234619 s. 
 0: [AUX I 2025-10-09 14:43:46 custom_callbacks:202] Finished training warmup: 0.8126728534698486 s. 
 0: [AUX I 2025-10-09 14:43:46 custom_callbacks:212] Time spent in run_training_warmup: 0.8179514408111572s
 0: :::MLLOG {"namespace": "", "time_ms": 1760021026561, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"warmup_time": 101.04639801301528}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021026604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"init_finished": 0.043697210028767586}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021026604, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021026606, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021026606, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021044299, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5502884387969971, "reduced_train_loss": 8.21461296081543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 31.0, "samples_count": 1024.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021061977, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5554184913635254, "reduced_train_loss": 7.7638349533081055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 63.0, "samples_count": 2048.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021080154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5683162212371826, "reduced_train_loss": 7.219351291656494}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 95.0, "samples_count": 3072.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021098705, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5762512683868408, "reduced_train_loss": 7.102668285369873}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 127.0, "samples_count": 4096.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021117604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6358718872070312, "reduced_train_loss": 6.8734846115112305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 159.0, "samples_count": 5120.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021137013, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5739996433258057, "reduced_train_loss": 6.780654430389404}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 191.0, "samples_count": 6144.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021156081, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6051661968231201, "reduced_train_loss": 6.7151336669921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 223.0, "samples_count": 7168.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021175074, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5727567672729492, "reduced_train_loss": 6.61610221862793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 255.0, "samples_count": 8192.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021194073, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5758368968963623, "reduced_train_loss": 6.56387996673584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 287.0, "samples_count": 9216.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021213277, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5777373313903809, "reduced_train_loss": 6.308149337768555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 319.0, "samples_count": 10240.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021232304, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5685293674468994, "reduced_train_loss": 6.2276105880737305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 351.0, "samples_count": 11264.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021251226, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6204180717468262, "reduced_train_loss": 6.285170555114746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 383.0, "samples_count": 12288.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021270570, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5787572860717773, "reduced_train_loss": 6.134818077087402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 415.0, "samples_count": 13312.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021289362, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5724074840545654, "reduced_train_loss": 5.9253082275390625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 447.0, "samples_count": 14336.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021308187, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6164162158966064, "reduced_train_loss": 5.7782487869262695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 479.0, "samples_count": 15360.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021327277, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6106410026550293, "reduced_train_loss": 5.680979251861572}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 511.0, "samples_count": 16384.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021346270, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5702197551727295, "reduced_train_loss": 5.522421836853027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 543.0, "samples_count": 17408.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021365172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.579439640045166, "reduced_train_loss": 5.45716667175293}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 575.0, "samples_count": 18432.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021384430, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.627922534942627, "reduced_train_loss": 5.312331199645996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 607.0, "samples_count": 19456.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021403749, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5838029384613037, "reduced_train_loss": 5.274730205535889}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 639.0, "samples_count": 20480.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021422699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5778026580810547, "reduced_train_loss": 5.068887710571289}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 671.0, "samples_count": 21504.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021441575, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6295247077941895, "reduced_train_loss": 4.976772308349609}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 703.0, "samples_count": 22528.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021461359, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6293070316314697, "reduced_train_loss": 4.908074378967285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 735.0, "samples_count": 23552.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021480392, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5876352787017822, "reduced_train_loss": 4.817791938781738}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 767.0, "samples_count": 24576.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021480644, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5911974920858635}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021480644, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021480645, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 24576, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021480806, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.4151496887207031}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 0, "samples_count": 32}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021480978, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1726069450378418}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1, "samples_count": 64}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021481149, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1708824634552002}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2, "samples_count": 96}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021481319, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17018795013427734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3, "samples_count": 128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021481487, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1682753562927246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4, "samples_count": 160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021481651, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16390514373779297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5, "samples_count": 192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021481824, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17231488227844238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6, "samples_count": 224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021481993, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16936683654785156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 7, "samples_count": 256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021482164, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17064452171325684}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 8, "samples_count": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021482336, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1722116470336914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 9, "samples_count": 320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021482503, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16704821586608887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 10, "samples_count": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021482673, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16997694969177246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 11, "samples_count": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021482842, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16933250427246094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 12, "samples_count": 416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021483013, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17057323455810547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 13, "samples_count": 448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021483180, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16728925704956055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 14, "samples_count": 480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021483349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16862177848815918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 15, "samples_count": 512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021483516, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16717123985290527}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 16, "samples_count": 544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021483687, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17093491554260254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 17, "samples_count": 576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021483855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16807317733764648}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 18, "samples_count": 608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021484025, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16997671127319336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 19, "samples_count": 640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021484192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.167067289352417}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 20, "samples_count": 672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021484359, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1672971248626709}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 21, "samples_count": 704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021484532, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1730027198791504}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 22, "samples_count": 736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021484696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16392302513122559}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 23, "samples_count": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021484864, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16751503944396973}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 24, "samples_count": 800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021485036, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17287278175354004}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 25, "samples_count": 832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021485206, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16962099075317383}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 26, "samples_count": 864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021485371, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1652235984802246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 27, "samples_count": 896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021485543, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17179274559020996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 28, "samples_count": 928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021485712, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1691751480102539}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 29, "samples_count": 960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021485879, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1664719581604004}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 30, "samples_count": 992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021486051, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1726057529449463}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 31, "samples_count": 1024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021486058, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 4.856475830078125, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021486058, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.414453564037103}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021486058, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 24576, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021486059, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021504837, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.624988317489624, "reduced_train_loss": 4.816575527191162}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 799.0, "samples_count": 25600.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021523774, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5690658092498779, "reduced_train_loss": 4.765488147735596}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 831.0, "samples_count": 26624.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021542534, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5678961277008057, "reduced_train_loss": 4.644761085510254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 863.0, "samples_count": 27648.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021561650, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6347265243530273, "reduced_train_loss": 4.570996284484863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 895.0, "samples_count": 28672.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021580223, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5767457485198975, "reduced_train_loss": 4.5539350509643555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 927.0, "samples_count": 29696.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021599177, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.582463264465332, "reduced_train_loss": 4.51810359954834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 959.0, "samples_count": 30720.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021618111, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6350569725036621, "reduced_train_loss": 4.409773826599121}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 991.0, "samples_count": 31744.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021637343, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5893545150756836, "reduced_train_loss": 4.387414932250977}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1023.0, "samples_count": 32768.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021656531, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6443326473236084, "reduced_train_loss": 4.335615158081055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1055.0, "samples_count": 33792.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021676053, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6289057731628418, "reduced_train_loss": 4.238117694854736}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1087.0, "samples_count": 34816.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021695250, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5750641822814941, "reduced_train_loss": 4.205099105834961}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1119.0, "samples_count": 35840.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021714307, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5926690101623535, "reduced_train_loss": 4.176291465759277}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1151.0, "samples_count": 36864.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021733463, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5775573253631592, "reduced_train_loss": 4.1269989013671875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1183.0, "samples_count": 37888.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021752236, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.575197696685791, "reduced_train_loss": 4.069867134094238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1215.0, "samples_count": 38912.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021771157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6395208835601807, "reduced_train_loss": 4.124438285827637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1247.0, "samples_count": 39936.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021790509, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6254723072052002, "reduced_train_loss": 4.070929527282715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1279.0, "samples_count": 40960.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021810665, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6317436695098877, "reduced_train_loss": 4.124738693237305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1311.0, "samples_count": 41984.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021829533, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5765933990478516, "reduced_train_loss": 4.073192596435547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1343.0, "samples_count": 43008.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021848260, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5700037479400635, "reduced_train_loss": 4.033561706542969}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1375.0, "samples_count": 44032.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021867302, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6198024749755859, "reduced_train_loss": 3.9991884231567383}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1407.0, "samples_count": 45056.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021886224, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5747041702270508, "reduced_train_loss": 3.9790825843811035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1439.0, "samples_count": 46080.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021905303, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5955390930175781, "reduced_train_loss": 3.957963228225708}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1471.0, "samples_count": 47104.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021924410, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.625525951385498, "reduced_train_loss": 4.020299434661865}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1503.0, "samples_count": 48128.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943122, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6225109100341797, "reduced_train_loss": 3.959651470184326}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1535.0, "samples_count": 49152.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943453, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5955661228698167}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943454, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943454, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 49152, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943608, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.4872303009033203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 32, "samples_count": 1056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943770, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1622779369354248}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 33, "samples_count": 1088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021943930, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.15986990928649902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 34, "samples_count": 1120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021944105, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17473483085632324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 35, "samples_count": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021944302, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19790315628051758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 36, "samples_count": 1184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021944492, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1893024444580078}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 37, "samples_count": 1216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021944680, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18816637992858887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 38, "samples_count": 1248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021944844, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16379213333129883}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 39, "samples_count": 1280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021945011, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16689276695251465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 40, "samples_count": 1312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021945177, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1667778491973877}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 41, "samples_count": 1344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021945344, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1667642593383789}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 42, "samples_count": 1376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021945511, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16663575172424316}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 43, "samples_count": 1408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021945679, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16808795928955078}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 44, "samples_count": 1440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021945850, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17125368118286133}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 45, "samples_count": 1472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021946019, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16847968101501465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 46, "samples_count": 1504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021946187, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1687614917755127}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 47, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021946357, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1701042652130127}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 48, "samples_count": 1568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021946528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16976714134216309}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 49, "samples_count": 1600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021946701, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17412638664245605}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 50, "samples_count": 1632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021946872, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1704082489013672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 51, "samples_count": 1664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021947038, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1666092872619629}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 52, "samples_count": 1696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021947212, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17352771759033203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 53, "samples_count": 1728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021947387, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17515850067138672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 54, "samples_count": 1760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021947553, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1659379005432129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 55, "samples_count": 1792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021947718, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16521263122558594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 56, "samples_count": 1824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021947887, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16922378540039062}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 57, "samples_count": 1856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948061, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17344164848327637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 58, "samples_count": 1888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948229, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1685488224029541}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 59, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948399, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16916584968566895}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 60, "samples_count": 1952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948568, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1696178913116455}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 61, "samples_count": 1984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948738, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1693873405456543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 62, "samples_count": 2016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948910, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17247414588928223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 63, "samples_count": 2048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948915, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.924262523651123, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 49152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948916, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.462449460988864}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948916, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 49152, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021948916, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021968270, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5741565227508545, "reduced_train_loss": 3.954662799835205}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1567.0, "samples_count": 50176.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760021987173, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.577568769454956, "reduced_train_loss": 3.9419994354248047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1599.0, "samples_count": 51200.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022006123, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5746524333953857, "reduced_train_loss": 3.8430066108703613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1631.0, "samples_count": 52224.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022024874, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5718669891357422, "reduced_train_loss": 3.835724353790283}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1663.0, "samples_count": 53248.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022043704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5824651718139648, "reduced_train_loss": 3.920753002166748}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1695.0, "samples_count": 54272.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022062721, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.587134838104248, "reduced_train_loss": 3.871952533721924}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1727.0, "samples_count": 55296.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022082203, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6295979022979736, "reduced_train_loss": 3.8597073554992676}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1759.0, "samples_count": 56320.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022100986, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6132714748382568, "reduced_train_loss": 3.842489719390869}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1791.0, "samples_count": 57344.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022120112, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5761070251464844, "reduced_train_loss": 3.8570377826690674}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1823.0, "samples_count": 58368.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022138930, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5754051208496094, "reduced_train_loss": 3.797677993774414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1855.0, "samples_count": 59392.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022157721, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5810770988464355, "reduced_train_loss": 3.817183494567871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1887.0, "samples_count": 60416.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022176548, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5681374073028564, "reduced_train_loss": 3.708077907562256}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1919.0, "samples_count": 61440.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022195499, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5865371227264404, "reduced_train_loss": 3.7613601684570312}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1951.0, "samples_count": 62464.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022214843, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6315844058990479, "reduced_train_loss": 3.770425319671631}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1983.0, "samples_count": 63488.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022233648, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5765798091888428, "reduced_train_loss": 3.7806918621063232}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2015.0, "samples_count": 64512.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022252562, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5827591419219971, "reduced_train_loss": 3.764894723892212}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2047.0, "samples_count": 65536.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022271282, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5913825035095215, "reduced_train_loss": 3.7898988723754883}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2079.0, "samples_count": 66560.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022290646, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6324234008789062, "reduced_train_loss": 3.7569730281829834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2111.0, "samples_count": 67584.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022309861, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6069896221160889, "reduced_train_loss": 3.685002565383911}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2143.0, "samples_count": 68608.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022328863, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5845186710357666, "reduced_train_loss": 3.657013416290283}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2175.0, "samples_count": 69632.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022347865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5743751525878906, "reduced_train_loss": 3.6557674407958984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2207.0, "samples_count": 70656.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022366820, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6145377159118652, "reduced_train_loss": 3.772660732269287}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2239.0, "samples_count": 71680.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022385700, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5793564319610596, "reduced_train_loss": 3.700544834136963}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2271.0, "samples_count": 72704.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022405247, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6331961154937744, "reduced_train_loss": 3.6741456985473633}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2303.0, "samples_count": 73728.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022405556, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5945832527551526}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022405556, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022405556, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 73728, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022405710, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.4643135070800781}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 64, "samples_count": 2080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022405869, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.15941548347473145}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 65, "samples_count": 2112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022406047, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1780409812927246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 66, "samples_count": 2144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022406217, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16933035850524902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 67, "samples_count": 2176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022406386, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16948986053466797}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 68, "samples_count": 2208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022406556, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16962194442749023}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 69, "samples_count": 2240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022406728, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17244601249694824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 70, "samples_count": 2272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022406903, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1749403476715088}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 71, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022407070, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16745638847351074}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 72, "samples_count": 2336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022407238, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16797542572021484}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 73, "samples_count": 2368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022407408, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1695880889892578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 74, "samples_count": 2400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022407577, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1688399314880371}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 75, "samples_count": 2432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022407746, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16951537132263184}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 76, "samples_count": 2464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022407918, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17126679420471191}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 77, "samples_count": 2496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022408085, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16698265075683594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 78, "samples_count": 2528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022408259, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17412877082824707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 79, "samples_count": 2560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022408428, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16969728469848633}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 80, "samples_count": 2592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022408597, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1689009666442871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 81, "samples_count": 2624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022408768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17092013359069824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 82, "samples_count": 2656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022408939, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17099809646606445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 83, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022409108, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16874384880065918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 84, "samples_count": 2720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022409277, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1694324016571045}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 85, "samples_count": 2752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022409447, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16970229148864746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 86, "samples_count": 2784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022409617, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1696939468383789}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 87, "samples_count": 2816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022409784, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16707777976989746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 88, "samples_count": 2848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022409955, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17087411880493164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 89, "samples_count": 2880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410126, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17128443717956543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 90, "samples_count": 2912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410302, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17557668685913086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 91, "samples_count": 2944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410477, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17480158805847168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 92, "samples_count": 2976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410646, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16927099227905273}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 93, "samples_count": 3008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410814, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16858839988708496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 94, "samples_count": 3040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410981, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16679763793945312}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 95, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410992, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.6678290367126465, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 73728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410992, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.4364642050350085}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410992, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 73728, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022410992, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022429669, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5791680812835693, "reduced_train_loss": 3.6175496578216553}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2335.0, "samples_count": 74752.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022448677, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5848617553710938, "reduced_train_loss": 3.688316583633423}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2367.0, "samples_count": 75776.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022467522, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6410777568817139, "reduced_train_loss": 3.697476863861084}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2399.0, "samples_count": 76800.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022486253, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5784096717834473, "reduced_train_loss": 3.66049861907959}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2431.0, "samples_count": 77824.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022505052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5833234786987305, "reduced_train_loss": 3.6595916748046875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2463.0, "samples_count": 78848.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022523822, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5850543975830078, "reduced_train_loss": 3.6714468002319336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2495.0, "samples_count": 79872.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022543187, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.577441930770874, "reduced_train_loss": 3.614840030670166}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2527.0, "samples_count": 80896.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022562091, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5799746513366699, "reduced_train_loss": 3.711829423904419}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2559.0, "samples_count": 81920.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022580762, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5725002288818359, "reduced_train_loss": 3.637543201446533}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2591.0, "samples_count": 82944.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022599647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5727236270904541, "reduced_train_loss": 3.544529676437378}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2623.0, "samples_count": 83968.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022618414, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5827264785766602, "reduced_train_loss": 3.6664345264434814}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2655.0, "samples_count": 84992.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022637351, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5740420818328857, "reduced_train_loss": 3.6949217319488525}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2687.0, "samples_count": 86016.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022656283, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5774502754211426, "reduced_train_loss": 3.5719199180603027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2719.0, "samples_count": 87040.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022675051, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5758497714996338, "reduced_train_loss": 3.6131691932678223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2751.0, "samples_count": 88064.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022693873, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5805070400238037, "reduced_train_loss": 3.5617902278900146}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2783.0, "samples_count": 89088.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022713183, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5759546756744385, "reduced_train_loss": 3.536404609680176}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2815.0, "samples_count": 90112.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022732314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5746028423309326, "reduced_train_loss": 3.6456503868103027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2847.0, "samples_count": 91136.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022751356, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.575392484664917, "reduced_train_loss": 3.563572406768799}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2879.0, "samples_count": 92160.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022770026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.581575870513916, "reduced_train_loss": 3.584167003631592}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2911.0, "samples_count": 93184.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022789593, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6362259387969971, "reduced_train_loss": 3.597933053970337}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2943.0, "samples_count": 94208.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022808517, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6319773197174072, "reduced_train_loss": 3.5358619689941406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2975.0, "samples_count": 95232.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022827309, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5752568244934082, "reduced_train_loss": 3.60174298286438}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3007.0, "samples_count": 96256.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022845996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5738418102264404, "reduced_train_loss": 3.5504589080810547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3039.0, "samples_count": 97280.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022864831, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.58278489112854, "reduced_train_loss": 3.61181640625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3071.0, "samples_count": 98304.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5913572631172125}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865155, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865155, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 98304, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865317, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.4880039691925049}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 96, "samples_count": 3104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865482, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16491436958312988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 97, "samples_count": 3136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865664, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18160080909729004}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 98, "samples_count": 3168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022865852, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18783855438232422}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 99, "samples_count": 3200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022866044, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19208526611328125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 100, "samples_count": 3232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022866234, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18993735313415527}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 101, "samples_count": 3264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022866420, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18593549728393555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 102, "samples_count": 3296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022866607, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18748688697814941}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 103, "samples_count": 3328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022866799, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19218230247497559}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 104, "samples_count": 3360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022866987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1878976821899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 105, "samples_count": 3392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022867175, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18826556205749512}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 106, "samples_count": 3424}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022867370, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19443941116333008}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 107, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022867556, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.185624361038208}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 108, "samples_count": 3488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022867746, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19020605087280273}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 109, "samples_count": 3520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022867932, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18669939041137695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 110, "samples_count": 3552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022868133, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.20017170906066895}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 111, "samples_count": 3584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022868313, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18015050888061523}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 112, "samples_count": 3616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022868506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19293475151062012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 113, "samples_count": 3648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022868692, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18639898300170898}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 114, "samples_count": 3680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022868877, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18445634841918945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 115, "samples_count": 3712}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022869068, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19159221649169922}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 116, "samples_count": 3744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022869267, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1990830898284912}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 117, "samples_count": 3776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022869452, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1846919059753418}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 118, "samples_count": 3808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022869645, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1931450366973877}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 119, "samples_count": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022869831, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18622183799743652}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 120, "samples_count": 3872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022870026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19507765769958496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 121, "samples_count": 3904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022870208, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18211030960083008}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 122, "samples_count": 3936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022870404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.19597840309143066}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 123, "samples_count": 3968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022870586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1816728115081787}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 124, "samples_count": 4000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022870825, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.2391195297241211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 125, "samples_count": 4032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022871002, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17671966552734375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 126, "samples_count": 4064}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022871169, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16701745986938477}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 127, "samples_count": 4096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022871200, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.525815486907959, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 98304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022871200, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 6.045469188946299}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022871200, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 98304, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022871200, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022890154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6177401542663574, "reduced_train_loss": 3.5509302616119385}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3103.0, "samples_count": 99328.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022909005, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6107416152954102, "reduced_train_loss": 3.509518623352051}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3135.0, "samples_count": 100352.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022927874, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5784604549407959, "reduced_train_loss": 3.558802604675293}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3167.0, "samples_count": 101376.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022946441, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.581902265548706, "reduced_train_loss": 3.539280414581299}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3199.0, "samples_count": 102400.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022965281, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5952417850494385, "reduced_train_loss": 3.6112923622131348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3231.0, "samples_count": 103424.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760022983898, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5719285011291504, "reduced_train_loss": 3.496215343475342}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3263.0, "samples_count": 104448.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023002746, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5755014419555664, "reduced_train_loss": 3.5402984619140625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3295.0, "samples_count": 105472.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023021481, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5738363265991211, "reduced_train_loss": 3.634807825088501}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3327.0, "samples_count": 106496.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023040297, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5703651905059814, "reduced_train_loss": 3.5336556434631348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3359.0, "samples_count": 107520.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023059742, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6316382884979248, "reduced_train_loss": 3.5820822715759277}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3391.0, "samples_count": 108544.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023078294, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5795040130615234, "reduced_train_loss": 3.479861259460449}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3423.0, "samples_count": 109568.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023097262, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5751492977142334, "reduced_train_loss": 3.466858386993408}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3455.0, "samples_count": 110592.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023115836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5885670185089111, "reduced_train_loss": 3.507258415222168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3487.0, "samples_count": 111616.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023134726, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.577899694442749, "reduced_train_loss": 3.4555795192718506}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3519.0, "samples_count": 112640.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023153837, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5802807807922363, "reduced_train_loss": 3.4750685691833496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3551.0, "samples_count": 113664.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023173596, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6329691410064697, "reduced_train_loss": 3.493102788925171}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3583.0, "samples_count": 114688.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023192626, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5813515186309814, "reduced_train_loss": 3.417355537414551}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3615.0, "samples_count": 115712.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023211551, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.576754093170166, "reduced_train_loss": 3.402904987335205}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3647.0, "samples_count": 116736.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023230953, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6230111122131348, "reduced_train_loss": 3.398603677749634}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3679.0, "samples_count": 117760.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023250728, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5875015258789062, "reduced_train_loss": 3.4712979793548584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3711.0, "samples_count": 118784.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023269441, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.583064079284668, "reduced_train_loss": 3.446688652038574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3743.0, "samples_count": 119808.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023288175, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5835678577423096, "reduced_train_loss": 3.4041390419006348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3775.0, "samples_count": 120832.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023306813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5775344371795654, "reduced_train_loss": 3.568202257156372}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3807.0, "samples_count": 121856.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023325704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.618232011795044, "reduced_train_loss": 3.497910499572754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3839.0, "samples_count": 122880.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326022, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5922165367695319}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326023, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326023, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 122880, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326175, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.47170042991638184}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 128, "samples_count": 4128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326341, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16631674766540527}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 129, "samples_count": 4160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326510, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16929030418395996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 130, "samples_count": 4192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326688, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1774895191192627}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 131, "samples_count": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023326856, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1680891513824463}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 132, "samples_count": 4256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023327043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18651390075683594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 133, "samples_count": 4288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023327216, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17319989204406738}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 134, "samples_count": 4320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023327392, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1766955852508545}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 135, "samples_count": 4352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023327565, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17265009880065918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 136, "samples_count": 4384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023327735, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16984891891479492}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 137, "samples_count": 4416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023327904, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16870498657226562}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 138, "samples_count": 4448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023328075, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17131900787353516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 139, "samples_count": 4480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023328243, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16849970817565918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 140, "samples_count": 4512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023328414, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1701955795288086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 141, "samples_count": 4544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023328582, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16843438148498535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 142, "samples_count": 4576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023328751, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1685044765472412}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 143, "samples_count": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023328921, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17013812065124512}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 144, "samples_count": 4640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023329087, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.165846586227417}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 145, "samples_count": 4672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023329260, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.173309326171875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 146, "samples_count": 4704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023329426, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1662440299987793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 147, "samples_count": 4736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023329609, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.18329596519470215}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 148, "samples_count": 4768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023329802, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1923973560333252}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 149, "samples_count": 4800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023329977, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17557311058044434}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 150, "samples_count": 4832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023330145, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16721725463867188}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 151, "samples_count": 4864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023330314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16914606094360352}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 152, "samples_count": 4896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023330484, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16983294486999512}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 153, "samples_count": 4928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023330649, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16556262969970703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 154, "samples_count": 4960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023330817, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.168107271194458}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 155, "samples_count": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023330983, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16534090042114258}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 156, "samples_count": 5024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331152, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16940712928771973}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 157, "samples_count": 5056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331323, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1712183952331543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 158, "samples_count": 5088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331491, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16747403144836426}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 159, "samples_count": 5120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331500, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.42781925201416, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 122880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331500, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.477649168053176}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331500, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 122880, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023331500, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023350383, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5816965103149414, "reduced_train_loss": 3.4057984352111816}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3871.0, "samples_count": 123904.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023369098, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5787205696105957, "reduced_train_loss": 3.4229087829589844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3903.0, "samples_count": 124928.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023388376, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.643496036529541, "reduced_train_loss": 3.4777514934539795}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3935.0, "samples_count": 125952.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023407112, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5797579288482666, "reduced_train_loss": 3.4278788566589355}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3967.0, "samples_count": 126976.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023425831, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5820341110229492, "reduced_train_loss": 3.4524893760681152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3999.0, "samples_count": 128000.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023445010, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5793185234069824, "reduced_train_loss": 3.4658429622650146}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4031.0, "samples_count": 129024.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023463781, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.580747127532959, "reduced_train_loss": 3.4700872898101807}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4063.0, "samples_count": 130048.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023483061, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5680904388427734, "reduced_train_loss": 3.5047667026519775}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4095.0, "samples_count": 131072.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023502075, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5662646293640137, "reduced_train_loss": 3.427124500274658}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4127.0, "samples_count": 132096.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023520776, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5787572860717773, "reduced_train_loss": 3.350238800048828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4159.0, "samples_count": 133120.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023539700, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6193945407867432, "reduced_train_loss": 3.378688335418701}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4191.0, "samples_count": 134144.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023558499, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5751559734344482, "reduced_train_loss": 3.435924530029297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4223.0, "samples_count": 135168.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023577712, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.641442060470581, "reduced_train_loss": 3.4068946838378906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4255.0, "samples_count": 136192.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023596437, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5898935794830322, "reduced_train_loss": 3.4399585723876953}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4287.0, "samples_count": 137216.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023615287, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5753951072692871, "reduced_train_loss": 3.356534481048584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4319.0, "samples_count": 138240.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023634186, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5723233222961426, "reduced_train_loss": 3.448760509490967}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4351.0, "samples_count": 139264.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023653033, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5729930400848389, "reduced_train_loss": 3.4363465309143066}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4383.0, "samples_count": 140288.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023671875, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5775203704833984, "reduced_train_loss": 3.4014735221862793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4415.0, "samples_count": 141312.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023690675, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.582366943359375, "reduced_train_loss": 3.386460304260254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4447.0, "samples_count": 142336.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023709732, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5701620578765869, "reduced_train_loss": 3.3542065620422363}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4479.0, "samples_count": 143360.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023728558, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5768074989318848, "reduced_train_loss": 3.380359649658203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4511.0, "samples_count": 144384.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023747770, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6409454345703125, "reduced_train_loss": 3.3263602256774902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4543.0, "samples_count": 145408.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023766586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.574648380279541, "reduced_train_loss": 3.364011764526367}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4575.0, "samples_count": 146432.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023785405, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.627310037612915, "reduced_train_loss": 3.447010040283203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4607.0, "samples_count": 147456.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023785728, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5914434439296201}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023785729, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023785729, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 147456, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023785889, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.4846160411834717}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 160, "samples_count": 5152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023786054, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16518592834472656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 161, "samples_count": 5184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023786231, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17670202255249023}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 162, "samples_count": 5216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023786409, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17852377891540527}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 163, "samples_count": 5248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023786639, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.22927165031433105}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 164, "samples_count": 5280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023786815, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1761021614074707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 165, "samples_count": 5312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023786975, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.15991783142089844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 166, "samples_count": 5344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023787143, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16802430152893066}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 167, "samples_count": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023787311, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1679377555847168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 168, "samples_count": 5408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023787482, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17110657691955566}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 169, "samples_count": 5440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023787650, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16800594329833984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 170, "samples_count": 5472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023787821, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17167377471923828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 171, "samples_count": 5504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023787987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16598105430603027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 172, "samples_count": 5536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023788161, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17377853393554688}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 173, "samples_count": 5568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023788332, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17033743858337402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 174, "samples_count": 5600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023788503, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1710224151611328}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 175, "samples_count": 5632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023788671, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16831254959106445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 176, "samples_count": 5664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023788843, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1718285083770752}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 177, "samples_count": 5696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023789007, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16433238983154297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 178, "samples_count": 5728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023789179, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17145156860351562}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 179, "samples_count": 5760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023789349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17060184478759766}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 180, "samples_count": 5792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023789521, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17206144332885742}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 181, "samples_count": 5824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023789692, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1704709529876709}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 182, "samples_count": 5856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023789865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17367792129516602}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 183, "samples_count": 5888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023790030, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16454148292541504}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 184, "samples_count": 5920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023790201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17117595672607422}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 185, "samples_count": 5952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023790365, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1643233299255371}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 186, "samples_count": 5984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023790534, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16864609718322754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 187, "samples_count": 6016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023790705, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17096638679504395}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 188, "samples_count": 6048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023790873, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16766071319580078}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 189, "samples_count": 6080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023791044, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17150187492370605}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 190, "samples_count": 6112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023791214, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.169358491897583}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 191, "samples_count": 6144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023791220, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.376413583755493, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 147456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023791220, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.49179025599733}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023791220, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 147456, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023791220, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023810035, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5799062252044678, "reduced_train_loss": 3.4029736518859863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4639.0, "samples_count": 148480.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023828901, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5843489170074463, "reduced_train_loss": 3.377533435821533}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4671.0, "samples_count": 149504.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023848047, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5726110935211182, "reduced_train_loss": 3.4917855262756348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4703.0, "samples_count": 150528.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023867009, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5800721645355225, "reduced_train_loss": 3.3462328910827637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4735.0, "samples_count": 151552.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023886113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5795578956604004, "reduced_train_loss": 3.337538242340088}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4767.0, "samples_count": 152576.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023904743, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5752897262573242, "reduced_train_loss": 3.3472886085510254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4799.0, "samples_count": 153600.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023923765, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5748932361602783, "reduced_train_loss": 3.329005241394043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4831.0, "samples_count": 154624.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023942541, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5782887935638428, "reduced_train_loss": 3.332125663757324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4863.0, "samples_count": 155648.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023961304, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5764248371124268, "reduced_train_loss": 3.333568811416626}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4895.0, "samples_count": 156672.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023980131, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5768063068389893, "reduced_train_loss": 3.274240016937256}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4927.0, "samples_count": 157696.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760023999012, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5756008625030518, "reduced_train_loss": 3.467329978942871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4959.0, "samples_count": 158720.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024017690, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5820183753967285, "reduced_train_loss": 3.339294910430908}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4991.0, "samples_count": 159744.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024036569, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5758707523345947, "reduced_train_loss": 3.3902721405029297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5023.0, "samples_count": 160768.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024055375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6250884532928467, "reduced_train_loss": 3.3570966720581055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5055.0, "samples_count": 161792.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024073990, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5789620876312256, "reduced_train_loss": 3.274312973022461}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5087.0, "samples_count": 162816.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024092856, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5814187526702881, "reduced_train_loss": 3.286466121673584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5119.0, "samples_count": 163840.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024111620, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6385481357574463, "reduced_train_loss": 3.3996810913085938}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5151.0, "samples_count": 164864.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024130733, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6358363628387451, "reduced_train_loss": 3.3432021141052246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5183.0, "samples_count": 165888.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024150061, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5789165496826172, "reduced_train_loss": 3.3000950813293457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5215.0, "samples_count": 166912.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024169106, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5875179767608643, "reduced_train_loss": 3.3483757972717285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5247.0, "samples_count": 167936.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024188100, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5854716300964355, "reduced_train_loss": 3.25384521484375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5279.0, "samples_count": 168960.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024207154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.578101396560669, "reduced_train_loss": 3.268710136413574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5311.0, "samples_count": 169984.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024226301, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5831329822540283, "reduced_train_loss": 3.342686176300049}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5343.0, "samples_count": 171008.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024245515, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5881812572479248, "reduced_train_loss": 3.311572551727295}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5375.0, "samples_count": 172032.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024245830, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.591939170727907}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024245830, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024245830, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 172032, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024245986, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.4718294143676758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 192, "samples_count": 6176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024246148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16201114654541016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 193, "samples_count": 6208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024246318, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16981196403503418}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 194, "samples_count": 6240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024246515, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1971569061279297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 195, "samples_count": 6272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024246676, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16072654724121094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 196, "samples_count": 6304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024246842, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1665048599243164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 197, "samples_count": 6336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024247017, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17464637756347656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 198, "samples_count": 6368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024247192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1751265525817871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 199, "samples_count": 6400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024247363, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1711108684539795}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 200, "samples_count": 6432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024247531, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16841506958007812}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 201, "samples_count": 6464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024247703, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1720132827758789}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 202, "samples_count": 6496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024247874, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17034626007080078}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 203, "samples_count": 6528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024248043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16904592514038086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 204, "samples_count": 6560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024248209, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16602587699890137}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 205, "samples_count": 6592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024248378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16887211799621582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 206, "samples_count": 6624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024248552, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17382001876831055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 207, "samples_count": 6656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024248720, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16847467422485352}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 208, "samples_count": 6688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024248888, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16748499870300293}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 209, "samples_count": 6720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024249059, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1717679500579834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 210, "samples_count": 6752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024249232, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17248272895812988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 211, "samples_count": 6784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024249399, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16719603538513184}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 212, "samples_count": 6816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024249564, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16547918319702148}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 213, "samples_count": 6848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024249733, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16830730438232422}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 214, "samples_count": 6880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024249906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17331433296203613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 215, "samples_count": 6912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024250074, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16821551322937012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 216, "samples_count": 6944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024250246, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17127108573913574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 217, "samples_count": 6976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024250419, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17292308807373047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 218, "samples_count": 7008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024250586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16776537895202637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 219, "samples_count": 7040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024250756, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1695845127105713}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 220, "samples_count": 7072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024250926, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.169769287109375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 221, "samples_count": 7104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024251093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16753149032592773}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 222, "samples_count": 7136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024251262, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1687915325164795}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 223, "samples_count": 7168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024251272, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3090643882751465, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 172032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024251272, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.443004567991011}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024251273, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 172032, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024251273, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 24576, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024270045, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5801131725311279, "reduced_train_loss": 3.2846693992614746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5407.0, "samples_count": 173056.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024288924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5808696746826172, "reduced_train_loss": 3.4340109825134277}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5439.0, "samples_count": 174080.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024308065, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5960342884063721, "reduced_train_loss": 3.353031635284424}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5471.0, "samples_count": 175104.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024327508, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6401152610778809, "reduced_train_loss": 3.2994251251220703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5503.0, "samples_count": 176128.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024346630, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5893006324768066, "reduced_train_loss": 3.3267951011657715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5535.0, "samples_count": 177152.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024365931, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6583573818206787, "reduced_train_loss": 3.265167474746704}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5567.0, "samples_count": 178176.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024384858, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6147723197937012, "reduced_train_loss": 3.3120601177215576}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5599.0, "samples_count": 179200.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024404360, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6120610237121582, "reduced_train_loss": 3.268524169921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5631.0, "samples_count": 180224.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024423446, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5988955497741699, "reduced_train_loss": 3.292773723602295}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5663.0, "samples_count": 181248.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024442718, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5875394344329834, "reduced_train_loss": 3.2474489212036133}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5695.0, "samples_count": 182272.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024462187, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6334571838378906, "reduced_train_loss": 3.299180030822754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5727.0, "samples_count": 183296.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024481473, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6201999187469482, "reduced_train_loss": 3.189824104309082}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5759.0, "samples_count": 184320.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024500531, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6519145965576172, "reduced_train_loss": 3.305419921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5791.0, "samples_count": 185344.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024519885, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6274888515472412, "reduced_train_loss": 3.288546085357666}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5823.0, "samples_count": 186368.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024539203, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5746028423309326, "reduced_train_loss": 3.1446194648742676}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5855.0, "samples_count": 187392.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024558167, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5770854949951172, "reduced_train_loss": 3.2569665908813477}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5887.0, "samples_count": 188416.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024577192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5789191722869873, "reduced_train_loss": 3.2600560188293457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5919.0, "samples_count": 189440.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024596448, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6111948490142822, "reduced_train_loss": 3.391939163208008}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5951.0, "samples_count": 190464.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024616115, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6343691349029541, "reduced_train_loss": 3.2776036262512207}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5983.0, "samples_count": 191488.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024635458, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6176869869232178, "reduced_train_loss": 3.356823444366455}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6015.0, "samples_count": 192512.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024654570, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6181285381317139, "reduced_train_loss": 3.2699379920959473}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6047.0, "samples_count": 193536.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024673780, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5692431926727295, "reduced_train_loss": 3.249666690826416}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6079.0, "samples_count": 194560.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024692969, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6094951629638672, "reduced_train_loss": 3.2484936714172363}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6111.0, "samples_count": 195584.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024712313, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.5777053833007812, "reduced_train_loss": 3.2858762741088867}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6143.0, "samples_count": 196608.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024712631, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.6007269216211171}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024712631, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 24576, "step": 6144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024712631, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 196608, "step": 6144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024712791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.47908878326416016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 224, "samples_count": 7200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024712952, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16183257102966309}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 225, "samples_count": 7232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024713117, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16417670249938965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 226, "samples_count": 7264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024713286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1695249080657959}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 227, "samples_count": 7296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024713453, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16712188720703125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 228, "samples_count": 7328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024713624, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17077302932739258}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 229, "samples_count": 7360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024713791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1669912338256836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 230, "samples_count": 7392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024713959, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16809701919555664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 231, "samples_count": 7424}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024714131, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17207622528076172}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 232, "samples_count": 7456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024714300, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16929125785827637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 233, "samples_count": 7488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024714470, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1699235439300537}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 234, "samples_count": 7520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024714645, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1743605136871338}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 235, "samples_count": 7552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024714812, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16712331771850586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 236, "samples_count": 7584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024714982, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17022323608398438}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 237, "samples_count": 7616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024715150, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16754150390625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 238, "samples_count": 7648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024715317, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16768622398376465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 239, "samples_count": 7680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024715486, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16869401931762695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 240, "samples_count": 7712}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024715658, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1723473072052002}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 241, "samples_count": 7744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024715836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17786693572998047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 242, "samples_count": 7776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024716000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1635143756866455}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 243, "samples_count": 7808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024716167, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16711640357971191}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 244, "samples_count": 7840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024716335, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16812849044799805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 245, "samples_count": 7872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024716506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17076373100280762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 246, "samples_count": 7904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024716673, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16708612442016602}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 247, "samples_count": 7936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024716845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1721820831298828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 248, "samples_count": 7968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024717017, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1720585823059082}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 249, "samples_count": 8000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024717186, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16867303848266602}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 250, "samples_count": 8032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024717358, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.17270970344543457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 251, "samples_count": 8064}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024717526, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16756367683410645}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 252, "samples_count": 8096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024717695, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16916775703430176}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 253, "samples_count": 8128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024717864, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.1692206859588623}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 254, "samples_count": 8160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024718030, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.16571736335754395}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 255, "samples_count": 8192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024718037, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.256730556488037, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 196608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024718037, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.405972203996498}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 6144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760024718037, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 196608, "step": 6144}}
 0: Average train_step_time 0.5932421922916546
 0: :::MLLOG {"namespace": "", "time_ms": 1760024718050, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 106, "step": 6144, "samples_count": 196608, "status": "success"}}
++ date +%s
+ echo 'RUNANDTIME_STOP 1760024768'
RUNANDTIME_STOP 1760024768
+ set -e
