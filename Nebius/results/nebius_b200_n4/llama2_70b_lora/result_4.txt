+ echo 'Beginning trial 4 of 10'
Beginning trial 4 of 10
+ echo ':::DLPAL /mnt/data/llama2_70b_lora+20250922.sqsh 364 4 worker-[3,1-2,0] '\''unknown'\'' DGXB200_4x8x1xtp1pp1cp1'
:::DLPAL /mnt/data/llama2_70b_lora+20250922.sqsh 364 4 worker-[3,1-2,0] 'unknown' DGXB200_4x8x1xtp1pp1cp1
++ srun -N1 -n1 --container-name=llama2_70b_lora_364 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"4","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8580","host_processor_core_count":"40","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"v2.28.3-1","cublas_version":"13.0.2.14","cudnn_version":"9.13.0.50","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.11.0-1013-nvidia","nvidia_kernel_driver":"570.148.08"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"4","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8580","host_processor_core_count":"40","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"v2.28.3-1","cublas_version":"13.0.2.14","cudnn_version":"9.13.0.50","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.11.0-1013-nvidia","nvidia_kernel_driver":"570.148.08"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=llama2_70b_lora_364 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 --mpi=pmi2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on worker-2
Clearing cache on worker-1
Clearing cache on worker-3
Clearing cache on worker-0
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks-per-node=1 --mpi=pmi2 --container-name=llama2_70b_lora_364 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_common.callbacks import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
:::MLLOG {"namespace": "", "time_ms": 1759227903042, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759227903843, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759227904417, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759227905254, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ export SEED=23862
+ SEED=23862
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1759227907'
RUNANDTIME_START 1759227907
+ srun -l --mpi=pmi2 --ntasks-per-node=8 --time=10 --container-name=llama2_70b_lora_364 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/mnt/data/llama2_70b_lora/GovReport/gov_report/:/data:ro,/mnt/data/llama2_70b_lora/GovReport/model:/ckpt:ro,/home/katya/20250922/output_logs:/results:rw --container-workdir=/workspace/ft-llm --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
29: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/32
30: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/32
31: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/32
24: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/32
25: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/32
26: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/32
27: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/32
28: [2025-09-30 10:25:49,171][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/32
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 470}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949185, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 471}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949185, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 471}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949185, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 471}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949186, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 471}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949186, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "4xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 471}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949186, "event_type": "POINT_IN_TIME", "key": "target_accuracy", "value": 0.925, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 474}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949345, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0004, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 277}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949345, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 278}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949345, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 279}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949345, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949348, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 313}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949348, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 314}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949354, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 800, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 495}}
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: HPU available: False, using: 0 HPUs
 0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
 4: [2025-09-30 10:25:49,412][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/32
 5: [2025-09-30 10:25:49,412][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/32
 7: [2025-09-30 10:25:49,412][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/32
 1: [2025-09-30 10:25:49,413][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/32
 3: [2025-09-30 10:25:49,413][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/32
 2: [2025-09-30 10:25:49,413][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/32
 6: [2025-09-30 10:25:49,413][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/32
18: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/32
16: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/32
17: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/32
20: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/32
19: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/32
21: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/32
22: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/32
23: [2025-09-30 10:25:49,477][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/32
11: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/32
15: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/32
 8: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/32
 9: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/32
10: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/32
13: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/32
14: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/32
12: [2025-09-30 10:25:49,517][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/32
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949971, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 297}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949996, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 297}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949996, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 297}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949996, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 297}}
 0: [NeMo I 2025-09-30 10:25:49 nemo_logging:393] Experiments will be logged at /workspace/ft-llm/nemo_experiments/default/2025-09-30_10-25-49
 0: :::MLLOG {"namespace": "", "time_ms": 1759227949998, "event_type": "POINT_IN_TIME", "key": "seed", "value": 23862, "metadata": {"file": "/workspace/ft-llm/train.py", "lineno": 544}}
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has data parallel group : [0, 4, 8, 12, 16, 20, 24, 28]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has context parallel group: [0, 1, 2, 3]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has model parallel group: [0]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has embedding group: [0]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2025-09-30 10:25:50 nemo_logging:393] Rank 0 has embedding rank: 0
 0: [2025-09-30 10:25:50,006][nemo.lightning.pytorch.strategies.megatron_strategy][INFO] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/32
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 32 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 1: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 2: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 3: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 4: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 5: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 6: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 7: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
11: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
14: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
13: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
10: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
15: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
12: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 8: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 9: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
16: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
17: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
18: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
19: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
20: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
21: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
22: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
23: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
24: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
25: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
26: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
28: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
29: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
30: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
31: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
27: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
13: [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 2: [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
14: [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
12: [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
10: [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 3: [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 6: [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 0: [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
15: [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 7: [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 4: [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 8: [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
11: [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 9: [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
18: [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
26: [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
30: [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
22: [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
28: [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
24: [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
16: [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
19: [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
31: [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
20: [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
27: [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
23: [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 1: [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 5: [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
17: [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
25: [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
29: [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
21: [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
 0: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 1: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 2: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 3: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 4: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 5: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 6: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 7: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
12: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
13: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
11: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 9: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
15: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
10: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
14: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 8: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
16: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
17: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
18: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
19: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
20: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
24: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
30: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
26: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
21: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
22: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
27: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
28: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
31: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
29: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
25: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
23: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 0: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 1: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 2: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 3: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 4: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 5: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 6: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 7: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 8: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 9: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
11: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
10: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
13: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
12: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
15: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
14: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
24: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
16: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
27: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
17: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
18: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
19: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
20: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
26: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
30: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
21: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
22: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
25: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
28: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
29: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
31: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
23: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Setting up ModelTransform for stage: TrainerFn.FITTING
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Found model_transform attribute on pl_module
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Set model_transform to: <function _call_counter.<locals>.wrapper at 0x12d1980f0c20>
 0: :::MLLOG {"namespace": "", "time_ms": 1759227961054, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"before_model_init": 11.700653775944375}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
 0: :::MLLOG {"namespace": "", "time_ms": 1759227961600, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"after_model_init": 0.5457998810452409}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Copying Trainer's 'max_steps' (800) to LR scheduler's 'max_steps'.
 0: [NeMo I 2025-09-30 10:26:01 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Doing selective restore from RestoreConfig(path='/ckpt', load_model_state=True, load_optim_state=False, load_artifacts=False)
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo W 2025-09-30 10:26:01 serialization:184] DEPRECATED: Passing 'checkpoint_dir' as a Path object in load_common_state_dict will no longer be supported in a future release. Please pass it as a string instead.
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo W 2025-09-30 10:26:01 zarr:80] `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 0: [NeMo I 2025-09-30 10:26:01 nemo_logging:393] Loaded sharded_state_dict_metadata from checkpoint: None
 0: [NeMo W 2025-09-30 10:26:01 mapping:155] ShardedTensor.prepend_axis_num greater than 0 is deprecated. In Megatron-Core this can be prevented by setting sharded_state_dict metadata['singleton_local_shards'] to True.
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo W 2025-09-30 10:26:01 zarr:80] `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
 0: [NeMo I 2025-09-30 10:29:17 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1759227961.657s : Time spent in load_checkpoint: 195.997s
 0: [NeMo I 2025-09-30 10:29:17 nemo_logging:393] Restoring model weights from RestoreConfig(path='/ckpt', load_model_state=True, load_optim_state=False, load_artifacts=False)
 0: [NeMo I 2025-09-30 10:29:49 nemo_logging:393] Finished restoring from RestoreConfig(path='/ckpt', load_model_state=True, load_optim_state=False, load_artifacts=False), cleaning up.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
 0: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
17: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
 0: Optimized config:
 0: skip_evals: 3
 0: load_ckpt: true
 0: data_root: /data
 0: ckpt_root: /ckpt
 0: trainer:
 0:   devices: 8
 0:   num_nodes: 4
 0:   max_steps: 800
 0:   val_check_interval: 192
 0:   limit_val_batches: 1.0
 0: model:
 0:   num_layers: 80
 0:   seed: 23862
 0:   tensor_model_parallel_size: 1
 0:   pipeline_model_parallel_size: 1
 0:   context_parallel_size: 4
 0:   eval_cp: null
 0:   global_batch_size: 8
 0:   micro_batch_size: 1
 0:   val_micro_batch_size: 1
 0:   val_global_batch_size: 8
 0:   max_position_embeddings: 8192
 0:   encoder_seq_length: 8192
 0:   sequence_parallel: 0
 0:   ub_tp_comm_overlap: false
 0:   fp8: true
 0:   fp8_param_gather: 0
 0:   fp8_recipe: delayed
 0:   fp8_hybrid: true
 0:   fp8_amax_history_len: 4
 0:   fp8_amax_compute_algo: max
 0:   reduce_amax: true
 0:   fp8_e4m3: false
 0:   fp8_interval: 1
 0:   fp8_margin: 0
 0:   fp8_dot_product_attention: 1
 0:   cp_comm_type: a2a
 0:   activation_func_fp8_input_store: 1
 0:   external_cuda_graph: false
 0:   enable_cuda_graph: 1
 0:   cuda_graph_scope: full_iteration
 0:   use_te_rng_tracker: true
 0:   enable_cg_fp8_weight_caching: 0
 0:   cpu_offloading: false
 0:   cpu_offloading_num_layers: 20
 0:   cpu_offloading_double_buffering: false
 0:   cpu_offloading_activations: true
 0:   cpu_offloading_weights: false
 0:   dropout_recompute: false
 0:   recompute_granularity: null
 0:   recompute_method: null
 0:   recompute_num_layers: null
 0:   distribute_saved_activations: false
 0:   recompute_modules: null
 0:   use_transformer_engine_op_fuser: 1
 0:   fused_single_qkv_rope: 1
 0:   fsdp: null
 0:   use_sharp: false
 0:   memory_profile:
 0:     enabled: false
 0:     start_step: 1
 0:     end_step: 4
 0:     rank: 0
 0:     output_path: /results/
 0:   custom:
 0:     warmup: true
 0:     warmup_train_steps: 5
 0:     warmup_validation_steps: 5
 0:     reset_fp8_stats_after_warmup: 1
 0: optim:
 0:   lr: 0.0004
 0:   on_device_clip_grad: 1
 0:   use_distributed_optimizer: true
 0:   overlap_param_gather_with_optimizer_step: false
 0:   sched:
 0:     warmup_steps: 0
 0: ddp:
 0:   overlap_grad_reduce: false
 0:   overlap_param_gather: false
 0:   fp8_param_gather: 0
 0:   average_in_collective: false
 0:   nccl_ub: false
 0: dataloader:
 0:   num_workers: 8
 0: 
 0: 
 0: MCore config:
 0: Llama2Config70B(tensor_model_parallel_size=1,
 0:                 pipeline_model_parallel_comm_backend=None,
 0:                 pipeline_model_parallel_size=1,
 0:                 virtual_pipeline_model_parallel_size=None,
 0:                 sequence_parallel=0,
 0:                 context_parallel_size=4,
 0:                 hierarchical_context_parallel_sizes=None,
 0:                 expert_model_parallel_size=1,
 0:                 expert_tensor_parallel_size=1,
 0:                 moe_extended_tp=False,
 0:                 perform_initialization=True,
 0:                 use_cpu_initialization=False,
 0:                 fp16=False,
 0:                 bf16=True,
 0:                 params_dtype=torch.bfloat16,
 0:                 timers=<megatron.core.timers.Timers object at 0x12d209a367b0>,
 0:                 finalize_model_grads_func=None,
 0:                 grad_scale_func=None,
 0:                 no_sync_func=None,
 0:                 grad_sync_func=None,
 0:                 param_sync_func=None,
 0:                 deterministic_mode=False,
 0:                 enable_autocast=False,
 0:                 autocast_dtype=torch.bfloat16,
 0:                 num_microbatches_with_partial_activation_checkpoints=None,
 0:                 gradient_accumulation_fusion=False,
 0:                 async_tensor_model_parallel_allreduce=False,
 0:                 use_te_rng_tracker=True,
 0:                 tp_comm_overlap=False,
 0:                 tp_comm_bulk_wgrad=True,
 0:                 tp_comm_bulk_dgrad=True,
 0:                 tp_comm_overlap_ag=True,
 0:                 tp_comm_overlap_rs=True,
 0:                 tp_comm_overlap_rs_dgrad=False,
 0:                 tp_comm_split_ag=True,
 0:                 tp_comm_atomic_ag=False,
 0:                 tp_comm_split_rs=True,
 0:                 tp_comm_atomic_rs=False,
 0:                 cross_entropy_loss_fusion=False,
 0:                 cross_entropy_fusion_impl='native',
 0:                 tp_comm_overlap_disable_qkv=True,
 0:                 tp_comm_overlap_disable_fc1=False,
 0:                 tp_comm_bootstrap_backend='nccl',
 0:                 overlap_moe_expert_parallel_comm=False,
 0:                 delay_wgrad_compute=False,
 0:                 pipeline_dtype=torch.bfloat16,
 0:                 variable_seq_lengths=False,
 0:                 overlap_p2p_comm=False,
 0:                 batch_p2p_comm=True,
 0:                 batch_p2p_sync=True,
 0:                 use_ring_exchange_p2p=False,
 0:                 deallocate_pipeline_outputs=True,
 0:                 defer_embedding_wgrad_compute=False,
 0:                 wgrad_deferral_limit=0,
 0:                 overlap_p2p_comm_warmup_flush=False,
 0:                 microbatch_group_size_per_vp_stage=1,
 0:                 cpu_offloading=False,
 0:                 cpu_offloading_num_layers=20,
 0:                 _cpu_offloading_context=None,
 0:                 cpu_offloading_activations=True,
 0:                 cpu_offloading_weights=False,
 0:                 cpu_offloading_double_buffering=False,
 0:                 barrier_with_L1_time=True,
 0:                 num_layers=80,
 0:                 mtp_num_layers=None,
 0:                 mtp_loss_scaling_factor=None,
 0:                 num_layers_in_first_pipeline_stage=None,
 0:                 num_layers_in_last_pipeline_stage=None,
 0:                 pipeline_model_parallel_layout=None,
 0:                 account_for_embedding_in_pipeline_split=False,
 0:                 account_for_loss_in_pipeline_split=False,
 0:                 hidden_size=8192,
 0:                 num_attention_heads=64,
 0:                 attention_backend=<AttnBackend.auto: 5>,
 0:                 softmax_scale=None,
 0:                 softmax_type='vanilla',
 0:                 num_query_groups=8,
 0:                 ffn_hidden_size=28672,
 0:                 kv_channels=128,
 0:                 hidden_dropout=0.0,
 0:                 attention_dropout=0.0,
 0:                 fp32_residual_connection=False,
 0:                 apply_residual_connection_post_layernorm=False,
 0:                 layernorm_epsilon=1e-05,
 0:                 layernorm_zero_centered_gamma=False,
 0:                 add_bias_linear=False,
 0:                 add_qkv_bias=False,
 0:                 gated_linear_unit=True,
 0:                 activation_func=<function silu at 0x12d503080cc0>,
 0:                 activation_func_fp8_input_store=1,
 0:                 glu_linear_offset=0.0,
 0:                 activation_func_clamp_value=None,
 0:                 num_moe_experts=None,
 0:                 rotary_interleaved=False,
 0:                 window_size=None,
 0:                 window_attn_skip_freq=None,
 0:                 normalization='RMSNorm',
 0:                 qk_layernorm=False,
 0:                 test_mode=False,
 0:                 calculate_per_token_loss=False,
 0:                 multi_latent_attention=False,
 0:                 no_rope_freq=None,
 0:                 moe_deepep_num_sms=20,
 0:                 init_method=functools.partial(<function normal_ at 0x12d502efd620>, mean=0.0, std=0.02),
 0:                 output_layer_init_method=functools.partial(<function normal_ at 0x12d502efd620>, mean=0.0, std=0.0015811388300841897),
 0:                 init_method_std=0.02,
 0:                 embedding_init_method=functools.partial(<function normal_ at 0x12d502efd620>, mean=0.0, std=0.02),
 0:                 embedding_init_method_std=0.02,
 0:                 init_model_with_meta_device=False,
 0:                 apply_query_key_layer_scaling=False,
 0:                 attention_softmax_in_fp32=False,
 0:                 disable_bf16_reduced_precision_matmul=False,
 0:                 bias_activation_fusion=True,
 0:                 masked_softmax_fusion=True,
 0:                 persist_layer_norm=True,
 0:                 memory_efficient_layer_norm=False,
 0:                 bias_dropout_fusion=True,
 0:                 apply_rope_fusion=True,
 0:                 use_fused_weighted_squared_relu=False,
 0:                 fused_single_qkv_rope=1,
 0:                 recompute_granularity=None,
 0:                 recompute_method=None,
 0:                 recompute_num_layers=None,
 0:                 distribute_saved_activations=False,
 0:                 recompute_modules=['core_attn'],
 0:                 fp8='hybrid',
 0:                 fp8_recipe='delayed',
 0:                 fp8_param=False,
 0:                 fp8_margin=0,
 0:                 fp8_interval=1,
 0:                 fp8_amax_history_len=4,
 0:                 fp8_amax_compute_algo='max',
 0:                 fp8_wgrad=True,
 0:                 fp8_dot_product_attention=1,
 0:                 fp8_multi_head_attention=False,
 0:                 tp_only_amax_red=False,
 0:                 first_last_layers_bf16=False,
 0:                 num_layers_at_start_in_bf16=0,
 0:                 num_layers_at_end_in_bf16=0,
 0:                 use_kitchen=False,
 0:                 fp4=None,
 0:                 fp4_recipe='nvfp4',
 0:                 fp4_param=False,
 0:                 moe_shared_expert_intermediate_size=None,
 0:                 moe_shared_expert_overlap=False,
 0:                 moe_layer_freq=1,
 0:                 moe_ffn_hidden_size=None,
 0:                 moe_router_load_balancing_type='aux_loss',
 0:                 moe_router_topk=2,
 0:                 moe_router_topk_limited_devices=None,
 0:                 moe_router_padding_for_fp8=False,
 0:                 moe_router_num_groups=None,
 0:                 moe_router_group_topk=None,
 0:                 moe_router_pre_softmax=False,
 0:                 moe_router_topk_scaling_factor=None,
 0:                 moe_router_score_function='softmax',
 0:                 moe_router_dtype=None,
 0:                 moe_router_enable_expert_bias=False,
 0:                 moe_router_bias_update_rate=0.001,
 0:                 moe_router_force_load_balancing=False,
 0:                 moe_grouped_gemm=False,
 0:                 moe_use_legacy_grouped_gemm=False,
 0:                 moe_aux_loss_coeff=0.0,
 0:                 moe_z_loss_coeff=None,
 0:                 moe_input_jitter_eps=None,
 0:                 moe_token_dropping=False,
 0:                 moe_token_dispatcher_type='allgather',
 0:                 moe_enable_deepep=False,
 0:                 moe_per_layer_logging=False,
 0:                 moe_expert_capacity_factor=None,
 0:                 moe_pad_expert_input_to_capacity=False,
 0:                 moe_token_drop_policy='probs',
 0:                 moe_layer_recompute=False,
 0:                 moe_permute_fusion=False,
 0:                 moe_router_fusion=False,
 0:                 moe_apply_probs_on_input=False,
 0:                 cp_comm_type='a2a',
 0:                 enable_cuda_graph=1,
 0:                 cuda_graph_use_single_mempool=False,
 0:                 cuda_graph_retain_backward_graph=False,
 0:                 cuda_graph_warmup_steps=3,
 0:                 external_cuda_graph=False,
 0:                 cuda_graph_scope='full_iteration',
 0:                 clone_scatter_output_in_embedding=True,
 0:                 disable_parameter_transpose_cache=False,
 0:                 config_logger_dir='',
 0:                 flash_decode=False,
 0:                 use_te_activation_func=False,
 0:                 inference_rng_tracker=False,
 0:                 inference_sampling_seed=42,
 0:                 symmetric_ar_type=None,
 0:                 mrope_section=None,
 0:                 is_hybrid_model=False,
 0:                 mamba_state_dim=128,
 0:                 mamba_head_dim=64,
 0:                 mamba_num_groups=8,
 0:                 mamba_num_heads=None,
 0:                 use_mamba_mem_eff_path=True,
 0:                 mlp_chunks_for_prefill=1,
 0:                 heterogeneous_block_specs=False,
 0:                 hetereogenous_dist_checkpoint=False,
 0:                 quant_recipe=None,
 0:                 transformer_impl='transformer_engine',
 0:                 fp16_lm_cross_entropy=False,
 0:                 parallel_output=True,
 0:                 share_embeddings_and_output_weights=False,
 0:                 make_vocab_size_divisible_by=128,
 0:                 position_embedding_type='rope',
 0:                 rotary_base=10000,
 0:                 rotary_percent=1.0,
 0:                 seq_len_interpolation_factor=None,
 0:                 seq_length=8192,
 0:                 scatter_embedding_sequence_parallel=True,
 0:                 use_transformer_engine_full_layer_spec=False,
 0:                 transformer_layer_spec=<function default_layer_spec at 0x12d20af49da0>,
 0:                 forward_step_fn=<function gpt_forward_step at 0x12d20af485e0>,
 0:                 data_step_fn=<function gpt_data_step at 0x12d20af48220>,
 0:                 generation_config=None,
 0:                 vocab_size=None,
 0:                 tp_comm_overlap_cfg=None,
 0:                 use_transformer_engine_op_fuser=1)
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.0.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.1.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.2.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.3.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.4.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.5.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.6.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.7.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.8.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.9.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.10.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.11.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.12.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.13.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.14.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.15.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.16.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.17.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.18.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.19.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.20.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.21.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.22.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.23.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.24.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.25.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.26.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.27.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.28.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.29.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.30.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.31.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.32.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.32.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.33.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.33.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.34.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.34.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.35.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.35.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.36.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.36.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.37.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.37.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.38.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.38.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.39.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.39.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.40.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.40.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.41.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.41.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.42.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.42.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.43.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.43.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.44.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.44.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.45.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.45.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.46.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.46.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.47.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.47.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.48.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.48.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.49.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.49.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.50.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.50.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.51.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.51.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.52.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.52.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.53.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.53.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.54.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.54.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.55.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.55.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.56.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.56.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.57.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.57.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.58.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.58.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.59.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.59.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.60.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.60.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.61.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.61.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.62.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.62.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.63.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.63.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.64.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.64.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.65.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.65.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.66.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.66.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.67.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.67.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.68.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.68.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.69.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.69.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.70.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.70.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.71.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.71.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.72.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.72.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.73.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.73.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.74.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.74.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.75.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.75.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.76.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.76.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.77.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.77.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.78.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.78.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.79.self_attention.linear_proj
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Adding lora to: module.decoder.layers.79.self_attention.linear_qkv
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] After applying model_transform:
 0:     
 0:       | Name   | Type     | Params | Mode 
 0:     --------------------------------------------
 0:     0 | module | GPTModel | 69.0 B | train
 0:     --------------------------------------------
 0:     44.6 M    Trainable params
 0:     69.0 B    Non-trainable params
 0:     69.0 B    Total params
 0:     276,084.851Total estimated model params size (MB)
 0:     2569      Modules in train mode
 0:     0         Modules in eval mode
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393] Initializing model parallel
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 69021212672
 0: [NeMo I 2025-09-30 10:30:32 nemo_logging:393]  > number of trainable parameters: 44564480 (0.06% of total)
 0: [NeMo I 2025-09-30 10:30:32 utils:662] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=0, reuse_grad_buf_for_mxfp8_param_ag=False, use_megatron_fsdp=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache=False, nccl_ub=False, fsdp_double_buffer=False, outer_dp_sharding_strategy='no_shard', disable_symmetric_registration=False, delay_wgrad_compute=False)
 0: [NeMo I 2025-09-30 10:30:33 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 1
 0:     Params for bucket 1 (44564480 elements, 44564480 padded size):
 0:     	module.decoder.layers.76.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.50.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.37.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.57.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.25.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.7.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.70.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.44.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.62.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.12.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.77.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.65.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.50.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.0.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.38.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.57.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.25.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.69.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.44.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.13.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.77.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.51.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.1.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.38.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.57.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.70.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.44.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.66.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.32.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.12.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.77.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.65.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.51.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.19.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.0.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.38.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.7.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.57.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.70.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.45.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.62.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.32.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.77.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.51.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.19.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.38.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.58.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.26.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.8.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.71.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.45.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.62.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.32.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.13.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.51.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.1.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.39.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.58.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.26.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.70.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.45.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.64.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.32.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.14.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.67.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.52.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.2.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.39.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.58.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.8.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.71.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.45.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.63.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.33.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.52.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.20.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.39.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.58.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.8.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.71.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.46.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.33.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.52.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.20.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.39.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.59.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.27.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.9.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.72.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.46.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.33.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.14.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.78.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.52.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.20.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.2.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.40.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.8.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.59.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.27.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.71.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.46.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.74.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.33.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.15.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.3.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.53.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.20.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.40.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.59.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.72.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.46.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.63.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.34.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.78.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.53.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.21.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.40.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.59.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.9.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.72.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.47.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.0.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.63.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.34.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.78.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.53.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.21.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.40.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.60.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.28.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.73.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.47.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.63.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.34.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.15.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.53.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.3.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.41.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.60.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.28.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.72.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.47.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.0.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.34.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.16.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.78.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.54.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.4.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.41.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.60.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.28.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.10.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.73.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.47.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.35.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.79.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.54.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.22.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.41.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.60.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.28.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.73.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.48.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.16.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.64.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.35.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.79.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.54.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.22.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.4.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.67.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.41.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.61.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.29.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.10.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.48.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.74.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.35.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.16.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.54.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.4.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.67.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.42.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.61.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.29.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.73.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.48.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.35.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.17.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.55.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.5.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.68.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.42.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.61.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.75.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.48.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.36.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.16.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.55.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.23.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.4.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.67.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.42.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.61.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.11.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.74.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.49.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.64.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.36.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.55.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.23.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.68.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.42.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.66.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.30.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.75.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.49.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.65.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.36.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.17.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.55.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.24.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.5.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.68.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.43.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.66.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.30.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.11.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.75.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.49.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.64.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.36.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.18.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.56.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.6.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.69.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.43.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.66.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.76.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.49.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.65.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.37.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.56.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.24.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.68.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.43.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.75.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.12.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.76.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.50.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.37.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.79.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.56.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.24.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.69.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.43.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.74.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.31.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.76.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.50.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.37.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.18.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.79.self_attention.linear_qkv.adapter.linear_out.weight
 0:     	module.decoder.layers.56.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.24.self_attention.linear_qkv.adapter.linear_in.weight
 0:     	module.decoder.layers.6.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.69.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.44.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.62.self_attention.linear_proj.adapter.linear_out.weight
 0:     	module.decoder.layers.31.self_attention.linear_proj.adapter.linear_in.weight
 0:     	module.decoder.layers.12.self_attention.linear_qkv.adapter.linear_out.weight
 0: [NeMo I 2025-09-30 10:30:33 nemo_logging:393] Setting up optimizers
 0: [NeMo I 2025-09-30 10:30:33 utils:662] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0004, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_
 0: time=False, timers=None, config_logger_dir='')
 0: [NeMo I 2025-09-30 10:31:46 full_cuda_graph:164] Capture CUDA graph for training!!!
 0: [NeMo I 2025-09-30 10:31:53 full_cuda_graph:182] CUDA graph capture done!!!
 0: [NeMo I 2025-09-30 10:31:55 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: [NeMo I 2025-09-30 10:32:06 full_cuda_graph:164] Capture CUDA graph for validation!!!
 0: [NeMo I 2025-09-30 10:32:07 full_cuda_graph:182] CUDA graph capture done!!!
 0: [NeMo I 2025-09-30 10:32:08 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228328131, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"warmup_time": 366.5313039929606}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228328132, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"init_finished": 0.0004270430072210729}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228328137, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228328138, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228328139, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 1536, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228332167, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 8.174003601074219}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 80, "lr": 0.0003998458072481446, "step": 10}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228336160, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.699590682983398}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 160, "lr": 0.0003993834667466256, "step": 20}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228340144, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.485879898071289}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 240, "lr": 0.0003986136913909853, "step": 30}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228344157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.221586227416992}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 320, "lr": 0.00039753766811902755, "step": 40}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228348172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.752781867980957}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 400, "lr": 0.0003961570560806461, "step": 50}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228352191, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 6.2356109619140625}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 480, "lr": 0.0003944739840795353, "step": 60}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228356216, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.898014068603516}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 560, "lr": 0.00039249104729072946, "step": 70}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228360249, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.041607856750488}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 640, "lr": 0.00039021130325903074, "step": 80}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228364290, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.904031276702881}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 720, "lr": 0.00038763826718449685, "step": 90}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228368340, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.78854513168335}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 800, "lr": 0.0003847759065022574, "step": 100}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228372386, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.693962574005127}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 880, "lr": 0.0003816286347650163, "step": 110}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228376437, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.992781639099121}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 960, "lr": 0.0003782013048376736, "step": 120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228380482, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.902266502380371}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1040, "lr": 0.00037449920141455944, "step": 130}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228384528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.828021049499512}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1120, "lr": 0.00037052803287081844, "step": 140}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228388584, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.068774223327637}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1200, "lr": 0.0003662939224605091, "step": 150}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228392627, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.951291561126709}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1280, "lr": 0.0003618033988749895, "step": 160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228396670, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.297821521759033}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1360, "lr": 0.00035706338617614897, "step": 170}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228400721, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.741083145141602}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1440, "lr": 0.0003520811931200062, "step": 180}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228404759, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.324284076690674}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1520, "lr": 0.0003468645018871371, "step": 190}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228406240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.4068130482398071}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228406241, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 1536, "step": 192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228406241, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 1536, "step": 192}}
 0: [NeMo I 2025-09-30 10:33:26 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228411362, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9381539229023663, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 1536}}
 0: [NeMo I 2025-09-30 10:33:31 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228411363, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 5.122801900957711}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228411363, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 1536, "step": 192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228411363, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 384, "step": 192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228414624, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.589473724365234}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1600, "lr": 0.0003414213562373095, "step": 200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228418678, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.807037353515625}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1680, "lr": 0.0003357601491065884, "step": 210}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228422717, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.358198642730713}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1760, "lr": 0.00032988960966603677, "step": 220}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228426754, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.413284778594971}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1840, "lr": 0.0003238187898619668, "step": 230}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228430802, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.580580711364746}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 1920, "lr": 0.0003175570504584947, "step": 240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228430822, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.40540904681256507}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228430823, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 384, "step": 240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228430823, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 1920, "step": 240}}
 0: [NeMo I 2025-09-30 10:33:50 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228435445, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9340132343975794, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 1920}}
 0: [NeMo I 2025-09-30 10:33:55 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228435446, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 4.623297462007031}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228435446, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 1920, "step": 240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228435446, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 384, "step": 240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228439508, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.2496185302734375}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2000, "lr": 0.0003111140466039205, "step": 250}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228443565, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.962148189544678}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2080, "lr": 0.0003044997129431898, "step": 260}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228447612, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 4.240565776824951}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2160, "lr": 0.00029772424829939106, "step": 270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228451655, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.842158317565918}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2240, "lr": 0.00029079809994790937, "step": 280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228454916, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.4056218573956964}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228454916, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 384, "step": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228454916, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 2304, "step": 288}}
 0: [NeMo I 2025-09-30 10:34:14 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228459527, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9311846253499819, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 2304}}
 0: [NeMo I 2025-09-30 10:34:19 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228459527, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 4.611990839999635}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228459528, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 2304, "step": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228459528, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 384, "step": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228460348, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.284538269042969}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2320, "lr": 0.0002837319475074856, "step": 290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228464404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.559079647064209}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2400, "lr": 0.000276536686473018, "step": 300}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228468456, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.4008331298828125}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2480, "lr": 0.0002692234114154986, "step": 310}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228472506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.9182915687561035}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2560, "lr": 0.00026180339887498953, "step": 320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228476560, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_loss": 5.021084308624268}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 86, "samples_count": 2640, "lr": 0.00025428808997301496, "step": 330}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228479015, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.4059897064168278}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228479015, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 384, "step": 336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228479015, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 2688, "step": 336}}
 0: [NeMo I 2025-09-30 10:34:39 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228483654, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9249072653709809, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 2688}}
 0: [NeMo I 2025-09-30 10:34:43 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1759228483654, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 4.639000368013512}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228483654, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 2688, "step": 336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759228483775, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 106, "step": 336, "samples_count": 2688, "status": "success"}}
++ date +%s
+ echo 'RUNANDTIME_STOP 1759228509'
RUNANDTIME_STOP 1759228509
+ set -e
