+ echo 'Beginning trial 13 of 22'
Beginning trial 13 of 22
+ echo ':::DLPAL nvcr.io/nvdlfwea/mlperftv51/llama31_8b-arm:20250922 208 18 node[01-18] '\''unknown'\'' GB300_18x4x1xtp1pp1cp2_8b'
:::DLPAL nvcr.io/nvdlfwea/mlperftv51/llama31_8b-arm:20250922 208 18 node[01-18] 'unknown' GB300_18x4x1xtp1pp1cp2_8b
++ srun -N1 -n1 --container-name=llama31_8b_208 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"18","host_processors_per_node":"2","host_processor_model_name":"Neoverse-V2","host_processor_core_count":"72","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"4","accelerator_model_name":"NVIDIA GB300","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"284208 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"v2.28.3-1","cublas_version":"13.0.2.14","cudnn_version":"9.13.0.50","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.11.0-1013-nvidia-64k","nvidia_kernel_driver":"580.95.05"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"18","host_processors_per_node":"2","host_processor_model_name":"Neoverse-V2","host_processor_core_count":"72","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"4","accelerator_model_name":"NVIDIA GB300","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"284208 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"v2.28.3-1","cublas_version":"13.0.2.14","cudnn_version":"9.13.0.50","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.11.0-1013-nvidia-64k","nvidia_kernel_driver":"580.95.05"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=llama31_8b_208 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID  
+ export SEED=17758
+ SEED=17758
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c '
                 host=$(hostname)
                 echo "$host sync_start"
                 sync && echo "$host sync_done"
                 cache_before=$(awk "/^Cached:/ {print \$2}" /proc/meminfo)
                 sudo /sbin/sysctl vm.drop_caches=3
                 cache_after=$(awk "/^Cached:/ {print \$2}" /proc/meminfo)
                 echo "$host cache_cleared ${cache_before}kB to ${cache_after}kB"
            '
node15 sync_start
node04 sync_start
node15 sync_done
node04 sync_done
node03 sync_start
node03 sync_done
node06 sync_start
node06 sync_done
node08 sync_start
node05 sync_start
node08 sync_done
node05 sync_done
node01 sync_start
node02 sync_start
node07 sync_start
node01 sync_done
node07 sync_done
node02 sync_done
node10 sync_start
node10 sync_done
node09 sync_start
node09 sync_done
node16 sync_start
node12 sync_start
node12 sync_done
node16 sync_done
node11 sync_start
node13 sync_start
node11 sync_done
node13 sync_done
node17 sync_start
node14 sync_start
node17 sync_done
node14 sync_done
node18 sync_start
node18 sync_done
vm.drop_caches = 3
node15 cache_cleared 19087936kB to 223360kB
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
node08 cache_cleared 17629440kB to 219008kB
node06 cache_cleared 17545472kB to 279808kB
vm.drop_caches = 3
node07 cache_cleared 17743104kB to 297856kB
node04 cache_cleared 19514112kB to 209856kB
node17 cache_cleared 17158720kB to 266240kB
vm.drop_caches = 3
vm.drop_caches = 3
node10 cache_cleared 18394240kB to 293760kB
vm.drop_caches = 3
node16 cache_cleared 17419968kB to 227712kB
vm.drop_caches = 3
node12 cache_cleared 17049344kB to 208640kB
vm.drop_caches = 3
node11 cache_cleared 16973760kB to 271360kB
vm.drop_caches = 3
node09 cache_cleared 19498176kB to 269952kB
vm.drop_caches = 3
vm.drop_caches = 3
node13 cache_cleared 17437952kB to 188736kB
node02 cache_cleared 21384896kB to 260096kB
node01 cache_cleared 24178112kB to 276928kB
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
node18 cache_cleared 17218816kB to 267456kB
node14 cache_cleared 17299328kB to 278400kB
node03 cache_cleared 22180672kB to 197440kB
vm.drop_caches = 3
node05 cache_cleared 19257984kB to 299008kB
+ srun --ntasks-per-node=1 --container-name=llama31_8b_208 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_common.callbacks import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
:::MLLOG {"namespace": "", "time_ms": 1759985783085, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783149, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783235, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783235, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783241, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783246, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783249, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783254, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783263, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783272, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783308, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783335, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783341, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783361, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783380, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783494, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1759985783766, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1759985784'
RUNANDTIME_START 1759985784
+ SLURM_HOSTFILE=/mlperf/scratch/llama_31_8b/log2/hostfile.208.ten7
+ NV_MLPERF_DEBUG=1
+ srun -l --mpi=pmix --ntasks-per-node=4 --distribution=arbitrary --time=30 --container-name=llama31_8b_208 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/mlperf/scratch/llama_31_8b/log2:/results,/mlperf/scratch/llama_31_8b/log2/251009011334405739666_npy_index:/npy_index,/mlperf/scratch/llama_31_8b/log2/mem_dump:/mem_dump,/mlperf/scratch/llama_31_8b/8b/tokenizer:/workspace/llm/nemo_tokenizer:ro,/mlperf/scratch/llama_31_8b/8b:/preproc_data:ro --container-workdir=/workspace/llm --container-env=MASTER_PORT,MASTER_ADDR,NCCL_SHARP_GROUP_SIZE_THRESH,NCCL_NVLS_ENABLE slurm2pytorch ./run_and_time.sh
31: LOAD_CHECKPOINT=
28: LOAD_CHECKPOINT=
28: Hello from: node08
30: LOAD_CHECKPOINT=
15: LOAD_CHECKPOINT=
29: LOAD_CHECKPOINT=
25: LOAD_CHECKPOINT=
12: LOAD_CHECKPOINT=
12: Hello from: node04
26: LOAD_CHECKPOINT=
14: LOAD_CHECKPOINT=
13: LOAD_CHECKPOINT=
20: LOAD_CHECKPOINT=
20: Hello from: node06
27: LOAD_CHECKPOINT=
24: LOAD_CHECKPOINT=
24: Hello from: node07
22: LOAD_CHECKPOINT=
21: LOAD_CHECKPOINT=
23: LOAD_CHECKPOINT=
18: LOAD_CHECKPOINT=
17: LOAD_CHECKPOINT=
16: LOAD_CHECKPOINT=
19: LOAD_CHECKPOINT=
16: Hello from: node05
59: LOAD_CHECKPOINT=
56: LOAD_CHECKPOINT=
56: Hello from: node15
58: LOAD_CHECKPOINT=
 0: slurm2pytorch: MASTER_ADDR=node01 MASTER_PORT=29500 WORLD_SIZE=72
57: LOAD_CHECKPOINT=
36: LOAD_CHECKPOINT=
36: Hello from: node10
45: LOAD_CHECKPOINT=
42: LOAD_CHECKPOINT=
32: LOAD_CHECKPOINT=
37: LOAD_CHECKPOINT=
32: Hello from: node09
47: LOAD_CHECKPOINT=
38: LOAD_CHECKPOINT=
33: LOAD_CHECKPOINT=
40: LOAD_CHECKPOINT=
41: LOAD_CHECKPOINT=
40: Hello from: node11
 3: LOAD_CHECKPOINT=
39: LOAD_CHECKPOINT=
65: LOAD_CHECKPOINT=
53: LOAD_CHECKPOINT=
44: LOAD_CHECKPOINT=
46: LOAD_CHECKPOINT=
43: LOAD_CHECKPOINT=
44: Hello from: node12
51: LOAD_CHECKPOINT=
 2: LOAD_CHECKPOINT=
34: LOAD_CHECKPOINT=
35: LOAD_CHECKPOINT=
67: LOAD_CHECKPOINT=
54: LOAD_CHECKPOINT=
50: LOAD_CHECKPOINT=
 0: LOAD_CHECKPOINT=
 1: LOAD_CHECKPOINT=
49: LOAD_CHECKPOINT=
48: LOAD_CHECKPOINT=
48: Hello from: node13
 0: Hello from: node01
 0: running LLM benchmark
 0: Extra args:  exp_manager.explicit_log_dir="/results/251009011334405739666"
62: LOAD_CHECKPOINT=
66: LOAD_CHECKPOINT=
 7: LOAD_CHECKPOINT=
64: LOAD_CHECKPOINT=
12: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
13: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
14: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
15: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
64: Hello from: node17
10: LOAD_CHECKPOINT=
52: LOAD_CHECKPOINT=
55: LOAD_CHECKPOINT=
52: Hello from: node14
60: LOAD_CHECKPOINT=
60: Hello from: node16
 8: LOAD_CHECKPOINT=
 8: Hello from: node03
61: LOAD_CHECKPOINT=
 6: LOAD_CHECKPOINT=
63: LOAD_CHECKPOINT=
20: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
21: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
22: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
23: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
11: LOAD_CHECKPOINT=
24: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
25: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
26: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
27: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
68: LOAD_CHECKPOINT=
68: Hello from: node18
 9: LOAD_CHECKPOINT=
 4: LOAD_CHECKPOINT=
 5: LOAD_CHECKPOINT=
 4: Hello from: node02
28: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
30: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
31: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
29: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
71: LOAD_CHECKPOINT=
70: LOAD_CHECKPOINT=
69: LOAD_CHECKPOINT=
16: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
17: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
18: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
19: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
57: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
45: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
56: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
58: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
59: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
42: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
41: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
40: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
47: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
43: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
44: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
46: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
32: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
33: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
65: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
52: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
55: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
53: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
54: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
36: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
35: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
34: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
37: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
38: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
39: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
62: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
67: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
50: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
48: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
49: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
51: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 0: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
10: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
64: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
66: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
61: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 2: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 1: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 3: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
60: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 4: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 5: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 6: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 7: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
63: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
11: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
68: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
70: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 8: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
69: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
 9: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
71: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=72
12: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
12:   import pynvml  # type: ignore[import]
13: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
13:   import pynvml  # type: ignore[import]
14: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
14:   import pynvml  # type: ignore[import]
15: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
15:   import pynvml  # type: ignore[import]
20: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
20:   import pynvml  # type: ignore[import]
21: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
21:   import pynvml  # type: ignore[import]
24: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
24:   import pynvml  # type: ignore[import]
25: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
25:   import pynvml  # type: ignore[import]
23: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
23:   import pynvml  # type: ignore[import]
22: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
22:   import pynvml  # type: ignore[import]
26: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
26:   import pynvml  # type: ignore[import]
27: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
27:   import pynvml  # type: ignore[import]
30: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
30:   import pynvml  # type: ignore[import]
31: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
31:   import pynvml  # type: ignore[import]
28: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
28:   import pynvml  # type: ignore[import]
29: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
29:   import pynvml  # type: ignore[import]
16: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
16:   import pynvml  # type: ignore[import]
17: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
17:   import pynvml  # type: ignore[import]
18: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
18:   import pynvml  # type: ignore[import]
19: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
19:   import pynvml  # type: ignore[import]
59: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
59:   import pynvml  # type: ignore[import]
58: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
58:   import pynvml  # type: ignore[import]
45: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
45:   import pynvml  # type: ignore[import]
44: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
44:   import pynvml  # type: ignore[import]
56: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
56:   import pynvml  # type: ignore[import]
57: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
57:   import pynvml  # type: ignore[import]
40: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
40:   import pynvml  # type: ignore[import]
41: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
41:   import pynvml  # type: ignore[import]
47: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
47:   import pynvml  # type: ignore[import]
46: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
46:   import pynvml  # type: ignore[import]
42: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
42:   import pynvml  # type: ignore[import]
43: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
43:   import pynvml  # type: ignore[import]
35: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
35:   import pynvml  # type: ignore[import]
34: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
34:   import pynvml  # type: ignore[import]
52: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
52:   import pynvml  # type: ignore[import]
53: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
53:   import pynvml  # type: ignore[import]
33: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
33:   import pynvml  # type: ignore[import]
32: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
32:   import pynvml  # type: ignore[import]
38: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
38:   import pynvml  # type: ignore[import]
39: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
39:   import pynvml  # type: ignore[import]
54: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
54:   import pynvml  # type: ignore[import]
55: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
55:   import pynvml  # type: ignore[import]
37: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
37:   import pynvml  # type: ignore[import]
36: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
36:   import pynvml  # type: ignore[import]
67: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
67:   import pynvml  # type: ignore[import]
66: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
66:   import pynvml  # type: ignore[import]
64: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
64:   import pynvml  # type: ignore[import]
65: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
65:   import pynvml  # type: ignore[import]
60: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
60:   import pynvml  # type: ignore[import]
61: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
61:   import pynvml  # type: ignore[import]
48: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
48:   import pynvml  # type: ignore[import]
62: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
62:   import pynvml  # type: ignore[import]
49: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
49:   import pynvml  # type: ignore[import]
63: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
63:   import pynvml  # type: ignore[import]
10: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
10:   import pynvml  # type: ignore[import]
11: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
11:   import pynvml  # type: ignore[import]
50: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
50:   import pynvml  # type: ignore[import]
51: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
51:   import pynvml  # type: ignore[import]
 0: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 0:   import pynvml  # type: ignore[import]
 1: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 1:   import pynvml  # type: ignore[import]
 6: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 6:   import pynvml  # type: ignore[import]
 7: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 7:   import pynvml  # type: ignore[import]
 4: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 4:   import pynvml  # type: ignore[import]
 3: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 3:   import pynvml  # type: ignore[import]
 2: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 2:   import pynvml  # type: ignore[import]
 5: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 5:   import pynvml  # type: ignore[import]
 8: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 8:   import pynvml  # type: ignore[import]
 9: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 9:   import pynvml  # type: ignore[import]
70: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
70:   import pynvml  # type: ignore[import]
71: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
71:   import pynvml  # type: ignore[import]
68: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
68:   import pynvml  # type: ignore[import]
69: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
69:   import pynvml  # type: ignore[import]
28: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
29: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
30: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
31: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
14: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
15: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
12: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
13: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
22: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
23: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
20: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
21: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
26: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
24: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
25: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
27: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
16: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
17: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
18: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
19: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
52: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
53: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
54: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
55: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
44: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
45: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
47: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
46: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
38: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
39: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
36: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
37: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
58: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
59: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
56: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
57: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 4: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 5: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 6: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 7: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
43: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
41: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
42: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
40: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
62: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
60: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
61: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
63: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
48: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
49: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
50: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
51: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
67: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
64: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
65: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
66: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
69: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
70: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
71: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
68: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 9: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
10: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
11: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 8: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 2: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 3: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 0: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 1: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
34: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
35: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
32: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
33: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 744}}
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] 
 0:     
 0:     **************** Experiment configuration ****************
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] 
 0:     model:
 0:       data:
 0:         data_prefix:
 0:           train:
 0:           - 0.5
 0:           - /preproc_data/c4_en_6_c4_spm_text_document
 0:           - 0.5
 0:           - /preproc_data/c4_en_7_c4_spm_text_document
 0:           validation:
 0:           - /preproc_data/c4_en_validation_subset_c4_spm_text_document
 0:           test:
 0:           - /preproc_data/c4_en_validation_subset_c4_spm_text_document
 0:         index_mapping_dir: /npy_index
 0:         splits_string: null
 0:         validation_drop_last: false
 0:         pad_samples_to_global_batch_size: true
 0:         shuffle_documents: false
 0:         legacy_dataset: true
 0:         delay_data_init: true
 0:         delay_data_mmap: true
 0:         no_seqlen_plus_one_input_tokens: true
 0:         exchange_indices_distributed: true
 0:         mock_dataset: false
 0:         mock_tokenizer_vocab_size: 32000
 0:       mcore_gpt: true
 0:       name: megatron_gpt_full_te_layer_autocast
 0:       micro_batch_size: 1
 0:       tensor_model_parallel_size: 1
 0:       pipeline_model_parallel_size: 1
 0:       virtual_pipeline_model_parallel_size: null
 0:       context_parallel_size: 2
 0:       expert_model_parallel_size: 1
 0:       global_batch_size: 36
 0:       use_tp_pp_dp_mapping: true
 0:       base_config: 8b
 0:       overwritten_attributes:
 0:         num_layers: 32
 0:         enable_cuda_graph: 1
 0:         cuda_graph_scope: full_iteration
 0:       encoder_seq_length: 8192
 0:       overlap_p2p_comm: true
 0:       batch_p2p_comm: false
 0:       account_for_embedding_in_pipeline_split: false
 0:       account_for_loss_in_pipeline_split: false
 0:       external_cuda_graph: false
 0:       defer_embedding_wgrad_compute: false
 0:       wgrad_deferral_limit: 50
 0:       tokenizer:
 0:         model: /workspace/llm/nemo_tokenizer
 0:       gradient_accumulation_fusion: true
 0:       fused_single_qkv_rope: true
 0:       cross_entropy_loss_fusion: true
 0:       deterministic_mode: false
 0:       seed: 17758
 0:       resume_from_checkpoint: null
 0:       dist_ckpt_format: torch_dist
 0:       dist_ckpt_parallel_load: true
 0:       sync_batch_comm: false
 0:       activations_checkpoint_granularity: null
 0:       activations_checkpoint_method: null
 0:       activations_checkpoint_num_layers: null
 0:       sequence_parallel: false
 0:       transformer_engine: true
 0:       fp8: true
 0:       fp8_hybrid: false
 0:       fp8_recipe: tensorwise
 0:       fp8_amax_history_len: 1
 0:       fp8_amax_compute_algo: most_recent
 0:       fp4: false
 0:       fp4_recipe: nvfp4
 0:       reduce_amax: true
 0:       tp_only_amax_red: true
 0:       first_last_layers_bf16: false
 0:       num_layers_at_start_in_bf16: 0
 0:       num_layers_at_end_in_bf16: 0
 0:       use_te_rng_tracker: true
 0:       use_transformer_engine_op_fuser: true
 0:       cross_entropy_fusion_impl: te
 0:       ub_tp_comm_overlap: false
 0:       tp_comm_overlap_ag: true
 0:       tp_comm_overlap_rs: true
 0:       nccl_communicator_config_path: /workspace/llm/conf/nccl/custom_communicator_cta.yaml
 0:       sharp: false
 0:       optim:
 0:         overlap_grad_reduce: true
 0:         overlap_param_gather: true
 0:         align_param_gather: false
 0:         use_distributed_optimizer: true
 0:         bucket_size: 200
 0:         fp8_param_gather: true
 0:         overlap_param_gather_with_optim_step: false
 0:         lr: 0.0008
 0:         sched:
 0:           min_lr: 8.0e-05
 0:           warmup_steps: 96
 0:           max_steps_for_lr_sched: 38400000.0
 0:         lock_timeout: null
 0:       gc_interval_train: 10000
 0:       gc_interval_valid: 10000
 0:       nsys_profile:
 0:         enabled: false
 0:         start_step: 10
 0:         end_step: 10
 0:         ranks:
 0:         - 0
 0:         gen_shape: false
 0:         nvtx_ranges: false
 0:       custom:
 0:         log_metrics: NEMO
 0:         init_global_step: 0
 0:         target_log_ppl: 3.3
 0:         use_distributed_checkpointing: 1
 0:         run_warmup_on_synth_data: 1
 0:         reset_fp8_stats_after_warmup: 1
 0:         pre_validate: 0
 0:         override_zero_consumed_samples: 1
 0:         force_success_status: 0
 0:         warmup_train_steps: 2
 0:         warmup_validation_steps: 2
 0:         extend_run_evals: 0
 0:         disable_nemo_logs: true
 0:     proxy_gbs: 36
 0:     is_proxy_run: false
 0:     skip_evals: 12
 0:     default_val_check_interval: 342
 0:     trainer:
 0:       devices: 4
 0:       num_nodes: 18
 0:       precision: bf16
 0:       max_steps: 1200000
 0:       max_epochs: 1
 0:       log_every_n_steps: 32
 0:       val_check_interval: 342
 0:       limit_val_batches: 29
 0:       limit_test_batches: 1
 0:       limit_train_batches: null
 0:       enable_progress_bar: false
 0:       num_sanity_val_steps: 0
 0:     exp_manager:
 0:       explicit_log_dir: /results/251009011334405739666
 0:       resume_if_exists: 0
 0:       create_checkpoint_callback: 0
 0:       checkpoint_callback_params:
 0:         save_top_k: 1
 0:         mode: max
 0:         every_n_epochs: 0
 0:         save_last: true
 0:       log_step_timing: true
 0:       create_tensorboard_logger: false
 0:       log_global_rank_0_only: true
 0:     misc:
 0:       print_config: false
 0:       memory_profiler:
 0:         enable: false
 0:         file_prefix: memdump
 0:         max_entries: 1000000
 0:         rank_0_only: true
 0:         start_location: init
 0:         end_location: train_start
 0:         force_oom_before_stop: false
 0:         possible_oom: false
 0:     
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] 
 0:     TP: 1; PP: 1; VP: None; CP: 2
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] ======== Benchmarked setups ========
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: HPU available: False, using: 0 HPUs
 0: `Trainer(limit_test_batches=1)` was configured so 1 batch will be used.
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama31_8b", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 561}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 561}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 561}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 561}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "18xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 561}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "seed", "value": 17758, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798959, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 36, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798960, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1.0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798960, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 8192, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798960, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 1024, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798960, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1574207408, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798960, "event_type": "POINT_IN_TIME", "key": "init_checkpoint_step", "value": 0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798961, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798961, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0008, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798961, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798961, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.95, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798961, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-05, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798961, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.1, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 1.0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "opt_end_learning_rate", "value": 8e-05, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 96, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 1199904, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "max_steps", "value": 1200000, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_schedule", "value": "cosine with linear warmup", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985798962, "event_type": "POINT_IN_TIME", "key": "target_accuracy", "value": 3.3, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 595}}
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] ======== Benchmarked fit ========
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Experiments will be logged at /workspace/llm/nemo_experiments/default/2025-10-09_04-56-38
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has data parallel group : [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has context parallel group: [0, 1]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23], [24, 25], [26, 27], [28, 29], [30, 31], [32, 33], [34, 35], [36, 37], [38, 39], [40, 41], [42, 43], [44, 45], [46, 47], [48, 49], [50, 51], [52, 53], [54, 55], [56, 57], [58, 59], [60, 61], [62, 63], [64, 65], [66, 67], [68, 69], [70, 71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has model parallel group: [0]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All expert tensor parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23], [24, 25], [26, 27], [28, 29], [30, 31], [32, 33], [34, 35], [36, 37], [38, 39], [40, 41], [42, 43], [44, 45], [46, 47], [48, 49], [50, 51], [52, 53], [54, 55], [56, 57], [58, 59], [60, 61], [62, 63], [64, 65], [66, 67], [68, 69], [70, 71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has expert tensor parallel rank: 0
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has embedding group: [0]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71]]
 0: [NeMo I 2025-10-09 04:56:38 nemo_logging:393] Rank 0 has embedding rank: 0
 0: [AUX I 2025-10-09 04:56:38 megatron_strategy:607] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/72
 1: [W1009 04:56:38.132097712 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 2: [W1009 04:56:38.132112785 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 3: [W1009 04:56:38.136225035 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
34: [W1009 04:56:39.180999832 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
35: [W1009 04:56:39.181020184 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
32: [W1009 04:56:39.181094009 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
33: [W1009 04:56:39.182829707 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
20: [W1009 04:56:39.226421396 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
26: [W1009 04:56:39.719889252 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
14: [W1009 04:56:39.634306866 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
22: [W1009 04:56:39.253986435 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
40: [W1009 04:56:39.486252842 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
57: [W1009 04:56:39.380027253 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
63: [W1009 04:56:39.341268743 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
55: [W1009 04:56:39.123978859 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
61: [W1009 04:56:39.372283778 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
45: [W1009 04:56:39.426248106 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
27: [W1009 04:56:39.855754726 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
58: [W1009 04:56:39.442230864 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
37: [W1009 04:56:39.096089332 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
39: [W1009 04:56:39.100253311 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 6: [W1009 04:56:39.339676232 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
46: [W1009 04:56:39.449112429 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
24: [W1009 04:56:39.935208689 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
15: [W1009 04:56:39.857529751 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 5: [W1009 04:56:39.428724328 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
64: [W1009 04:56:39.927289200 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
70: [W1009 04:56:39.230961605 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
18: [W1009 04:56:39.987367417 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
43: [W1009 04:56:39.745098192 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 7: [W1009 04:56:39.477592554 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
49: [W1009 04:56:39.590223444 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
41: [W1009 04:56:39.761312638 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
30: [W1009 04:56:39.832839353 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
44: [W1009 04:56:39.639191363 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
42: [W1009 04:56:39.806754100 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
68: [W1009 04:56:39.314106046 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
66: [W1009 04:56:39.025270790 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
19: [W1009 04:56:39.077422265 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
38: [W1009 04:56:39.312512951 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
56: [W1009 04:56:39.665528762 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
11: [W1009 04:56:39.886341347 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
52: [W1009 04:56:39.393063371 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
36: [W1009 04:56:39.349293679 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
51: [W1009 04:56:39.712419539 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
28: [W1009 04:56:39.916857464 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
25: [W1009 04:56:39.172471400 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
71: [W1009 04:56:39.417928068 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
16: [W1009 04:56:39.170085388 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
48: [W1009 04:56:39.766411234 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
60: [W1009 04:56:39.773204619 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
10: [W1009 04:56:39.042269938 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
65: [W1009 04:56:39.203809497 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 8: [W1009 04:56:39.044171299 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
23: [W1009 04:56:39.766171591 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
62: [W1009 04:56:39.780126043 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
31: [W1009 04:56:39.047871770 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
67: [W1009 04:56:39.239344815 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
50: [W1009 04:56:39.895377887 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
69: [W1009 04:56:39.595988965 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
17: [W1009 04:56:39.361457057 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
12: [W1009 04:56:39.290642705 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 9: [W1009 04:56:39.170105126 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
21: [W1009 04:56:39.924093092 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
54: [W1009 04:56:39.725575674 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
29: [W1009 04:56:39.206787514 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
59: [W1009 04:56:39.026898247 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
53: [W1009 04:56:39.833042175 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
47: [W1009 04:56:39.120738961 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 4: [W1009 04:56:39.033935579 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
13: [W1009 04:56:40.078808910 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 0: [W1009 04:56:40.636781646 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 72 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: [Gloo] Rank 0 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 1: [Gloo] Rank 1 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 2: [Gloo] Rank 2 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 3: [Gloo] Rank 3 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 4: [Gloo] Rank 4 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 9: [Gloo] Rank 9 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 5: [Gloo] Rank 5 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 6: [Gloo] Rank 6 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 7: [Gloo] Rank 7 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 8: [Gloo] Rank 8 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
10: [Gloo] Rank 10 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
12: [Gloo] Rank 12 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
11: [Gloo] Rank 11 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
13: [Gloo] Rank 13 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
20: [Gloo] Rank 20 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
14: [Gloo] Rank 14 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
15: [Gloo] Rank 15 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
21: [Gloo] Rank 21 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
16: [Gloo] Rank 16 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
18: [Gloo] Rank 18 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
22: [Gloo] Rank 22 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
17: [Gloo] Rank 17 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
19: [Gloo] Rank 19 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
23: [Gloo] Rank 23 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
24: [Gloo] Rank 24 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
29: [Gloo] Rank 29 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
30: [Gloo] Rank 30 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
25: [Gloo] Rank 25 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
31: [Gloo] Rank 31 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
26: [Gloo] Rank 26 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
32: [Gloo] Rank 32 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
28: [Gloo] Rank 28 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
27: [Gloo] Rank 27 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
33: [Gloo] Rank 33 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
34: [Gloo] Rank 34 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
35: [Gloo] Rank 35 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
40: [Gloo] Rank 40 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
36: [Gloo] Rank 36 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
43: [Gloo] Rank 43 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
37: [Gloo] Rank 37 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
44: [Gloo] Rank 44 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
38: [Gloo] Rank 38 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
42: [Gloo] Rank 42 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
39: [Gloo] Rank 39 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
45: [Gloo] Rank 45 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
46: [Gloo] Rank 46 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
41: [Gloo] Rank 41 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
47: [Gloo] Rank 47 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
48: [Gloo] Rank 48 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
52: [Gloo] Rank 52 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
49: [Gloo] Rank 49 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
50: [Gloo] Rank 50 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
53: [Gloo] Rank 53 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
51: [Gloo] Rank 51 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
54: [Gloo] Rank 54 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
55: [Gloo] Rank 55 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
56: [Gloo] Rank 56 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
57: [Gloo] Rank 57 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
58: [Gloo] Rank 58 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
59: [Gloo] Rank 59 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
60: [Gloo] Rank 60 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
69: [Gloo] Rank 69 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
70: [Gloo] Rank 70 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
61: [Gloo] Rank 61 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
62: [Gloo] Rank 62 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
68: [Gloo] Rank 68 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
63: [Gloo] Rank 63 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
71: [Gloo] Rank 71 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
64: [Gloo] Rank 64 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
65: [Gloo] Rank 65 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
66: [Gloo] Rank 66 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
67: [Gloo] Rank 67 is connected to 71 peer ranks. Expected number of connected peer ranks is : 71
 2: [Gloo] Rank 1 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 1: [Gloo] Rank 0 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 3: [Gloo] Rank 1 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 4: [Gloo] Rank 2 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 5: [Gloo] Rank 2 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 6: [Gloo] Rank 3 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 7: [Gloo] Rank 3 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 8: [Gloo] Rank 4 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
12: [Gloo] Rank 6 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
10: [Gloo] Rank 5 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
14: [Gloo] Rank 7 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
16: [Gloo] Rank 8 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 9: [Gloo] Rank 4 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
13: [Gloo] Rank 6 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
11: [Gloo] Rank 5 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
18: [Gloo] Rank 9 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
15: [Gloo] Rank 7 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
17: [Gloo] Rank 8 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
20: [Gloo] Rank 10 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
19: [Gloo] Rank 9 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
21: [Gloo] Rank 10 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
22: [Gloo] Rank 11 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
24: [Gloo] Rank 12 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
23: [Gloo] Rank 11 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
25: [Gloo] Rank 12 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
26: [Gloo] Rank 13 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
28: [Gloo] Rank 14 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
27: [Gloo] Rank 13 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
29: [Gloo] Rank 14 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
30: [Gloo] Rank 15 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
32: [Gloo] Rank 16 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
31: [Gloo] Rank 15 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
33: [Gloo] Rank 16 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
34: [Gloo] Rank 17 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
36: [Gloo] Rank 18 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
35: [Gloo] Rank 17 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
40: [Gloo] Rank 20 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
37: [Gloo] Rank 18 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
42: [Gloo] Rank 21 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
38: [Gloo] Rank 19 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
41: [Gloo] Rank 20 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
39: [Gloo] Rank 19 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
44: [Gloo] Rank 22 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
43: [Gloo] Rank 21 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
46: [Gloo] Rank 23 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
45: [Gloo] Rank 22 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
48: [Gloo] Rank 24 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
47: [Gloo] Rank 23 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
52: [Gloo] Rank 26 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
49: [Gloo] Rank 24 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
50: [Gloo] Rank 25 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
51: [Gloo] Rank 25 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
53: [Gloo] Rank 26 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
69: [Gloo] Rank 34 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
54: [Gloo] Rank 27 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
56: [Gloo] Rank 28 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
67: [Gloo] Rank 33 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
55: [Gloo] Rank 27 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
60: [Gloo] Rank 30 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
57: [Gloo] Rank 28 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
71: [Gloo] Rank 35 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
58: [Gloo] Rank 29 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
62: [Gloo] Rank 31 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
64: [Gloo] Rank 32 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
59: [Gloo] Rank 29 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
61: [Gloo] Rank 30 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
68: [Gloo] Rank 34 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
65: [Gloo] Rank 32 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
63: [Gloo] Rank 31 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
70: [Gloo] Rank 35 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
66: [Gloo] Rank 33 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 0: [Gloo] Rank 0 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
69: [Gloo] Rank 34 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
71: [Gloo] Rank 35 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
67: [Gloo] Rank 33 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 1: [Gloo] Rank 0 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 3: [Gloo] Rank 1 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 5: [Gloo] Rank 2 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 7: [Gloo] Rank 3 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 9: [Gloo] Rank 4 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
11: [Gloo] Rank 5 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
13: [Gloo] Rank 6 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
15: [Gloo] Rank 7 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
17: [Gloo] Rank 8 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
19: [Gloo] Rank 9 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
21: [Gloo] Rank 10 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
23: [Gloo] Rank 11 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
25: [Gloo] Rank 12 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
53: [Gloo] Rank 26 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
27: [Gloo] Rank 13 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
29: [Gloo] Rank 14 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
55: [Gloo] Rank 27 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
31: [Gloo] Rank 15 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
57: [Gloo] Rank 28 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
33: [Gloo] Rank 16 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
59: [Gloo] Rank 29 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
35: [Gloo] Rank 17 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
37: [Gloo] Rank 18 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
65: [Gloo] Rank 32 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
61: [Gloo] Rank 30 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
39: [Gloo] Rank 19 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
41: [Gloo] Rank 20 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
63: [Gloo] Rank 31 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
43: [Gloo] Rank 21 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
45: [Gloo] Rank 22 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
47: [Gloo] Rank 23 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
49: [Gloo] Rank 24 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
51: [Gloo] Rank 25 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 2: [Gloo] Rank 1 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 0: [Gloo] Rank 0 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 4: [Gloo] Rank 2 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
10: [Gloo] Rank 5 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 6: [Gloo] Rank 3 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 8: [Gloo] Rank 4 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
12: [Gloo] Rank 6 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
14: [Gloo] Rank 7 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
16: [Gloo] Rank 8 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
18: [Gloo] Rank 9 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
20: [Gloo] Rank 10 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
22: [Gloo] Rank 11 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
24: [Gloo] Rank 12 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
34: [Gloo] Rank 17 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
26: [Gloo] Rank 13 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
36: [Gloo] Rank 18 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
28: [Gloo] Rank 14 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
38: [Gloo] Rank 19 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
32: [Gloo] Rank 16 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
30: [Gloo] Rank 15 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
40: [Gloo] Rank 20 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
42: [Gloo] Rank 21 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
44: [Gloo] Rank 22 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
46: [Gloo] Rank 23 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
48: [Gloo] Rank 24 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
50: [Gloo] Rank 25 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
52: [Gloo] Rank 26 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
54: [Gloo] Rank 27 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
56: [Gloo] Rank 28 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
68: [Gloo] Rank 34 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
58: [Gloo] Rank 29 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
70: [Gloo] Rank 35 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
60: [Gloo] Rank 30 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
62: [Gloo] Rank 31 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
64: [Gloo] Rank 32 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
66: [Gloo] Rank 33 is connected to 35 peer ranks. Expected number of connected peer ranks is : 35
 0: [NeMo I 2025-10-09 04:56:43 utils:662] Building GPTDataset splits with sizes=[43200000, 3663396, 36] and config=GPTDatasetConfig(random_seed=17758, sequence_length=8192, blend=None, blend_per_split=[(['/preproc_data/c4-train.en_6_text_document'], [50.0]), (['/preproc_data/c4-validation-91205-samples.en_text_document'], None), (['/preproc_data/c4-validation-91205-samples.en_text_document'], None)], multiple_validation_sets=None, full_validation=None, split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache='/npy_index', mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0xe7a04b5f5d00>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
 0: [NeMo I 2025-10-09 04:56:43 utils:662] Load the _IndexReader from /preproc_data/c4-train.en_6_text_document.idx
 0: [NeMo I 2025-10-09 04:56:43 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 04:56:43 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 04:56:43 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 04:56:43 utils:662] > total number of sequences: 45608611
 0: [NeMo I 2025-10-09 04:56:43 utils:662] > total number of documents: 45608611
 0: [NeMo I 2025-10-09 04:56:43 utils:662] Build and save the GPTDataset train indices
 0: [NeMo I 2025-10-09 04:57:55 utils:662] > total number of samples: 43567052
 0: [NeMo I 2025-10-09 04:57:55 utils:662] > total number of epochs: 17
 0: [NeMo I 2025-10-09 04:57:55 utils:662] Load the _IndexReader from /preproc_data/c4-validation-91205-samples.en_text_document.idx
 0: [NeMo I 2025-10-09 04:57:55 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 04:57:55 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 04:57:55 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 04:57:55 utils:662] > total number of sequences: 91205
 0: [NeMo I 2025-10-09 04:57:55 utils:662] > total number of documents: 91205
 0: [NeMo I 2025-10-09 04:57:55 utils:662] Build and save the GPTDataset valid indices
 0: [NeMo I 2025-10-09 04:57:59 utils:662] > total number of samples: 3665032
 0: [NeMo I 2025-10-09 04:57:59 utils:662] > total number of epochs: 721
 0: [NeMo I 2025-10-09 04:57:59 utils:662] Load the _IndexReader from /preproc_data/c4-validation-91205-samples.en_text_document.idx
 0: [NeMo I 2025-10-09 04:57:59 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 04:57:59 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 04:57:59 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 04:57:59 utils:662] > total number of sequences: 91205
 0: [NeMo I 2025-10-09 04:57:59 utils:662] > total number of documents: 91205
 0: [NeMo I 2025-10-09 04:57:59 utils:662] Build and save the GPTDataset test indices
 0: [NeMo I 2025-10-09 04:57:59 utils:662] > total number of samples: 5083
 0: [NeMo I 2025-10-09 04:57:59 utils:662] > total number of epochs: 1
 0: [NeMo W 2025-10-09 04:57:59 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=32 for best performance                         but get 1
 0: [NeMo I 2025-10-09 04:57:59 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 0: [NeMo I 2025-10-09 04:58:03 nemo_logging:393] Apply rope scaling with factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, old_context_len=8192.
 7: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 0: [NeMo I 2025-10-09 04:58:03 nemo_logging:393] Copying Trainer's 'max_steps' (1200000) to LR scheduler's 'max_steps'.
 0: [NeMo I 2025-10-09 04:58:03 num_microbatches_calculator:228] setting number of microbatches to constant 1
 0: [NeMo I 2025-10-09 04:58:03 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 8030261248
 0: [NeMo I 2025-10-09 04:58:03 utils:662] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=134217728, pad_buckets_for_high_nccl_busbw=False, average_in_collective=True, fp8_param_gather=True, reuse_grad_buf_for_mxfp8_param_ag=False, use_megatron_fsdp=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache=False, nccl_ub=False, fsdp_double_buffer=False, outer_dp_sharding_strategy='no_shard', disable_symmetric_registration=False, delay_wgrad_compute=False)
52: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
53: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
54: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
55: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
67: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
66: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
64: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
65: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
21: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
20: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
22: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
23: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
30: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
31: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
28: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
29: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 4: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 5: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 6: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
39: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
38: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
36: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
37: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
68: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
69: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
70: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
71: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
45: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
46: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
47: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
44: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
12: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
15: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
14: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
13: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
62: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
63: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
60: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
61: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 0: [NeMo I 2025-10-09 04:58:04 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 2
 0:     Params for bucket 1 (525336576 elements, 525337344 padded size):
 0:     	module.output_layer.weight
 0:     Params for bucket 2 (525602816 elements, 525603456 padded size):
 0:     	module.decoder.final_layernorm.weight
 0:     	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
 0:     	module.embedding.word_embeddings.weight
 0:     	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
 0: [NeMo I 2025-10-09 04:58:08 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 33
 0:     Params for bucket 1 (176160768 elements, 176161536 padded size):
 0:     	module.decoder.layers.31.mlp.linear_fc2.weight
 0:     	module.decoder.layers.31.mlp.linear_fc1.weight
 0:     Params for bucket 2 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.31.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.30.mlp.linear_fc2.weight
 0:     	module.decoder.layers.30.mlp.linear_fc1.weight
 0:     	module.decoder.layers.31.self_attention.linear_proj.weight
 0:     Params for bucket 3 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.30.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.30.self_attention.linear_proj.weight
 0:     	module.decoder.layers.29.mlp.linear_fc2.weight
 0:     	module.decoder.layers.29.mlp.linear_fc1.weight
 0:     Params for bucket 4 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.29.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.28.mlp.linear_fc2.weight
 0:     	module.decoder.layers.28.mlp.linear_fc1.weight
 0:     	module.decoder.layers.29.self_attention.linear_proj.weight
 0:     Params for bucket 5 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.28.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.27.mlp.linear_fc1.weight
 0:     	module.decoder.layers.28.self_attention.linear_proj.weight
 0:     	module.decoder.layers.27.mlp.linear_fc2.weight
 0:     Params for bucket 6 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.27.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.27.self_attention.linear_proj.weight
 0:     	module.decoder.layers.26.mlp.linear_fc1.weight
 0:     	module.decoder.layers.26.mlp.linear_fc2.weight
 0:     Params for bucket 7 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.26.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.25.mlp.linear_fc1.weight
 0:     	module.decoder.layers.26.self_attention.linear_proj.weight
 0:     	module.decoder.layers.25.mlp.linear_fc2.weight
 0:     Params for bucket 8 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.25.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.24.mlp.linear_fc2.weight
 0:     	module.decoder.layers.24.mlp.linear_fc1.weight
 0:     	module.decoder.layers.25.self_attention.linear_proj.weight
 0:     Params for bucket 9 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.24.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.23.mlp.linear_fc1.weight
 0:     	module.decoder.layers.24.self_attention.linear_proj.weight
 0:     	module.decoder.layers.23.mlp.linear_fc2.weight
 0:     Params for bucket 10 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.23.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.22.mlp.linear_fc2.weight
 0:     	module.decoder.layers.23.self_attention.linear_proj.weight
 0:     	module.decoder.layers.22.mlp.linear_fc1.weight
 0:     Params for bucket 11 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.22.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.21.mlp.linear_fc1.weight
 0:     	module.decoder.layers.22.self_attention.linear_proj.weight
 0:     	module.decoder.layers.21.mlp.linear_fc2.weight
 0:     Params for bucket 12 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.21.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.20.mlp.linear_fc1.weight
 0:     	module.decoder.layers.21.self_attention.linear_proj.weight
 0:     	module.decoder.layers.20.mlp.linear_fc2.weight
 0:     Params for bucket 13 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.20.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.19.mlp.linear_fc1.weight
 0:     	module.decoder.layers.20.self_attention.linear_proj.weight
 0:     	module.decoder.layers.19.mlp.linear_fc2.weight
 0:     Params for bucket 14 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.19.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.18.mlp.linear_fc1.weight
 0:     	module.decoder.layers.19.self_attention.linear_proj.weight
 0:     	module.decoder.layers.18.mlp.linear_fc2.weight
 0:     Params for bucket 15 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.18.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.17.mlp.linear_fc2.weight
 0:     	module.decoder.layers.17.mlp.linear_fc1.weight
 0:     	module.decoder.layers.18.self_attention.linear_proj.weight
 0:     Params for bucket 16 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.17.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.16.mlp.linear_fc1.weight
 0:     	module.decoder.layers.17.self_attention.linear_proj.weight
 0:     	module.decoder.layers.16.mlp.linear_fc2.weight
 0:     Params for bucket 17 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.16.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.15.mlp.linear_fc2.weight
 0:     	module.decoder.layers.15.mlp.linear_fc1.weight
 0:     	module.decoder.layers.16.self_attention.linear_proj.weight
 0:     Params for bucket 18 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.15.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.14.mlp.linear_fc1.weight
 0:     	module.decoder.layers.15.self_attention.linear_proj.weight
 0:     	module.decoder.layers.14.mlp.linear_fc2.weight
 0:     Params for bucket 19 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.14.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.14.self_attention.linear_proj.weight
 0:     	module.decoder.layers.13.mlp.linear_fc2.weight
 0:     	module.decoder.layers.13.mlp.linear_fc1.weight
 0:     Params for bucket 20 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.13.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.12.mlp.linear_fc1.weight
 0:     	module.decoder.layers.13.self_attention.linear_proj.weight
 0:     	module.decoder.layers.12.mlp.linear_fc2.weight
 0:     Params for bucket 21 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.12.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.12.self_attention.linear_proj.weight
 0:     	module.decoder.layers.11.mlp.linear_fc2.weight
 0:     	module.decoder.layers.11.mlp.linear_fc1.weight
 0:     Params for bucket 22 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.11.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.10.mlp.linear_fc2.weight
 0:     	module.decoder.layers.10.mlp.linear_fc1.weight
 0:     	module.decoder.layers.11.self_attention.linear_proj.weight
 0:     Params for bucket 23 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.10.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.10.self_attention.linear_proj.weight
 0:     	module.decoder.layers.9.mlp.linear_fc2.weight
 0:     	module.decoder.layers.9.mlp.linear_fc1.weight
 0:     Params for bucket 24 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.9.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.8.mlp.linear_fc2.weight
 0:     	module.decoder.layers.8.mlp.linear_fc1.weight
 0:     	module.decoder.layers.9.self_attention.linear_proj.weight
 0:     Params for bucket 25 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.8.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.7.mlp.linear_fc1.weight
 0:     	module.decoder.layers.8.self_attention.linear_proj.weight
 0:     	module.decoder.layers.7.mlp.linear_fc2.weight
 0:     Params for bucket 26 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.7.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.6.mlp.linear_fc2.weight
 0:     	module.decoder.layers.6.mlp.linear_fc1.weight
 0:     	module.decoder.layers.7.self_attention.linear_proj.weight
 0:     Params for bucket 27 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.6.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.5.mlp.linear_fc1.weight
 0:     	module.decoder.layers.6.self_attention.linear_proj.weight
 0:     	module.decoder.layers.5.mlp.linear_fc2.weight
 0:     Params for bucket 28 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.5.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.4.mlp.linear_fc1.weight
 0:     	module.decoder.layers.5.self_attention.linear_proj.weight
 0:     	module.decoder.layers.4.mlp.linear_fc2.weight
 0:     Params for bucket 29 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.4.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.4.self_attention.linear_proj.weight
 0:     	module.decoder.layers.3.mlp.linear_fc1.weight
 0:     	module.decoder.layers.3.mlp.linear_fc2.weight
 0:     Params for bucket 30 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.3.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.2.mlp.linear_fc1.weight
 0:     	module.decoder.layers.3.self_attention.linear_proj.weight
 0:     	module.decoder.layers.2.mlp.linear_fc2.weight
 0:     Params for bucket 31 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.2.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.2.self_attention.linear_proj.weight
 0:     	module.decoder.layers.1.mlp.linear_fc1.weight
 0:     	module.decoder.layers.1.mlp.linear_fc2.weight
 0:     Params for bucket 32 (218103808 elements, 218104704 padded size):
 0:     	module.decoder.layers.1.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.0.mlp.linear_fc2.weight
 0:     	module.decoder.layers.1.self_attention.linear_proj.weight
 0:     	module.decoder.layers.0.mlp.linear_fc1.weight
 0:     Params for bucket 33 (41943040 elements, 41943168 padded size):
 0:     	module.decoder.layers.0.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.0.self_attention.linear_proj.weight
 0: [NeMo I 2025-10-09 04:58:08 utils:662] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0008, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='tensorwise', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_ti
 0: me=False, timers=None, config_logger_dir='')
59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 0: 
 0:   | Name   | Type | Params | Mode 
 0: ----------------------------------------
 0: 0 | module | DDP  | 8.0 B  | train
 0: ----------------------------------------
 0: 8.0 B     Trainable params
 0: 0         Non-trainable params
 0: 8.0 B     Total params
 0: 32,121.045Total estimated model params size (MB)
 0: 651       Modules in train mode
 0: 0         Modules in eval mode
 0: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
56: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
40: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
68: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
44: SLURM auto-requeueing enabled. Setting signal handlers.
52: SLURM auto-requeueing enabled. Setting signal handlers.
48: SLURM auto-requeueing enabled. Setting signal handlers.
36: SLURM auto-requeueing enabled. Setting signal handlers.
57: SLURM auto-requeueing enabled. Setting signal handlers.
32: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
64: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
60: SLURM auto-requeueing enabled. Setting signal handlers.
41: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
69: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
53: SLURM auto-requeueing enabled. Setting signal handlers.
49: SLURM auto-requeueing enabled. Setting signal handlers.
17: SLURM auto-requeueing enabled. Setting signal handlers.
45: SLURM auto-requeueing enabled. Setting signal handlers.
33: SLURM auto-requeueing enabled. Setting signal handlers.
37: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
58: SLURM auto-requeueing enabled. Setting signal handlers.
65: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
42: SLURM auto-requeueing enabled. Setting signal handlers.
70: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
61: SLURM auto-requeueing enabled. Setting signal handlers.
54: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
46: SLURM auto-requeueing enabled. Setting signal handlers.
34: SLURM auto-requeueing enabled. Setting signal handlers.
50: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
38: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
66: SLURM auto-requeueing enabled. Setting signal handlers.
71: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
43: SLURM auto-requeueing enabled. Setting signal handlers.
62: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
55: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
47: SLURM auto-requeueing enabled. Setting signal handlers.
35: SLURM auto-requeueing enabled. Setting signal handlers.
51: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
39: SLURM auto-requeueing enabled. Setting signal handlers.
67: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
63: SLURM auto-requeueing enabled. Setting signal handlers.
59: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [AUX I 2025-10-09 04:58:11 data:291] Instantiating MegatronPretrainingSampler with total_samples: 43567052 and consumed_samples: 0
 0: [AUX I 2025-10-09 04:58:12 data:291] Instantiating MegatronPretrainingSampler with total_samples: 3665032 and consumed_samples: 0
 5: GPTModel(
 5:   (embedding): LanguageModelEmbedding(
 5:     (word_embeddings): VocabParallelEmbedding()
 5:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 5:   )
 5:   (rotary_pos_emb): RotaryEmbedding()
 5:   (decoder): TransformerBlock(
 5:     (layers): ModuleList(
 5:       (0-31): 32 x TransformerLayer(
 5:         (input_layernorm): IdentityOp()
 5:         (self_attention): SelfAttention(
 5:           (core_attention): TEDotProductAttention(
 5:             (flash_attention): FlashAttention()
 5:             (fused_attention): FusedAttention()
 5:             (unfused_attention): UnfusedDotProductAttention(
 5:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 5:               (attention_dropout): Dropout(p=0.0, inplace=False)
 5:             )
 5:           )
 5:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 5:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 5:           (q_layernorm): IdentityOp()
 5:           (k_layernorm): IdentityOp()
41: GPTModel(
41:   (embedding): LanguageModelEmbedding(
41:     (word_embeddings): VocabParallelEmbedding()
41:     (embedding_dropout): Dropout(p=0.0, inplace=False)
41:   )
41:   (rotary_pos_emb): RotaryEmbedding()
41:   (decoder): TransformerBlock(
41:     (layers): ModuleList(
41:       (0-31): 32 x TransformerLayer(
41:         (input_layernorm): IdentityOp()
41:         (self_attention): SelfAttention(
41:           (core_attention): TEDotProductAttention(
41:             (flash_attention): FlashAttention()
41:             (fused_attention): FusedAttention()
41:             (unfused_attention): UnfusedDotProductAttention(
41:               (scale_mask_softmax): FusedScaleMaskSoftmax()
41:               (attention_dropout): Dropout(p=0.0, inplace=False)
41:             )
41:           )
41:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
41:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
41:           (q_layernorm): IdentityOp()
41:           (k_layernorm): IdentityOp()
52: GPTModel(
52:   (embedding): LanguageModelEmbedding(
52:     (word_embeddings): VocabParallelEmbedding()
52:     (embedding_dropout): Dropout(p=0.0, inplace=False)
52:   )
52:   (rotary_pos_emb): RotaryEmbedding()
52:   (decoder): TransformerBlock(
52:     (layers): ModuleList(
52:       (0-31): 32 x TransformerLayer(
52:         (input_layernorm): IdentityOp()
52:         (self_attention): SelfAttention(
52:           (core_attention): TEDotProductAttention(
52:             (flash_attention): FlashAttention()
52:             (fused_attention): FusedAttention()
52:             (unfused_attention): UnfusedDotProductAttention(
52:               (scale_mask_softmax): FusedScaleMaskSoftmax()
52:               (attention_dropout): Dropout(p=0.0, inplace=False)
52:             )
52:           )
52:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
52:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
52:           (q_layernorm): IdentityOp()
52:           (k_layernorm): IdentityOp()
 5:         )
 5:         (pre_cross_attn_layernorm): IdentityOp()
 5:         (cross_attention): IdentityOp()
 5:         (cross_attn_bda): IdentityFuncOp()
 5:         (pre_mlp_layernorm): IdentityOp()
 5:         (mlp): TEFusedMLP(
 5:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 5:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 5:         )
 5:       )
 5:     )
 5:     (final_layernorm): RMSNorm()
 5:   )
 5:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 5: )
57: GPTModel(
57:   (embedding): LanguageModelEmbedding(
57:     (word_embeddings): VocabParallelEmbedding()
57:     (embedding_dropout): Dropout(p=0.0, inplace=False)
57:   )
57:   (rotary_pos_emb): RotaryEmbedding()
57:   (decoder): TransformerBlock(
57:     (layers): ModuleList(
57:       (0-31): 32 x TransformerLayer(
57:         (input_layernorm): IdentityOp()
57:         (self_attention): SelfAttention(
57:           (core_attention): TEDotProductAttention(
57:             (flash_attention): FlashAttention()
57:             (fused_attention): FusedAttention()
57:             (unfused_attention): UnfusedDotProductAttention(
57:               (scale_mask_softmax): FusedScaleMaskSoftmax()
57:               (attention_dropout): Dropout(p=0.0, inplace=False)
57:             )
57:           )
57:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
57:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
57:           (q_layernorm): IdentityOp()
57:           (k_layernorm): IdentityOp()
41:         )
41:         (pre_cross_attn_layernorm): IdentityOp()
41:         (cross_attention): IdentityOp()
41:         (cross_attn_bda): IdentityFuncOp()
41:         (pre_mlp_layernorm): IdentityOp()
41:         (mlp): TEFusedMLP(
41:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
41:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
41:         )
41:       )
41:     )
41:     (final_layernorm): RMSNorm()
41:   )
41:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
28: GPTModel(
28:   (embedding): LanguageModelEmbedding(
28:     (word_embeddings): VocabParallelEmbedding()
28:     (embedding_dropout): Dropout(p=0.0, inplace=False)
28:   )
28:   (rotary_pos_emb): RotaryEmbedding()
28:   (decoder): TransformerBlock(
28:     (layers): ModuleList(
28:       (0-31): 32 x TransformerLayer(
28:         (input_layernorm): IdentityOp()
28:         (self_attention): SelfAttention(
28:           (core_attention): TEDotProductAttention(
28:             (flash_attention): FlashAttention()
28:             (fused_attention): FusedAttention()
28:             (unfused_attention): UnfusedDotProductAttention(
28:               (scale_mask_softmax): FusedScaleMaskSoftmax()
28:               (attention_dropout): Dropout(p=0.0, inplace=False)
28:             )
28:           )
28:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
28:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
28:           (q_layernorm): IdentityOp()
28:           (k_layernorm): IdentityOp()
20: GPTModel(
20:   (embedding): LanguageModelEmbedding(
20:     (word_embeddings): VocabParallelEmbedding()
20:     (embedding_dropout): Dropout(p=0.0, inplace=False)
20:   )
20:   (rotary_pos_emb): RotaryEmbedding()
20:   (decoder): TransformerBlock(
20:     (layers): ModuleList(
20:       (0-31): 32 x TransformerLayer(
20:         (input_layernorm): IdentityOp()
20:         (self_attention): SelfAttention(
20:           (core_attention): TEDotProductAttention(
20:             (flash_attention): FlashAttention()
20:             (fused_attention): FusedAttention()
20:             (unfused_attention): UnfusedDotProductAttention(
20:               (scale_mask_softmax): FusedScaleMaskSoftmax()
20:               (attention_dropout): Dropout(p=0.0, inplace=False)
20:             )
20:           )
20:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
20:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
20:           (q_layernorm): IdentityOp()
20:           (k_layernorm): IdentityOp()
44: GPTModel(
44:   (embedding): LanguageModelEmbedding(
44:     (word_embeddings): VocabParallelEmbedding()
44:     (embedding_dropout): Dropout(p=0.0, inplace=False)
44:   )
44:   (rotary_pos_emb): RotaryEmbedding()
44:   (decoder): TransformerBlock(
44:     (layers): ModuleList(
44:       (0-31): 32 x TransformerLayer(
44:         (input_layernorm): IdentityOp()
44:         (self_attention): SelfAttention(
44:           (core_attention): TEDotProductAttention(
44:             (flash_attention): FlashAttention()
44:             (fused_attention): FusedAttention()
44:             (unfused_attention): UnfusedDotProductAttention(
44:               (scale_mask_softmax): FusedScaleMaskSoftmax()
44:               (attention_dropout): Dropout(p=0.0, inplace=False)
44:             )
44:           )
44:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
44:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
44:           (q_layernorm): IdentityOp()
44:           (k_layernorm): IdentityOp()
52:         )
52:         (pre_cross_attn_layernorm): IdentityOp()
52:         (cross_attention): IdentityOp()
52:         (cross_attn_bda): IdentityFuncOp()
52:         (pre_mlp_layernorm): IdentityOp()
52:         (mlp): TEFusedMLP(
52:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
52:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
52:         )
52:       )
52:     )
52:     (final_layernorm): RMSNorm()
52:   )
52:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
52: )
48: GPTModel(
48:   (embedding): LanguageModelEmbedding(
48:     (word_embeddings): VocabParallelEmbedding()
48:     (embedding_dropout): Dropout(p=0.0, inplace=False)
48:   )
48:   (rotary_pos_emb): RotaryEmbedding()
48:   (decoder): TransformerBlock(
48:     (layers): ModuleList(
48:       (0-31): 32 x TransformerLayer(
48:         (input_layernorm): IdentityOp()
48:         (self_attention): SelfAttention(
48:           (core_attention): TEDotProductAttention(
48:             (flash_attention): FlashAttention()
48:             (fused_attention): FusedAttention()
48:             (unfused_attention): UnfusedDotProductAttention(
48:               (scale_mask_softmax): FusedScaleMaskSoftmax()
48:               (attention_dropout): Dropout(p=0.0, inplace=False)
48:             )
48:           )
48:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
48:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
48:           (q_layernorm): IdentityOp()
48:           (k_layernorm): IdentityOp()
57:         )
57:         (pre_cross_attn_layernorm): IdentityOp()
57:         (cross_attention): IdentityOp()
57:         (cross_attn_bda): IdentityFuncOp()
57:         (pre_mlp_layernorm): IdentityOp()
57:         (mlp): TEFusedMLP(
57:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
57:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
57:         )
57:       )
57:     )
57:     (final_layernorm): RMSNorm()
57:   )
57:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
48:         )
48:         (pre_cross_attn_layernorm): IdentityOp()
48:         (cross_attention): IdentityOp()
48:         (cross_attn_bda): IdentityFuncOp()
48:         (pre_mlp_layernorm): IdentityOp()
48:         (mlp): TEFusedMLP(
48:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
48:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
48:         )
48:       )
48:     )
48:     (final_layernorm): RMSNorm()
48:   )
48:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
20:         )
20:         (pre_cross_attn_layernorm): IdentityOp()
20:         (cross_attention): IdentityOp()
20:         (cross_attn_bda): IdentityFuncOp()
20:         (pre_mlp_layernorm): IdentityOp()
20:         (mlp): TEFusedMLP(
20:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
20:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
20:         )
20:       )
20:     )
20:     (final_layernorm): RMSNorm()
20:   )
20:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
28:         )
28:         (pre_cross_attn_layernorm): IdentityOp()
28:         (cross_attention): IdentityOp()
28:         (cross_attn_bda): IdentityFuncOp()
28:         (pre_mlp_layernorm): IdentityOp()
28:         (mlp): TEFusedMLP(
28:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
28:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
28:         )
28:       )
28:     )
28:     (final_layernorm): RMSNorm()
28:   )
28:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
40: GPTModel(
40:   (embedding): LanguageModelEmbedding(
40:     (word_embeddings): VocabParallelEmbedding()
40:     (embedding_dropout): Dropout(p=0.0, inplace=False)
40:   )
40:   (rotary_pos_emb): RotaryEmbedding()
40:   (decoder): TransformerBlock(
40:     (layers): ModuleList(
40:       (0-31): 32 x TransformerLayer(
40:         (input_layernorm): IdentityOp()
40:         (self_attention): SelfAttention(
40:           (core_attention): TEDotProductAttention(
40:             (flash_attention): FlashAttention()
40:             (fused_attention): FusedAttention()
40:             (unfused_attention): UnfusedDotProductAttention(
40:               (scale_mask_softmax): FusedScaleMaskSoftmax()
40:               (attention_dropout): Dropout(p=0.0, inplace=False)
40:             )
40:           )
40:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
40:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
40:           (q_layernorm): IdentityOp()
40:           (k_layernorm): IdentityOp()
 4: GPTModel(
 4:   (embedding): LanguageModelEmbedding(
 4:     (word_embeddings): VocabParallelEmbedding()
 4:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 4:   )
 4:   (rotary_pos_emb): RotaryEmbedding()
 4:   (decoder): TransformerBlock(
 4:     (layers): ModuleList(
 4:       (0-31): 32 x TransformerLayer(
 4:         (input_layernorm): IdentityOp()
 4:         (self_attention): SelfAttention(
 4:           (core_attention): TEDotProductAttention(
 4:             (flash_attention): FlashAttention()
 4:             (fused_attention): FusedAttention()
 4:             (unfused_attention): UnfusedDotProductAttention(
 4:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 4:               (attention_dropout): Dropout(p=0.0, inplace=False)
 4:             )
 4:           )
 4:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 4:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 4:           (q_layernorm): IdentityOp()
 4:           (k_layernorm): IdentityOp()
 9: GPTModel(
 9:   (embedding): LanguageModelEmbedding(
 9:     (word_embeddings): VocabParallelEmbedding()
 9:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 9:   )
 9:   (rotary_pos_emb): RotaryEmbedding()
 9:   (decoder): TransformerBlock(
 9:     (layers): ModuleList(
 9:       (0-31): 32 x TransformerLayer(
 9:         (input_layernorm): IdentityOp()
 9:         (self_attention): SelfAttention(
 9:           (core_attention): TEDotProductAttention(
 9:             (flash_attention): FlashAttention()
 9:             (fused_attention): FusedAttention()
 9:             (unfused_attention): UnfusedDotProductAttention(
 9:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 9:               (attention_dropout): Dropout(p=0.0, inplace=False)
 9:             )
 9:           )
 9:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 9:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 9:           (q_layernorm): IdentityOp()
 9:           (k_layernorm): IdentityOp()
44:         )
44:         (pre_cross_attn_layernorm): IdentityOp()
44:         (cross_attention): IdentityOp()
44:         (cross_attn_bda): IdentityFuncOp()
44:         (pre_mlp_layernorm): IdentityOp()
44:         (mlp): TEFusedMLP(
44:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
44:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
44:         )
44:       )
44:     )
44:     (final_layernorm): RMSNorm()
44:   )
44:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
44: )
32: GPTModel(
32:   (embedding): LanguageModelEmbedding(
32:     (word_embeddings): VocabParallelEmbedding()
32:     (embedding_dropout): Dropout(p=0.0, inplace=False)
32:   )
32:   (rotary_pos_emb): RotaryEmbedding()
32:   (decoder): TransformerBlock(
32:     (layers): ModuleList(
32:       (0-31): 32 x TransformerLayer(
32:         (input_layernorm): IdentityOp()
32:         (self_attention): SelfAttention(
32:           (core_attention): TEDotProductAttention(
32:             (flash_attention): FlashAttention()
32:             (fused_attention): FusedAttention()
32:             (unfused_attention): UnfusedDotProductAttention(
32:               (scale_mask_softmax): FusedScaleMaskSoftmax()
32:               (attention_dropout): Dropout(p=0.0, inplace=False)
32:             )
32:           )
32:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
32:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
32:           (q_layernorm): IdentityOp()
32:           (k_layernorm): IdentityOp()
28: )
57: )
48: )
53: GPTModel(
53:   (embedding): LanguageModelEmbedding(
53:     (word_embeddings): VocabParallelEmbedding()
53:     (embedding_dropout): Dropout(p=0.0, inplace=False)
53:   )
53:   (rotary_pos_emb): RotaryEmbedding()
53:   (decoder): TransformerBlock(
53:     (layers): ModuleList(
53:       (0-31): 32 x TransformerLayer(
53:         (input_layernorm): IdentityOp()
53:         (self_attention): SelfAttention(
53:           (core_attention): TEDotProductAttention(
53:             (flash_attention): FlashAttention()
53:             (fused_attention): FusedAttention()
53:             (unfused_attention): UnfusedDotProductAttention(
53:               (scale_mask_softmax): FusedScaleMaskSoftmax()
53:               (attention_dropout): Dropout(p=0.0, inplace=False)
53:             )
53:           )
53:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
53:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
53:           (q_layernorm): IdentityOp()
53:           (k_layernorm): IdentityOp()
20: )
45: GPTModel(
45:   (embedding): LanguageModelEmbedding(
45:     (word_embeddings): VocabParallelEmbedding()
45:     (embedding_dropout): Dropout(p=0.0, inplace=False)
45:   )
45:   (rotary_pos_emb): RotaryEmbedding()
45:   (decoder): TransformerBlock(
45:     (layers): ModuleList(
45:       (0-31): 32 x TransformerLayer(
45:         (input_layernorm): IdentityOp()
45:         (self_attention): SelfAttention(
45:           (core_attention): TEDotProductAttention(
45:             (flash_attention): FlashAttention()
45:             (fused_attention): FusedAttention()
45:             (unfused_attention): UnfusedDotProductAttention(
45:               (scale_mask_softmax): FusedScaleMaskSoftmax()
45:               (attention_dropout): Dropout(p=0.0, inplace=False)
45:             )
45:           )
45:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
45:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
45:           (q_layernorm): IdentityOp()
45:           (k_layernorm): IdentityOp()
32:         )
32:         (pre_cross_attn_layernorm): IdentityOp()
32:         (cross_attention): IdentityOp()
32:         (cross_attn_bda): IdentityFuncOp()
32:         (pre_mlp_layernorm): IdentityOp()
32:         (mlp): TEFusedMLP(
32:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
32:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
32:         )
32:       )
32:     )
32:     (final_layernorm): RMSNorm()
32:   )
32:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 2: GPTModel(
 2:   (embedding): LanguageModelEmbedding(
 2:     (word_embeddings): VocabParallelEmbedding()
 2:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 2:   )
 2:   (rotary_pos_emb): RotaryEmbedding()
 2:   (decoder): TransformerBlock(
 2:     (layers): ModuleList(
 2:       (0-31): 32 x TransformerLayer(
 2:         (input_layernorm): IdentityOp()
 2:         (self_attention): SelfAttention(
 2:           (core_attention): TEDotProductAttention(
 2:             (flash_attention): FlashAttention()
 2:             (fused_attention): FusedAttention()
 2:             (unfused_attention): UnfusedDotProductAttention(
 2:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 2:               (attention_dropout): Dropout(p=0.0, inplace=False)
 2:             )
 2:           )
 2:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 2:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 2:           (q_layernorm): IdentityOp()
 2:           (k_layernorm): IdentityOp()
29: GPTModel(
29:   (embedding): LanguageModelEmbedding(
29:     (word_embeddings): VocabParallelEmbedding()
29:     (embedding_dropout): Dropout(p=0.0, inplace=False)
29:   )
29:   (rotary_pos_emb): RotaryEmbedding()
29:   (decoder): TransformerBlock(
29:     (layers): ModuleList(
29:       (0-31): 32 x TransformerLayer(
29:         (input_layernorm): IdentityOp()
29:         (self_attention): SelfAttention(
29:           (core_attention): TEDotProductAttention(
29:             (flash_attention): FlashAttention()
29:             (fused_attention): FusedAttention()
29:             (unfused_attention): UnfusedDotProductAttention(
29:               (scale_mask_softmax): FusedScaleMaskSoftmax()
29:               (attention_dropout): Dropout(p=0.0, inplace=False)
29:             )
29:           )
29:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
29:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
29:           (q_layernorm): IdentityOp()
29:           (k_layernorm): IdentityOp()
39: GPTModel(
39:   (embedding): LanguageModelEmbedding(
39:     (word_embeddings): VocabParallelEmbedding()
39:     (embedding_dropout): Dropout(p=0.0, inplace=False)
39:   )
39:   (rotary_pos_emb): RotaryEmbedding()
39:   (decoder): TransformerBlock(
39:     (layers): ModuleList(
39:       (0-31): 32 x TransformerLayer(
39:         (input_layernorm): IdentityOp()
39:         (self_attention): SelfAttention(
39:           (core_attention): TEDotProductAttention(
39:             (flash_attention): FlashAttention()
39:             (fused_attention): FusedAttention()
39:             (unfused_attention): UnfusedDotProductAttention(
39:               (scale_mask_softmax): FusedScaleMaskSoftmax()
39:               (attention_dropout): Dropout(p=0.0, inplace=False)
39:             )
39:           )
39:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
39:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
39:           (q_layernorm): IdentityOp()
39:           (k_layernorm): IdentityOp()
40:         )
40:         (pre_cross_attn_layernorm): IdentityOp()
40:         (cross_attention): IdentityOp()
40:         (cross_attn_bda): IdentityFuncOp()
40:         (pre_mlp_layernorm): IdentityOp()
40:         (mlp): TEFusedMLP(
40:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
40:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
40:         )
40:       )
40:     )
40:     (final_layernorm): RMSNorm()
40:   )
40:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
40: )
58: GPTModel(
58:   (embedding): LanguageModelEmbedding(
58:     (word_embeddings): VocabParallelEmbedding()
58:     (embedding_dropout): Dropout(p=0.0, inplace=False)
58:   )
58:   (rotary_pos_emb): RotaryEmbedding()
58:   (decoder): TransformerBlock(
58:     (layers): ModuleList(
58:       (0-31): 32 x TransformerLayer(
58:         (input_layernorm): IdentityOp()
58:         (self_attention): SelfAttention(
58:           (core_attention): TEDotProductAttention(
58:             (flash_attention): FlashAttention()
58:             (fused_attention): FusedAttention()
58:             (unfused_attention): UnfusedDotProductAttention(
58:               (scale_mask_softmax): FusedScaleMaskSoftmax()
58:               (attention_dropout): Dropout(p=0.0, inplace=False)
58:             )
58:           )
58:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
58:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
58:           (q_layernorm): IdentityOp()
58:           (k_layernorm): IdentityOp()
 9:         )
 9:         (pre_cross_attn_layernorm): IdentityOp()
 9:         (cross_attention): IdentityOp()
 9:         (cross_attn_bda): IdentityFuncOp()
 9:         (pre_mlp_layernorm): IdentityOp()
 9:         (mlp): TEFusedMLP(
 9:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 9:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 9:         )
 9:       )
 9:     )
 9:     (final_layernorm): RMSNorm()
 9:   )
 9:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 9: )
15: GPTModel(
15:   (embedding): LanguageModelEmbedding(
15:     (word_embeddings): VocabParallelEmbedding()
15:     (embedding_dropout): Dropout(p=0.0, inplace=False)
15:   )
15:   (rotary_pos_emb): RotaryEmbedding()
15:   (decoder): TransformerBlock(
15:     (layers): ModuleList(
15:       (0-31): 32 x TransformerLayer(
15:         (input_layernorm): IdentityOp()
15:         (self_attention): SelfAttention(
15:           (core_attention): TEDotProductAttention(
15:             (flash_attention): FlashAttention()
15:             (fused_attention): FusedAttention()
15:             (unfused_attention): UnfusedDotProductAttention(
15:               (scale_mask_softmax): FusedScaleMaskSoftmax()
15:               (attention_dropout): Dropout(p=0.0, inplace=False)
15:             )
15:           )
15:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
15:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
15:           (q_layernorm): IdentityOp()
15:           (k_layernorm): IdentityOp()
53:         )
53:         (pre_cross_attn_layernorm): IdentityOp()
53:         (cross_attention): IdentityOp()
53:         (cross_attn_bda): IdentityFuncOp()
53:         (pre_mlp_layernorm): IdentityOp()
53:         (mlp): TEFusedMLP(
53:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
53:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
53:         )
53:       )
53:     )
53:     (final_layernorm): RMSNorm()
53:   )
53:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
53: )
 4:         )
 4:         (pre_cross_attn_layernorm): IdentityOp()
 4:         (cross_attention): IdentityOp()
 4:         (cross_attn_bda): IdentityFuncOp()
 4:         (pre_mlp_layernorm): IdentityOp()
 4:         (mlp): TEFusedMLP(
 4:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 4:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 4:         )
 4:       )
 4:     )
 4:     (final_layernorm): RMSNorm()
 4:   )
 4:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 4: )
45:         )
45:         (pre_cross_attn_layernorm): IdentityOp()
45:         (cross_attention): IdentityOp()
45:         (cross_attn_bda): IdentityFuncOp()
45:         (pre_mlp_layernorm): IdentityOp()
45:         (mlp): TEFusedMLP(
45:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
45:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
45:         )
45:       )
45:     )
45:     (final_layernorm): RMSNorm()
45:   )
45:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
45: )
 2:         )
 2:         (pre_cross_attn_layernorm): IdentityOp()
 2:         (cross_attention): IdentityOp()
 2:         (cross_attn_bda): IdentityFuncOp()
 2:         (pre_mlp_layernorm): IdentityOp()
 2:         (mlp): TEFusedMLP(
 2:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 2:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 2:         )
 2:       )
 2:     )
 2:     (final_layernorm): RMSNorm()
 2:   )
 2:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
29:         )
29:         (pre_cross_attn_layernorm): IdentityOp()
29:         (cross_attention): IdentityOp()
29:         (cross_attn_bda): IdentityFuncOp()
29:         (pre_mlp_layernorm): IdentityOp()
29:         (mlp): TEFusedMLP(
29:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
29:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
29:         )
29:       )
29:     )
29:     (final_layernorm): RMSNorm()
29:   )
29:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
29: )
39:         )
39:         (pre_cross_attn_layernorm): IdentityOp()
39:         (cross_attention): IdentityOp()
39:         (cross_attn_bda): IdentityFuncOp()
39:         (pre_mlp_layernorm): IdentityOp()
39:         (mlp): TEFusedMLP(
39:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
39:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
39:         )
39:       )
39:     )
39:     (final_layernorm): RMSNorm()
39:   )
39:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
39: )
41: )
58:         )
58:         (pre_cross_attn_layernorm): IdentityOp()
58:         (cross_attention): IdentityOp()
58:         (cross_attn_bda): IdentityFuncOp()
58:         (pre_mlp_layernorm): IdentityOp()
58:         (mlp): TEFusedMLP(
58:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
58:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
58:         )
58:       )
58:     )
58:     (final_layernorm): RMSNorm()
58:   )
58:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
71: GPTModel(
71:   (embedding): LanguageModelEmbedding(
71:     (word_embeddings): VocabParallelEmbedding()
71:     (embedding_dropout): Dropout(p=0.0, inplace=False)
71:   )
71:   (rotary_pos_emb): RotaryEmbedding()
71:   (decoder): TransformerBlock(
71:     (layers): ModuleList(
71:       (0-31): 32 x TransformerLayer(
71:         (input_layernorm): IdentityOp()
71:         (self_attention): SelfAttention(
71:           (core_attention): TEDotProductAttention(
71:             (flash_attention): FlashAttention()
71:             (fused_attention): FusedAttention()
71:             (unfused_attention): UnfusedDotProductAttention(
71:               (scale_mask_softmax): FusedScaleMaskSoftmax()
71:               (attention_dropout): Dropout(p=0.0, inplace=False)
71:             )
71:           )
71:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
71:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
71:           (q_layernorm): IdentityOp()
71:           (k_layernorm): IdentityOp()
15:         )
15:         (pre_cross_attn_layernorm): IdentityOp()
15:         (cross_attention): IdentityOp()
15:         (cross_attn_bda): IdentityFuncOp()
15:         (pre_mlp_layernorm): IdentityOp()
15:         (mlp): TEFusedMLP(
15:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
15:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
15:         )
15:       )
15:     )
15:     (final_layernorm): RMSNorm()
15:   )
15:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
15: )
21: GPTModel(
21:   (embedding): LanguageModelEmbedding(
21:     (word_embeddings): VocabParallelEmbedding()
21:     (embedding_dropout): Dropout(p=0.0, inplace=False)
21:   )
21:   (rotary_pos_emb): RotaryEmbedding()
21:   (decoder): TransformerBlock(
21:     (layers): ModuleList(
21:       (0-31): 32 x TransformerLayer(
21:         (input_layernorm): IdentityOp()
21:         (self_attention): SelfAttention(
21:           (core_attention): TEDotProductAttention(
21:             (flash_attention): FlashAttention()
21:             (fused_attention): FusedAttention()
21:             (unfused_attention): UnfusedDotProductAttention(
21:               (scale_mask_softmax): FusedScaleMaskSoftmax()
21:               (attention_dropout): Dropout(p=0.0, inplace=False)
21:             )
21:           )
21:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
21:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
21:           (q_layernorm): IdentityOp()
21:           (k_layernorm): IdentityOp()
32: )
54: GPTModel(
54:   (embedding): LanguageModelEmbedding(
54:     (word_embeddings): VocabParallelEmbedding()
54:     (embedding_dropout): Dropout(p=0.0, inplace=False)
54:   )
54:   (rotary_pos_emb): RotaryEmbedding()
54:   (decoder): TransformerBlock(
54:     (layers): ModuleList(
54:       (0-31): 32 x TransformerLayer(
54:         (input_layernorm): IdentityOp()
54:         (self_attention): SelfAttention(
54:           (core_attention): TEDotProductAttention(
54:             (flash_attention): FlashAttention()
54:             (fused_attention): FusedAttention()
54:             (unfused_attention): UnfusedDotProductAttention(
54:               (scale_mask_softmax): FusedScaleMaskSoftmax()
54:               (attention_dropout): Dropout(p=0.0, inplace=False)
54:             )
54:           )
54:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
54:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
54:           (q_layernorm): IdentityOp()
54:           (k_layernorm): IdentityOp()
 7: GPTModel(
 7:   (embedding): LanguageModelEmbedding(
 7:     (word_embeddings): VocabParallelEmbedding()
 7:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 7:   )
 7:   (rotary_pos_emb): RotaryEmbedding()
 7:   (decoder): TransformerBlock(
 7:     (layers): ModuleList(
 7:       (0-31): 32 x TransformerLayer(
 7:         (input_layernorm): IdentityOp()
 7:         (self_attention): SelfAttention(
 7:           (core_attention): TEDotProductAttention(
 7:             (flash_attention): FlashAttention()
 7:             (fused_attention): FusedAttention()
 7:             (unfused_attention): UnfusedDotProductAttention(
 7:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 7:               (attention_dropout): Dropout(p=0.0, inplace=False)
 7:             )
 7:           )
 7:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 7:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 7:           (q_layernorm): IdentityOp()
 7:           (k_layernorm): IdentityOp()
 8: GPTModel(
 8:   (embedding): LanguageModelEmbedding(
 8:     (word_embeddings): VocabParallelEmbedding()
 8:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 8:   )
 8:   (rotary_pos_emb): RotaryEmbedding()
 8:   (decoder): TransformerBlock(
 8:     (layers): ModuleList(
 8:       (0-31): 32 x TransformerLayer(
 8:         (input_layernorm): IdentityOp()
 8:         (self_attention): SelfAttention(
 8:           (core_attention): TEDotProductAttention(
 8:             (flash_attention): FlashAttention()
 8:             (fused_attention): FusedAttention()
 8:             (unfused_attention): UnfusedDotProductAttention(
 8:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 8:               (attention_dropout): Dropout(p=0.0, inplace=False)
 8:             )
 8:           )
 8:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 8:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 8:           (q_layernorm): IdentityOp()
 8:           (k_layernorm): IdentityOp()
46: GPTModel(
46:   (embedding): LanguageModelEmbedding(
46:     (word_embeddings): VocabParallelEmbedding()
46:     (embedding_dropout): Dropout(p=0.0, inplace=False)
46:   )
46:   (rotary_pos_emb): RotaryEmbedding()
46:   (decoder): TransformerBlock(
46:     (layers): ModuleList(
46:       (0-31): 32 x TransformerLayer(
46:         (input_layernorm): IdentityOp()
46:         (self_attention): SelfAttention(
46:           (core_attention): TEDotProductAttention(
46:             (flash_attention): FlashAttention()
46:             (fused_attention): FusedAttention()
46:             (unfused_attention): UnfusedDotProductAttention(
46:               (scale_mask_softmax): FusedScaleMaskSoftmax()
46:               (attention_dropout): Dropout(p=0.0, inplace=False)
46:             )
46:           )
46:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
46:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
46:           (q_layernorm): IdentityOp()
46:           (k_layernorm): IdentityOp()
42: GPTModel(
42:   (embedding): LanguageModelEmbedding(
42:     (word_embeddings): VocabParallelEmbedding()
42:     (embedding_dropout): Dropout(p=0.0, inplace=False)
42:   )
42:   (rotary_pos_emb): RotaryEmbedding()
42:   (decoder): TransformerBlock(
42:     (layers): ModuleList(
42:       (0-31): 32 x TransformerLayer(
42:         (input_layernorm): IdentityOp()
42:         (self_attention): SelfAttention(
42:           (core_attention): TEDotProductAttention(
42:             (flash_attention): FlashAttention()
42:             (fused_attention): FusedAttention()
42:             (unfused_attention): UnfusedDotProductAttention(
42:               (scale_mask_softmax): FusedScaleMaskSoftmax()
42:               (attention_dropout): Dropout(p=0.0, inplace=False)
42:             )
42:           )
42:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
42:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
42:           (q_layernorm): IdentityOp()
42:           (k_layernorm): IdentityOp()
38: GPTModel(
38:   (embedding): LanguageModelEmbedding(
38:     (word_embeddings): VocabParallelEmbedding()
38:     (embedding_dropout): Dropout(p=0.0, inplace=False)
38:   )
38:   (rotary_pos_emb): RotaryEmbedding()
38:   (decoder): TransformerBlock(
38:     (layers): ModuleList(
38:       (0-31): 32 x TransformerLayer(
38:         (input_layernorm): IdentityOp()
38:         (self_attention): SelfAttention(
38:           (core_attention): TEDotProductAttention(
38:             (flash_attention): FlashAttention()
38:             (fused_attention): FusedAttention()
38:             (unfused_attention): UnfusedDotProductAttention(
38:               (scale_mask_softmax): FusedScaleMaskSoftmax()
38:               (attention_dropout): Dropout(p=0.0, inplace=False)
38:             )
38:           )
38:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
38:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
38:           (q_layernorm): IdentityOp()
38:           (k_layernorm): IdentityOp()
21:         )
21:         (pre_cross_attn_layernorm): IdentityOp()
21:         (cross_attention): IdentityOp()
21:         (cross_attn_bda): IdentityFuncOp()
21:         (pre_mlp_layernorm): IdentityOp()
21:         (mlp): TEFusedMLP(
21:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
21:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
21:         )
21:       )
21:     )
21:     (final_layernorm): RMSNorm()
21:   )
21:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
21: )
54:         )
54:         (pre_cross_attn_layernorm): IdentityOp()
54:         (cross_attention): IdentityOp()
54:         (cross_attn_bda): IdentityFuncOp()
54:         (pre_mlp_layernorm): IdentityOp()
54:         (mlp): TEFusedMLP(
54:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
54:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
54:         )
54:       )
54:     )
54:     (final_layernorm): RMSNorm()
54:   )
54:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
54: )
14: GPTModel(
14:   (embedding): LanguageModelEmbedding(
14:     (word_embeddings): VocabParallelEmbedding()
14:     (embedding_dropout): Dropout(p=0.0, inplace=False)
14:   )
14:   (rotary_pos_emb): RotaryEmbedding()
14:   (decoder): TransformerBlock(
14:     (layers): ModuleList(
14:       (0-31): 32 x TransformerLayer(
14:         (input_layernorm): IdentityOp()
14:         (self_attention): SelfAttention(
14:           (core_attention): TEDotProductAttention(
14:             (flash_attention): FlashAttention()
14:             (fused_attention): FusedAttention()
14:             (unfused_attention): UnfusedDotProductAttention(
14:               (scale_mask_softmax): FusedScaleMaskSoftmax()
14:               (attention_dropout): Dropout(p=0.0, inplace=False)
14:             )
14:           )
14:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
14:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
14:           (q_layernorm): IdentityOp()
14:           (k_layernorm): IdentityOp()
46:         )
46:         (pre_cross_attn_layernorm): IdentityOp()
46:         (cross_attention): IdentityOp()
46:         (cross_attn_bda): IdentityFuncOp()
46:         (pre_mlp_layernorm): IdentityOp()
46:         (mlp): TEFusedMLP(
46:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
46:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
46:         )
46:       )
46:     )
46:     (final_layernorm): RMSNorm()
46:   )
46:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
46: )
33: GPTModel(
33:   (embedding): LanguageModelEmbedding(
33:     (word_embeddings): VocabParallelEmbedding()
33:     (embedding_dropout): Dropout(p=0.0, inplace=False)
33:   )
33:   (rotary_pos_emb): RotaryEmbedding()
33:   (decoder): TransformerBlock(
33:     (layers): ModuleList(
33:       (0-31): 32 x TransformerLayer(
33:         (input_layernorm): IdentityOp()
33:         (self_attention): SelfAttention(
33:           (core_attention): TEDotProductAttention(
33:             (flash_attention): FlashAttention()
33:             (fused_attention): FusedAttention()
33:             (unfused_attention): UnfusedDotProductAttention(
33:               (scale_mask_softmax): FusedScaleMaskSoftmax()
33:               (attention_dropout): Dropout(p=0.0, inplace=False)
33:             )
33:           )
33:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
33:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
33:           (q_layernorm): IdentityOp()
33:           (k_layernorm): IdentityOp()
 7:         )
 7:         (pre_cross_attn_layernorm): IdentityOp()
 7:         (cross_attention): IdentityOp()
 7:         (cross_attn_bda): IdentityFuncOp()
 7:         (pre_mlp_layernorm): IdentityOp()
 7:         (mlp): TEFusedMLP(
 7:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 7:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 7:         )
 7:       )
 7:     )
 7:     (final_layernorm): RMSNorm()
 7:   )
 7:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 8:         )
 8:         (pre_cross_attn_layernorm): IdentityOp()
 8:         (cross_attention): IdentityOp()
 8:         (cross_attn_bda): IdentityFuncOp()
 8:         (pre_mlp_layernorm): IdentityOp()
 8:         (mlp): TEFusedMLP(
 8:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 8:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 8:         )
 8:       )
 8:     )
 8:     (final_layernorm): RMSNorm()
 8:   )
 8:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 8: )
42:         )
42:         (pre_cross_attn_layernorm): IdentityOp()
42:         (cross_attention): IdentityOp()
42:         (cross_attn_bda): IdentityFuncOp()
42:         (pre_mlp_layernorm): IdentityOp()
42:         (mlp): TEFusedMLP(
42:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
42:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
42:         )
42:       )
42:     )
42:     (final_layernorm): RMSNorm()
42:   )
42:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
42: )
 3: GPTModel(
 3:   (embedding): LanguageModelEmbedding(
 3:     (word_embeddings): VocabParallelEmbedding()
 3:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 3:   )
 3:   (rotary_pos_emb): RotaryEmbedding()
 3:   (decoder): TransformerBlock(
 3:     (layers): ModuleList(
 3:       (0-31): 32 x TransformerLayer(
 3:         (input_layernorm): IdentityOp()
 3:         (self_attention): SelfAttention(
 3:           (core_attention): TEDotProductAttention(
 3:             (flash_attention): FlashAttention()
 3:             (fused_attention): FusedAttention()
 3:             (unfused_attention): UnfusedDotProductAttention(
 3:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 3:               (attention_dropout): Dropout(p=0.0, inplace=False)
 3:             )
 3:           )
 3:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 3:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 3:           (q_layernorm): IdentityOp()
 3:           (k_layernorm): IdentityOp()
55: GPTModel(
55:   (embedding): LanguageModelEmbedding(
55:     (word_embeddings): VocabParallelEmbedding()
55:     (embedding_dropout): Dropout(p=0.0, inplace=False)
55:   )
55:   (rotary_pos_emb): RotaryEmbedding()
55:   (decoder): TransformerBlock(
55:     (layers): ModuleList(
55:       (0-31): 32 x TransformerLayer(
55:         (input_layernorm): IdentityOp()
55:         (self_attention): SelfAttention(
55:           (core_attention): TEDotProductAttention(
55:             (flash_attention): FlashAttention()
55:             (fused_attention): FusedAttention()
55:             (unfused_attention): UnfusedDotProductAttention(
55:               (scale_mask_softmax): FusedScaleMaskSoftmax()
55:               (attention_dropout): Dropout(p=0.0, inplace=False)
55:             )
55:           )
55:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
55:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
55:           (q_layernorm): IdentityOp()
55:           (k_layernorm): IdentityOp()
22: GPTModel(
22:   (embedding): LanguageModelEmbedding(
22:     (word_embeddings): VocabParallelEmbedding()
22:     (embedding_dropout): Dropout(p=0.0, inplace=False)
22:   )
22:   (rotary_pos_emb): RotaryEmbedding()
22:   (decoder): TransformerBlock(
22:     (layers): ModuleList(
22:       (0-31): 32 x TransformerLayer(
22:         (input_layernorm): IdentityOp()
22:         (self_attention): SelfAttention(
22:           (core_attention): TEDotProductAttention(
22:             (flash_attention): FlashAttention()
22:             (fused_attention): FusedAttention()
22:             (unfused_attention): UnfusedDotProductAttention(
22:               (scale_mask_softmax): FusedScaleMaskSoftmax()
22:               (attention_dropout): Dropout(p=0.0, inplace=False)
22:             )
22:           )
22:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
22:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
22:           (q_layernorm): IdentityOp()
22:           (k_layernorm): IdentityOp()
58: )
33:         )
33:         (pre_cross_attn_layernorm): IdentityOp()
33:         (cross_attention): IdentityOp()
33:         (cross_attn_bda): IdentityFuncOp()
33:         (pre_mlp_layernorm): IdentityOp()
33:         (mlp): TEFusedMLP(
33:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
33:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
33:         )
33:       )
33:     )
33:     (final_layernorm): RMSNorm()
33:   )
33:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 6: GPTModel(
 6:   (embedding): LanguageModelEmbedding(
 6:     (word_embeddings): VocabParallelEmbedding()
 6:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 6:   )
 6:   (rotary_pos_emb): RotaryEmbedding()
 6:   (decoder): TransformerBlock(
 6:     (layers): ModuleList(
 6:       (0-31): 32 x TransformerLayer(
 6:         (input_layernorm): IdentityOp()
 6:         (self_attention): SelfAttention(
 6:           (core_attention): TEDotProductAttention(
 6:             (flash_attention): FlashAttention()
 6:             (fused_attention): FusedAttention()
 6:             (unfused_attention): UnfusedDotProductAttention(
 6:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 6:               (attention_dropout): Dropout(p=0.0, inplace=False)
 6:             )
 6:           )
 6:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 6:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 6:           (q_layernorm): IdentityOp()
 6:           (k_layernorm): IdentityOp()
10: GPTModel(
10:   (embedding): LanguageModelEmbedding(
10:     (word_embeddings): VocabParallelEmbedding()
10:     (embedding_dropout): Dropout(p=0.0, inplace=False)
10:   )
10:   (rotary_pos_emb): RotaryEmbedding()
10:   (decoder): TransformerBlock(
10:     (layers): ModuleList(
10:       (0-31): 32 x TransformerLayer(
10:         (input_layernorm): IdentityOp()
10:         (self_attention): SelfAttention(
10:           (core_attention): TEDotProductAttention(
10:             (flash_attention): FlashAttention()
10:             (fused_attention): FusedAttention()
10:             (unfused_attention): UnfusedDotProductAttention(
10:               (scale_mask_softmax): FusedScaleMaskSoftmax()
10:               (attention_dropout): Dropout(p=0.0, inplace=False)
10:             )
10:           )
10:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
10:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
10:           (q_layernorm): IdentityOp()
10:           (k_layernorm): IdentityOp()
38:         )
38:         (pre_cross_attn_layernorm): IdentityOp()
38:         (cross_attention): IdentityOp()
38:         (cross_attn_bda): IdentityFuncOp()
38:         (pre_mlp_layernorm): IdentityOp()
38:         (mlp): TEFusedMLP(
38:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
38:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
38:         )
38:       )
38:     )
38:     (final_layernorm): RMSNorm()
38:   )
38:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
38: )
71:         )
71:         (pre_cross_attn_layernorm): IdentityOp()
71:         (cross_attention): IdentityOp()
71:         (cross_attn_bda): IdentityFuncOp()
71:         (pre_mlp_layernorm): IdentityOp()
71:         (mlp): TEFusedMLP(
71:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
71:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
71:         )
71:       )
71:     )
71:     (final_layernorm): RMSNorm()
71:   )
71:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
14:         )
14:         (pre_cross_attn_layernorm): IdentityOp()
14:         (cross_attention): IdentityOp()
14:         (cross_attn_bda): IdentityFuncOp()
14:         (pre_mlp_layernorm): IdentityOp()
14:         (mlp): TEFusedMLP(
14:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
14:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
14:         )
14:       )
14:     )
14:     (final_layernorm): RMSNorm()
14:   )
14:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
14: )
22:         )
22:         (pre_cross_attn_layernorm): IdentityOp()
22:         (cross_attention): IdentityOp()
22:         (cross_attn_bda): IdentityFuncOp()
22:         (pre_mlp_layernorm): IdentityOp()
22:         (mlp): TEFusedMLP(
22:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
22:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
22:         )
22:       )
22:     )
22:     (final_layernorm): RMSNorm()
22:   )
22:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
43: GPTModel(
43:   (embedding): LanguageModelEmbedding(
43:     (word_embeddings): VocabParallelEmbedding()
43:     (embedding_dropout): Dropout(p=0.0, inplace=False)
43:   )
43:   (rotary_pos_emb): RotaryEmbedding()
43:   (decoder): TransformerBlock(
43:     (layers): ModuleList(
43:       (0-31): 32 x TransformerLayer(
43:         (input_layernorm): IdentityOp()
43:         (self_attention): SelfAttention(
43:           (core_attention): TEDotProductAttention(
43:             (flash_attention): FlashAttention()
43:             (fused_attention): FusedAttention()
43:             (unfused_attention): UnfusedDotProductAttention(
43:               (scale_mask_softmax): FusedScaleMaskSoftmax()
43:               (attention_dropout): Dropout(p=0.0, inplace=False)
43:             )
43:           )
43:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
43:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
43:           (q_layernorm): IdentityOp()
43:           (k_layernorm): IdentityOp()
 3:         )
 3:         (pre_cross_attn_layernorm): IdentityOp()
 3:         (cross_attention): IdentityOp()
 3:         (cross_attn_bda): IdentityFuncOp()
 3:         (pre_mlp_layernorm): IdentityOp()
 3:         (mlp): TEFusedMLP(
 3:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 3:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 3:         )
 3:       )
 3:     )
 3:     (final_layernorm): RMSNorm()
 3:   )
 3:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 3: )
55:         )
55:         (pre_cross_attn_layernorm): IdentityOp()
55:         (cross_attention): IdentityOp()
55:         (cross_attn_bda): IdentityFuncOp()
55:         (pre_mlp_layernorm): IdentityOp()
55:         (mlp): TEFusedMLP(
55:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
55:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
55:         )
55:       )
55:     )
55:     (final_layernorm): RMSNorm()
55:   )
55:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
55: )
56: GPTModel(
56:   (embedding): LanguageModelEmbedding(
56:     (word_embeddings): VocabParallelEmbedding()
56:     (embedding_dropout): Dropout(p=0.0, inplace=False)
56:   )
56:   (rotary_pos_emb): RotaryEmbedding()
56:   (decoder): TransformerBlock(
56:     (layers): ModuleList(
56:       (0-31): 32 x TransformerLayer(
56:         (input_layernorm): IdentityOp()
56:         (self_attention): SelfAttention(
56:           (core_attention): TEDotProductAttention(
56:             (flash_attention): FlashAttention()
56:             (fused_attention): FusedAttention()
56:             (unfused_attention): UnfusedDotProductAttention(
56:               (scale_mask_softmax): FusedScaleMaskSoftmax()
56:               (attention_dropout): Dropout(p=0.0, inplace=False)
56:             )
56:           )
56:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
56:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
56:           (q_layernorm): IdentityOp()
56:           (k_layernorm): IdentityOp()
 6:         )
 6:         (pre_cross_attn_layernorm): IdentityOp()
 6:         (cross_attention): IdentityOp()
 6:         (cross_attn_bda): IdentityFuncOp()
 6:         (pre_mlp_layernorm): IdentityOp()
 6:         (mlp): TEFusedMLP(
 6:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 6:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 6:         )
 6:       )
 6:     )
 6:     (final_layernorm): RMSNorm()
 6:   )
 6:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 6: )
10:         )
10:         (pre_cross_attn_layernorm): IdentityOp()
10:         (cross_attention): IdentityOp()
10:         (cross_attn_bda): IdentityFuncOp()
10:         (pre_mlp_layernorm): IdentityOp()
10:         (mlp): TEFusedMLP(
10:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
10:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
10:         )
10:       )
10:     )
10:     (final_layernorm): RMSNorm()
10:   )
10:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
10: )
49: GPTModel(
49:   (embedding): LanguageModelEmbedding(
49:     (word_embeddings): VocabParallelEmbedding()
49:     (embedding_dropout): Dropout(p=0.0, inplace=False)
49:   )
49:   (rotary_pos_emb): RotaryEmbedding()
49:   (decoder): TransformerBlock(
49:     (layers): ModuleList(
49:       (0-31): 32 x TransformerLayer(
49:         (input_layernorm): IdentityOp()
49:         (self_attention): SelfAttention(
49:           (core_attention): TEDotProductAttention(
49:             (flash_attention): FlashAttention()
49:             (fused_attention): FusedAttention()
49:             (unfused_attention): UnfusedDotProductAttention(
49:               (scale_mask_softmax): FusedScaleMaskSoftmax()
49:               (attention_dropout): Dropout(p=0.0, inplace=False)
49:             )
49:           )
49:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
49:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
49:           (q_layernorm): IdentityOp()
49:           (k_layernorm): IdentityOp()
12: GPTModel(
12:   (embedding): LanguageModelEmbedding(
12:     (word_embeddings): VocabParallelEmbedding()
12:     (embedding_dropout): Dropout(p=0.0, inplace=False)
12:   )
12:   (rotary_pos_emb): RotaryEmbedding()
12:   (decoder): TransformerBlock(
12:     (layers): ModuleList(
12:       (0-31): 32 x TransformerLayer(
12:         (input_layernorm): IdentityOp()
12:         (self_attention): SelfAttention(
12:           (core_attention): TEDotProductAttention(
12:             (flash_attention): FlashAttention()
12:             (fused_attention): FusedAttention()
12:             (unfused_attention): UnfusedDotProductAttention(
12:               (scale_mask_softmax): FusedScaleMaskSoftmax()
12:               (attention_dropout): Dropout(p=0.0, inplace=False)
12:             )
12:           )
12:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
12:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
12:           (q_layernorm): IdentityOp()
12:           (k_layernorm): IdentityOp()
70: GPTModel(
70:   (embedding): LanguageModelEmbedding(
70:     (word_embeddings): VocabParallelEmbedding()
70:     (embedding_dropout): Dropout(p=0.0, inplace=False)
70:   )
70:   (rotary_pos_emb): RotaryEmbedding()
70:   (decoder): TransformerBlock(
70:     (layers): ModuleList(
70:       (0-31): 32 x TransformerLayer(
70:         (input_layernorm): IdentityOp()
70:         (self_attention): SelfAttention(
70:           (core_attention): TEDotProductAttention(
70:             (flash_attention): FlashAttention()
70:             (fused_attention): FusedAttention()
70:             (unfused_attention): UnfusedDotProductAttention(
70:               (scale_mask_softmax): FusedScaleMaskSoftmax()
70:               (attention_dropout): Dropout(p=0.0, inplace=False)
70:             )
70:           )
70:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
70:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
70:           (q_layernorm): IdentityOp()
70:           (k_layernorm): IdentityOp()
22: )
43:         )
43:         (pre_cross_attn_layernorm): IdentityOp()
43:         (cross_attention): IdentityOp()
43:         (cross_attn_bda): IdentityFuncOp()
43:         (pre_mlp_layernorm): IdentityOp()
43:         (mlp): TEFusedMLP(
43:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
43:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
43:         )
43:       )
43:     )
43:     (final_layernorm): RMSNorm()
43:   )
43:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
43: )
 7: )
37: GPTModel(
37:   (embedding): LanguageModelEmbedding(
37:     (word_embeddings): VocabParallelEmbedding()
37:     (embedding_dropout): Dropout(p=0.0, inplace=False)
37:   )
37:   (rotary_pos_emb): RotaryEmbedding()
37:   (decoder): TransformerBlock(
37:     (layers): ModuleList(
37:       (0-31): 32 x TransformerLayer(
37:         (input_layernorm): IdentityOp()
37:         (self_attention): SelfAttention(
37:           (core_attention): TEDotProductAttention(
37:             (flash_attention): FlashAttention()
37:             (fused_attention): FusedAttention()
37:             (unfused_attention): UnfusedDotProductAttention(
37:               (scale_mask_softmax): FusedScaleMaskSoftmax()
37:               (attention_dropout): Dropout(p=0.0, inplace=False)
37:             )
37:           )
37:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
37:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
37:           (q_layernorm): IdentityOp()
37:           (k_layernorm): IdentityOp()
 2: )
33: )
11: GPTModel(
11:   (embedding): LanguageModelEmbedding(
11:     (word_embeddings): VocabParallelEmbedding()
11:     (embedding_dropout): Dropout(p=0.0, inplace=False)
11:   )
11:   (rotary_pos_emb): RotaryEmbedding()
11:   (decoder): TransformerBlock(
11:     (layers): ModuleList(
11:       (0-31): 32 x TransformerLayer(
11:         (input_layernorm): IdentityOp()
11:         (self_attention): SelfAttention(
11:           (core_attention): TEDotProductAttention(
11:             (flash_attention): FlashAttention()
11:             (fused_attention): FusedAttention()
11:             (unfused_attention): UnfusedDotProductAttention(
11:               (scale_mask_softmax): FusedScaleMaskSoftmax()
11:               (attention_dropout): Dropout(p=0.0, inplace=False)
11:             )
11:           )
11:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
11:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
11:           (q_layernorm): IdentityOp()
11:           (k_layernorm): IdentityOp()
61: GPTModel(
61:   (embedding): LanguageModelEmbedding(
61:     (word_embeddings): VocabParallelEmbedding()
61:     (embedding_dropout): Dropout(p=0.0, inplace=False)
61:   )
61:   (rotary_pos_emb): RotaryEmbedding()
61:   (decoder): TransformerBlock(
61:     (layers): ModuleList(
61:       (0-31): 32 x TransformerLayer(
61:         (input_layernorm): IdentityOp()
61:         (self_attention): SelfAttention(
61:           (core_attention): TEDotProductAttention(
61:             (flash_attention): FlashAttention()
61:             (fused_attention): FusedAttention()
61:             (unfused_attention): UnfusedDotProductAttention(
61:               (scale_mask_softmax): FusedScaleMaskSoftmax()
61:               (attention_dropout): Dropout(p=0.0, inplace=False)
61:             )
61:           )
61:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
61:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
61:           (q_layernorm): IdentityOp()
61:           (k_layernorm): IdentityOp()
56:         )
56:         (pre_cross_attn_layernorm): IdentityOp()
56:         (cross_attention): IdentityOp()
56:         (cross_attn_bda): IdentityFuncOp()
56:         (pre_mlp_layernorm): IdentityOp()
56:         (mlp): TEFusedMLP(
56:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
56:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
56:         )
56:       )
56:     )
56:     (final_layernorm): RMSNorm()
56:   )
56:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
70:         )
70:         (pre_cross_attn_layernorm): IdentityOp()
70:         (cross_attention): IdentityOp()
70:         (cross_attn_bda): IdentityFuncOp()
70:         (pre_mlp_layernorm): IdentityOp()
70:         (mlp): TEFusedMLP(
70:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
70:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
70:         )
70:       )
70:     )
70:     (final_layernorm): RMSNorm()
70:   )
70:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
70: )
23: GPTModel(
23:   (embedding): LanguageModelEmbedding(
23:     (word_embeddings): VocabParallelEmbedding()
23:     (embedding_dropout): Dropout(p=0.0, inplace=False)
23:   )
23:   (rotary_pos_emb): RotaryEmbedding()
23:   (decoder): TransformerBlock(
23:     (layers): ModuleList(
23:       (0-31): 32 x TransformerLayer(
23:         (input_layernorm): IdentityOp()
23:         (self_attention): SelfAttention(
23:           (core_attention): TEDotProductAttention(
23:             (flash_attention): FlashAttention()
23:             (fused_attention): FusedAttention()
23:             (unfused_attention): UnfusedDotProductAttention(
23:               (scale_mask_softmax): FusedScaleMaskSoftmax()
23:               (attention_dropout): Dropout(p=0.0, inplace=False)
23:             )
23:           )
23:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
23:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
23:           (q_layernorm): IdentityOp()
23:           (k_layernorm): IdentityOp()
65: GPTModel(
65:   (embedding): LanguageModelEmbedding(
65:     (word_embeddings): VocabParallelEmbedding()
65:     (embedding_dropout): Dropout(p=0.0, inplace=False)
65:   )
65:   (rotary_pos_emb): RotaryEmbedding()
65:   (decoder): TransformerBlock(
65:     (layers): ModuleList(
65:       (0-31): 32 x TransformerLayer(
65:         (input_layernorm): IdentityOp()
65:         (self_attention): SelfAttention(
65:           (core_attention): TEDotProductAttention(
65:             (flash_attention): FlashAttention()
65:             (fused_attention): FusedAttention()
65:             (unfused_attention): UnfusedDotProductAttention(
65:               (scale_mask_softmax): FusedScaleMaskSoftmax()
65:               (attention_dropout): Dropout(p=0.0, inplace=False)
65:             )
65:           )
65:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
65:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
65:           (q_layernorm): IdentityOp()
65:           (k_layernorm): IdentityOp()
12:         )
12:         (pre_cross_attn_layernorm): IdentityOp()
12:         (cross_attention): IdentityOp()
12:         (cross_attn_bda): IdentityFuncOp()
12:         (pre_mlp_layernorm): IdentityOp()
12:         (mlp): TEFusedMLP(
12:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
12:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
12:         )
12:       )
12:     )
12:     (final_layernorm): RMSNorm()
12:   )
12:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
12: )
37:         )
37:         (pre_cross_attn_layernorm): IdentityOp()
37:         (cross_attention): IdentityOp()
37:         (cross_attn_bda): IdentityFuncOp()
37:         (pre_mlp_layernorm): IdentityOp()
37:         (mlp): TEFusedMLP(
37:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
37:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
37:         )
37:       )
37:     )
37:     (final_layernorm): RMSNorm()
37:   )
37:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
37: )
 0: GPTModel(
 0:   (embedding): LanguageModelEmbedding(
 0:     (word_embeddings): VocabParallelEmbedding()
 0:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 0:   )
 0:   (rotary_pos_emb): RotaryEmbedding()
 0:   (decoder): TransformerBlock(
 0:     (layers): ModuleList(
 0:       (0-31): 32 x TransformerLayer(
 0:         (input_layernorm): IdentityOp()
 0:         (self_attention): SelfAttention(
 0:           (core_attention): TEDotProductAttention(
 0:             (flash_attention): FlashAttention()
 0:             (fused_attention): FusedAttention()
 0:             (unfused_attention): UnfusedDotProductAttention(
 0:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 0:               (attention_dropout): Dropout(p=0.0, inplace=False)
 0:             )
 0:           )
 0:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 0:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 0:           (q_layernorm): IdentityOp()
 0:           (k_layernorm): IdentityOp()
56: )
11:         )
11:         (pre_cross_attn_layernorm): IdentityOp()
11:         (cross_attention): IdentityOp()
11:         (cross_attn_bda): IdentityFuncOp()
11:         (pre_mlp_layernorm): IdentityOp()
11:         (mlp): TEFusedMLP(
11:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
11:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
11:         )
11:       )
11:     )
11:     (final_layernorm): RMSNorm()
11:   )
11:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
11: )
35: GPTModel(
35:   (embedding): LanguageModelEmbedding(
35:     (word_embeddings): VocabParallelEmbedding()
35:     (embedding_dropout): Dropout(p=0.0, inplace=False)
35:   )
35:   (rotary_pos_emb): RotaryEmbedding()
35:   (decoder): TransformerBlock(
35:     (layers): ModuleList(
35:       (0-31): 32 x TransformerLayer(
35:         (input_layernorm): IdentityOp()
35:         (self_attention): SelfAttention(
35:           (core_attention): TEDotProductAttention(
35:             (flash_attention): FlashAttention()
35:             (fused_attention): FusedAttention()
35:             (unfused_attention): UnfusedDotProductAttention(
35:               (scale_mask_softmax): FusedScaleMaskSoftmax()
35:               (attention_dropout): Dropout(p=0.0, inplace=False)
35:             )
35:           )
35:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
35:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
35:           (q_layernorm): IdentityOp()
35:           (k_layernorm): IdentityOp()
49:         )
49:         (pre_cross_attn_layernorm): IdentityOp()
49:         (cross_attention): IdentityOp()
49:         (cross_attn_bda): IdentityFuncOp()
49:         (pre_mlp_layernorm): IdentityOp()
49:         (mlp): TEFusedMLP(
49:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
49:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
49:         )
49:       )
49:     )
49:     (final_layernorm): RMSNorm()
49:   )
49:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
71: )
23:         )
23:         (pre_cross_attn_layernorm): IdentityOp()
23:         (cross_attention): IdentityOp()
23:         (cross_attn_bda): IdentityFuncOp()
23:         (pre_mlp_layernorm): IdentityOp()
23:         (mlp): TEFusedMLP(
23:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
23:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
23:         )
23:       )
23:     )
23:     (final_layernorm): RMSNorm()
23:   )
23:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
23: )
49: )
36: GPTModel(
36:   (embedding): LanguageModelEmbedding(
36:     (word_embeddings): VocabParallelEmbedding()
36:     (embedding_dropout): Dropout(p=0.0, inplace=False)
36:   )
36:   (rotary_pos_emb): RotaryEmbedding()
36:   (decoder): TransformerBlock(
36:     (layers): ModuleList(
36:       (0-31): 32 x TransformerLayer(
36:         (input_layernorm): IdentityOp()
36:         (self_attention): SelfAttention(
36:           (core_attention): TEDotProductAttention(
36:             (flash_attention): FlashAttention()
36:             (fused_attention): FusedAttention()
36:             (unfused_attention): UnfusedDotProductAttention(
36:               (scale_mask_softmax): FusedScaleMaskSoftmax()
36:               (attention_dropout): Dropout(p=0.0, inplace=False)
36:             )
36:           )
36:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
36:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
36:           (q_layernorm): IdentityOp()
36:           (k_layernorm): IdentityOp()
13: GPTModel(
13:   (embedding): LanguageModelEmbedding(
13:     (word_embeddings): VocabParallelEmbedding()
13:     (embedding_dropout): Dropout(p=0.0, inplace=False)
13:   )
13:   (rotary_pos_emb): RotaryEmbedding()
13:   (decoder): TransformerBlock(
13:     (layers): ModuleList(
13:       (0-31): 32 x TransformerLayer(
13:         (input_layernorm): IdentityOp()
13:         (self_attention): SelfAttention(
13:           (core_attention): TEDotProductAttention(
13:             (flash_attention): FlashAttention()
13:             (fused_attention): FusedAttention()
13:             (unfused_attention): UnfusedDotProductAttention(
13:               (scale_mask_softmax): FusedScaleMaskSoftmax()
13:               (attention_dropout): Dropout(p=0.0, inplace=False)
13:             )
13:           )
13:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
13:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
13:           (q_layernorm): IdentityOp()
13:           (k_layernorm): IdentityOp()
35:         )
35:         (pre_cross_attn_layernorm): IdentityOp()
35:         (cross_attention): IdentityOp()
35:         (cross_attn_bda): IdentityFuncOp()
35:         (pre_mlp_layernorm): IdentityOp()
35:         (mlp): TEFusedMLP(
35:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
35:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
35:         )
35:       )
35:     )
35:     (final_layernorm): RMSNorm()
35:   )
35:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
31: GPTModel(
31:   (embedding): LanguageModelEmbedding(
31:     (word_embeddings): VocabParallelEmbedding()
31:     (embedding_dropout): Dropout(p=0.0, inplace=False)
31:   )
31:   (rotary_pos_emb): RotaryEmbedding()
31:   (decoder): TransformerBlock(
31:     (layers): ModuleList(
31:       (0-31): 32 x TransformerLayer(
31:         (input_layernorm): IdentityOp()
31:         (self_attention): SelfAttention(
31:           (core_attention): TEDotProductAttention(
31:             (flash_attention): FlashAttention()
31:             (fused_attention): FusedAttention()
31:             (unfused_attention): UnfusedDotProductAttention(
31:               (scale_mask_softmax): FusedScaleMaskSoftmax()
31:               (attention_dropout): Dropout(p=0.0, inplace=False)
31:             )
31:           )
31:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
31:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
31:           (q_layernorm): IdentityOp()
31:           (k_layernorm): IdentityOp()
25: GPTModel(
25:   (embedding): LanguageModelEmbedding(
25:     (word_embeddings): VocabParallelEmbedding()
25:     (embedding_dropout): Dropout(p=0.0, inplace=False)
25:   )
25:   (rotary_pos_emb): RotaryEmbedding()
25:   (decoder): TransformerBlock(
25:     (layers): ModuleList(
25:       (0-31): 32 x TransformerLayer(
25:         (input_layernorm): IdentityOp()
25:         (self_attention): SelfAttention(
25:           (core_attention): TEDotProductAttention(
25:             (flash_attention): FlashAttention()
25:             (fused_attention): FusedAttention()
25:             (unfused_attention): UnfusedDotProductAttention(
25:               (scale_mask_softmax): FusedScaleMaskSoftmax()
25:               (attention_dropout): Dropout(p=0.0, inplace=False)
25:             )
25:           )
25:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
25:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
25:           (q_layernorm): IdentityOp()
25:           (k_layernorm): IdentityOp()
68: GPTModel(
68:   (embedding): LanguageModelEmbedding(
68:     (word_embeddings): VocabParallelEmbedding()
68:     (embedding_dropout): Dropout(p=0.0, inplace=False)
68:   )
68:   (rotary_pos_emb): RotaryEmbedding()
68:   (decoder): TransformerBlock(
68:     (layers): ModuleList(
68:       (0-31): 32 x TransformerLayer(
68:         (input_layernorm): IdentityOp()
68:         (self_attention): SelfAttention(
68:           (core_attention): TEDotProductAttention(
68:             (flash_attention): FlashAttention()
68:             (fused_attention): FusedAttention()
68:             (unfused_attention): UnfusedDotProductAttention(
68:               (scale_mask_softmax): FusedScaleMaskSoftmax()
68:               (attention_dropout): Dropout(p=0.0, inplace=False)
68:             )
68:           )
68:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
68:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
68:           (q_layernorm): IdentityOp()
68:           (k_layernorm): IdentityOp()
65:         )
65:         (pre_cross_attn_layernorm): IdentityOp()
65:         (cross_attention): IdentityOp()
65:         (cross_attn_bda): IdentityFuncOp()
65:         (pre_mlp_layernorm): IdentityOp()
65:         (mlp): TEFusedMLP(
65:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
65:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
65:         )
65:       )
65:     )
65:     (final_layernorm): RMSNorm()
65:   )
65:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
65: )
61:         )
61:         (pre_cross_attn_layernorm): IdentityOp()
61:         (cross_attention): IdentityOp()
61:         (cross_attn_bda): IdentityFuncOp()
61:         (pre_mlp_layernorm): IdentityOp()
61:         (mlp): TEFusedMLP(
61:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
61:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
61:         )
61:       )
61:     )
61:     (final_layernorm): RMSNorm()
61:   )
61:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
61: )
36:         )
36:         (pre_cross_attn_layernorm): IdentityOp()
36:         (cross_attention): IdentityOp()
36:         (cross_attn_bda): IdentityFuncOp()
36:         (pre_mlp_layernorm): IdentityOp()
36:         (mlp): TEFusedMLP(
36:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
36:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
36:         )
36:       )
36:     )
36:     (final_layernorm): RMSNorm()
36:   )
36:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
36: )
35: )
68:         )
68:         (pre_cross_attn_layernorm): IdentityOp()
68:         (cross_attention): IdentityOp()
68:         (cross_attn_bda): IdentityFuncOp()
68:         (pre_mlp_layernorm): IdentityOp()
68:         (mlp): TEFusedMLP(
68:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
68:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
68:         )
68:       )
68:     )
68:     (final_layernorm): RMSNorm()
68:   )
68:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
13:         )
13:         (pre_cross_attn_layernorm): IdentityOp()
13:         (cross_attention): IdentityOp()
13:         (cross_attn_bda): IdentityFuncOp()
13:         (pre_mlp_layernorm): IdentityOp()
13:         (mlp): TEFusedMLP(
13:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
13:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
13:         )
13:       )
13:     )
13:     (final_layernorm): RMSNorm()
13:   )
13:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
13: )
25:         )
25:         (pre_cross_attn_layernorm): IdentityOp()
25:         (cross_attention): IdentityOp()
25:         (cross_attn_bda): IdentityFuncOp()
25:         (pre_mlp_layernorm): IdentityOp()
25:         (mlp): TEFusedMLP(
25:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
25:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
25:         )
25:       )
25:     )
25:     (final_layernorm): RMSNorm()
25:   )
25:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
25: )
 0:         )
 0:         (pre_cross_attn_layernorm): IdentityOp()
 0:         (cross_attention): IdentityOp()
 0:         (cross_attn_bda): IdentityFuncOp()
 0:         (pre_mlp_layernorm): IdentityOp()
 0:         (mlp): TEFusedMLP(
 0:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 0:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 0:         )
 0:       )
 0:     )
 0:     (final_layernorm): RMSNorm()
 0:   )
 0:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 0: )
31:         )
31:         (pre_cross_attn_layernorm): IdentityOp()
31:         (cross_attention): IdentityOp()
31:         (cross_attn_bda): IdentityFuncOp()
31:         (pre_mlp_layernorm): IdentityOp()
31:         (mlp): TEFusedMLP(
31:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
31:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
31:         )
31:       )
31:     )
31:     (final_layernorm): RMSNorm()
31:   )
31:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
63: GPTModel(
63:   (embedding): LanguageModelEmbedding(
63:     (word_embeddings): VocabParallelEmbedding()
63:     (embedding_dropout): Dropout(p=0.0, inplace=False)
63:   )
63:   (rotary_pos_emb): RotaryEmbedding()
63:   (decoder): TransformerBlock(
63:     (layers): ModuleList(
63:       (0-31): 32 x TransformerLayer(
63:         (input_layernorm): IdentityOp()
63:         (self_attention): SelfAttention(
63:           (core_attention): TEDotProductAttention(
63:             (flash_attention): FlashAttention()
63:             (fused_attention): FusedAttention()
63:             (unfused_attention): UnfusedDotProductAttention(
63:               (scale_mask_softmax): FusedScaleMaskSoftmax()
63:               (attention_dropout): Dropout(p=0.0, inplace=False)
63:             )
63:           )
63:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
63:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
63:           (q_layernorm): IdentityOp()
63:           (k_layernorm): IdentityOp()
64: GPTModel(
64:   (embedding): LanguageModelEmbedding(
64:     (word_embeddings): VocabParallelEmbedding()
64:     (embedding_dropout): Dropout(p=0.0, inplace=False)
64:   )
64:   (rotary_pos_emb): RotaryEmbedding()
64:   (decoder): TransformerBlock(
64:     (layers): ModuleList(
64:       (0-31): 32 x TransformerLayer(
64:         (input_layernorm): IdentityOp()
64:         (self_attention): SelfAttention(
64:           (core_attention): TEDotProductAttention(
64:             (flash_attention): FlashAttention()
64:             (fused_attention): FusedAttention()
64:             (unfused_attention): UnfusedDotProductAttention(
64:               (scale_mask_softmax): FusedScaleMaskSoftmax()
64:               (attention_dropout): Dropout(p=0.0, inplace=False)
64:             )
64:           )
64:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
64:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
64:           (q_layernorm): IdentityOp()
64:           (k_layernorm): IdentityOp()
69: GPTModel(
69:   (embedding): LanguageModelEmbedding(
69:     (word_embeddings): VocabParallelEmbedding()
69:     (embedding_dropout): Dropout(p=0.0, inplace=False)
69:   )
69:   (rotary_pos_emb): RotaryEmbedding()
69:   (decoder): TransformerBlock(
69:     (layers): ModuleList(
69:       (0-31): 32 x TransformerLayer(
69:         (input_layernorm): IdentityOp()
69:         (self_attention): SelfAttention(
69:           (core_attention): TEDotProductAttention(
69:             (flash_attention): FlashAttention()
69:             (fused_attention): FusedAttention()
69:             (unfused_attention): UnfusedDotProductAttention(
69:               (scale_mask_softmax): FusedScaleMaskSoftmax()
69:               (attention_dropout): Dropout(p=0.0, inplace=False)
69:             )
69:           )
69:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
69:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
69:           (q_layernorm): IdentityOp()
69:           (k_layernorm): IdentityOp()
63:         )
63:         (pre_cross_attn_layernorm): IdentityOp()
63:         (cross_attention): IdentityOp()
63:         (cross_attn_bda): IdentityFuncOp()
63:         (pre_mlp_layernorm): IdentityOp()
63:         (mlp): TEFusedMLP(
63:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
63:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
63:         )
63:       )
63:     )
63:     (final_layernorm): RMSNorm()
63:   )
63:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
30: GPTModel(
30:   (embedding): LanguageModelEmbedding(
30:     (word_embeddings): VocabParallelEmbedding()
30:     (embedding_dropout): Dropout(p=0.0, inplace=False)
30:   )
30:   (rotary_pos_emb): RotaryEmbedding()
30:   (decoder): TransformerBlock(
30:     (layers): ModuleList(
30:       (0-31): 32 x TransformerLayer(
30:         (input_layernorm): IdentityOp()
30:         (self_attention): SelfAttention(
30:           (core_attention): TEDotProductAttention(
30:             (flash_attention): FlashAttention()
30:             (fused_attention): FusedAttention()
30:             (unfused_attention): UnfusedDotProductAttention(
30:               (scale_mask_softmax): FusedScaleMaskSoftmax()
30:               (attention_dropout): Dropout(p=0.0, inplace=False)
30:             )
30:           )
30:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
30:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
30:           (q_layernorm): IdentityOp()
30:           (k_layernorm): IdentityOp()
 0: 
 0: MCore config:
69:         )
69:         (pre_cross_attn_layernorm): IdentityOp()
69:         (cross_attention): IdentityOp()
69:         (cross_attn_bda): IdentityFuncOp()
69:         (pre_mlp_layernorm): IdentityOp()
69:         (mlp): TEFusedMLP(
69:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
69:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
69:         )
69:       )
69:     )
69:     (final_layernorm): RMSNorm()
69:   )
69:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
69: )
64:         )
64:         (pre_cross_attn_layernorm): IdentityOp()
64:         (cross_attention): IdentityOp()
64:         (cross_attn_bda): IdentityFuncOp()
64:         (pre_mlp_layernorm): IdentityOp()
64:         (mlp): TEFusedMLP(
64:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
64:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
64:         )
64:       )
64:     )
64:     (final_layernorm): RMSNorm()
64:   )
64:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
64: )
50: GPTModel(
50:   (embedding): LanguageModelEmbedding(
50:     (word_embeddings): VocabParallelEmbedding()
50:     (embedding_dropout): Dropout(p=0.0, inplace=False)
50:   )
50:   (rotary_pos_emb): RotaryEmbedding()
50:   (decoder): TransformerBlock(
50:     (layers): ModuleList(
50:       (0-31): 32 x TransformerLayer(
50:         (input_layernorm): IdentityOp()
50:         (self_attention): SelfAttention(
50:           (core_attention): TEDotProductAttention(
50:             (flash_attention): FlashAttention()
50:             (fused_attention): FusedAttention()
50:             (unfused_attention): UnfusedDotProductAttention(
50:               (scale_mask_softmax): FusedScaleMaskSoftmax()
50:               (attention_dropout): Dropout(p=0.0, inplace=False)
50:             )
50:           )
50:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
50:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
50:           (q_layernorm): IdentityOp()
50:           (k_layernorm): IdentityOp()
60: GPTModel(
60:   (embedding): LanguageModelEmbedding(
60:     (word_embeddings): VocabParallelEmbedding()
60:     (embedding_dropout): Dropout(p=0.0, inplace=False)
60:   )
60:   (rotary_pos_emb): RotaryEmbedding()
60:   (decoder): TransformerBlock(
60:     (layers): ModuleList(
60:       (0-31): 32 x TransformerLayer(
60:         (input_layernorm): IdentityOp()
60:         (self_attention): SelfAttention(
60:           (core_attention): TEDotProductAttention(
60:             (flash_attention): FlashAttention()
60:             (fused_attention): FusedAttention()
60:             (unfused_attention): UnfusedDotProductAttention(
60:               (scale_mask_softmax): FusedScaleMaskSoftmax()
60:               (attention_dropout): Dropout(p=0.0, inplace=False)
60:             )
60:           )
60:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
60:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
60:           (q_layernorm): IdentityOp()
60:           (k_layernorm): IdentityOp()
30:         )
30:         (pre_cross_attn_layernorm): IdentityOp()
30:         (cross_attention): IdentityOp()
30:         (cross_attn_bda): IdentityFuncOp()
30:         (pre_mlp_layernorm): IdentityOp()
30:         (mlp): TEFusedMLP(
30:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
30:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
30:         )
30:       )
30:     )
30:     (final_layernorm): RMSNorm()
30:   )
30:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
24: GPTModel(
24:   (embedding): LanguageModelEmbedding(
24:     (word_embeddings): VocabParallelEmbedding()
24:     (embedding_dropout): Dropout(p=0.0, inplace=False)
24:   )
24:   (rotary_pos_emb): RotaryEmbedding()
24:   (decoder): TransformerBlock(
24:     (layers): ModuleList(
24:       (0-31): 32 x TransformerLayer(
24:         (input_layernorm): IdentityOp()
24:         (self_attention): SelfAttention(
24:           (core_attention): TEDotProductAttention(
24:             (flash_attention): FlashAttention()
24:             (fused_attention): FusedAttention()
24:             (unfused_attention): UnfusedDotProductAttention(
24:               (scale_mask_softmax): FusedScaleMaskSoftmax()
24:               (attention_dropout): Dropout(p=0.0, inplace=False)
24:             )
24:           )
24:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
24:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
24:           (q_layernorm): IdentityOp()
24:           (k_layernorm): IdentityOp()
47: GPTModel(
47:   (embedding): LanguageModelEmbedding(
47:     (word_embeddings): VocabParallelEmbedding()
47:     (embedding_dropout): Dropout(p=0.0, inplace=False)
47:   )
47:   (rotary_pos_emb): RotaryEmbedding()
47:   (decoder): TransformerBlock(
47:     (layers): ModuleList(
47:       (0-31): 32 x TransformerLayer(
47:         (input_layernorm): IdentityOp()
47:         (self_attention): SelfAttention(
47:           (core_attention): TEDotProductAttention(
47:             (flash_attention): FlashAttention()
47:             (fused_attention): FusedAttention()
47:             (unfused_attention): UnfusedDotProductAttention(
47:               (scale_mask_softmax): FusedScaleMaskSoftmax()
47:               (attention_dropout): Dropout(p=0.0, inplace=False)
47:             )
47:           )
47:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
47:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
47:           (q_layernorm): IdentityOp()
47:           (k_layernorm): IdentityOp()
 1: GPTModel(
 1:   (embedding): LanguageModelEmbedding(
 1:     (word_embeddings): VocabParallelEmbedding()
 1:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 1:   )
 1:   (rotary_pos_emb): RotaryEmbedding()
 1:   (decoder): TransformerBlock(
 1:     (layers): ModuleList(
 1:       (0-31): 32 x TransformerLayer(
 1:         (input_layernorm): IdentityOp()
 1:         (self_attention): SelfAttention(
 1:           (core_attention): TEDotProductAttention(
 1:             (flash_attention): FlashAttention()
 1:             (fused_attention): FusedAttention()
 1:             (unfused_attention): UnfusedDotProductAttention(
 1:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 1:               (attention_dropout): Dropout(p=0.0, inplace=False)
 1:             )
 1:           )
 1:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 1:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 1:           (q_layernorm): IdentityOp()
 1:           (k_layernorm): IdentityOp()
34: GPTModel(
34:   (embedding): LanguageModelEmbedding(
34:     (word_embeddings): VocabParallelEmbedding()
34:     (embedding_dropout): Dropout(p=0.0, inplace=False)
34:   )
34:   (rotary_pos_emb): RotaryEmbedding()
34:   (decoder): TransformerBlock(
34:     (layers): ModuleList(
34:       (0-31): 32 x TransformerLayer(
34:         (input_layernorm): IdentityOp()
34:         (self_attention): SelfAttention(
34:           (core_attention): TEDotProductAttention(
34:             (flash_attention): FlashAttention()
34:             (fused_attention): FusedAttention()
34:             (unfused_attention): UnfusedDotProductAttention(
34:               (scale_mask_softmax): FusedScaleMaskSoftmax()
34:               (attention_dropout): Dropout(p=0.0, inplace=False)
34:             )
34:           )
34:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
34:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
34:           (q_layernorm): IdentityOp()
34:           (k_layernorm): IdentityOp()
66: GPTModel(
66:   (embedding): LanguageModelEmbedding(
66:     (word_embeddings): VocabParallelEmbedding()
66:     (embedding_dropout): Dropout(p=0.0, inplace=False)
66:   )
66:   (rotary_pos_emb): RotaryEmbedding()
66:   (decoder): TransformerBlock(
66:     (layers): ModuleList(
66:       (0-31): 32 x TransformerLayer(
66:         (input_layernorm): IdentityOp()
66:         (self_attention): SelfAttention(
66:           (core_attention): TEDotProductAttention(
66:             (flash_attention): FlashAttention()
66:             (fused_attention): FusedAttention()
66:             (unfused_attention): UnfusedDotProductAttention(
66:               (scale_mask_softmax): FusedScaleMaskSoftmax()
66:               (attention_dropout): Dropout(p=0.0, inplace=False)
66:             )
66:           )
66:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
66:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
66:           (q_layernorm): IdentityOp()
66:           (k_layernorm): IdentityOp()
68: )
17: GPTModel(
17:   (embedding): LanguageModelEmbedding(
17:     (word_embeddings): VocabParallelEmbedding()
17:     (embedding_dropout): Dropout(p=0.0, inplace=False)
17:   )
17:   (rotary_pos_emb): RotaryEmbedding()
17:   (decoder): TransformerBlock(
17:     (layers): ModuleList(
17:       (0-31): 32 x TransformerLayer(
17:         (input_layernorm): IdentityOp()
17:         (self_attention): SelfAttention(
17:           (core_attention): TEDotProductAttention(
17:             (flash_attention): FlashAttention()
17:             (fused_attention): FusedAttention()
17:             (unfused_attention): UnfusedDotProductAttention(
17:               (scale_mask_softmax): FusedScaleMaskSoftmax()
17:               (attention_dropout): Dropout(p=0.0, inplace=False)
17:             )
17:           )
17:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
17:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
17:           (q_layernorm): IdentityOp()
17:           (k_layernorm): IdentityOp()
60:         )
60:         (pre_cross_attn_layernorm): IdentityOp()
60:         (cross_attention): IdentityOp()
60:         (cross_attn_bda): IdentityFuncOp()
60:         (pre_mlp_layernorm): IdentityOp()
60:         (mlp): TEFusedMLP(
60:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
60:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
60:         )
60:       )
60:     )
60:     (final_layernorm): RMSNorm()
60:   )
60:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
60: )
47:         )
47:         (pre_cross_attn_layernorm): IdentityOp()
47:         (cross_attention): IdentityOp()
47:         (cross_attn_bda): IdentityFuncOp()
47:         (pre_mlp_layernorm): IdentityOp()
47:         (mlp): TEFusedMLP(
47:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
47:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
47:         )
47:       )
47:     )
47:     (final_layernorm): RMSNorm()
47:   )
47:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 1:         )
 1:         (pre_cross_attn_layernorm): IdentityOp()
 1:         (cross_attention): IdentityOp()
 1:         (cross_attn_bda): IdentityFuncOp()
 1:         (pre_mlp_layernorm): IdentityOp()
 1:         (mlp): TEFusedMLP(
 1:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 1:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 1:         )
 1:       )
 1:     )
 1:     (final_layernorm): RMSNorm()
 1:   )
 1:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
59: GPTModel(
59:   (embedding): LanguageModelEmbedding(
59:     (word_embeddings): VocabParallelEmbedding()
59:     (embedding_dropout): Dropout(p=0.0, inplace=False)
59:   )
59:   (rotary_pos_emb): RotaryEmbedding()
59:   (decoder): TransformerBlock(
59:     (layers): ModuleList(
59:       (0-31): 32 x TransformerLayer(
59:         (input_layernorm): IdentityOp()
59:         (self_attention): SelfAttention(
59:           (core_attention): TEDotProductAttention(
59:             (flash_attention): FlashAttention()
59:             (fused_attention): FusedAttention()
59:             (unfused_attention): UnfusedDotProductAttention(
59:               (scale_mask_softmax): FusedScaleMaskSoftmax()
59:               (attention_dropout): Dropout(p=0.0, inplace=False)
59:             )
59:           )
59:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
59:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
59:           (q_layernorm): IdentityOp()
59:           (k_layernorm): IdentityOp()
34:         )
34:         (pre_cross_attn_layernorm): IdentityOp()
34:         (cross_attention): IdentityOp()
34:         (cross_attn_bda): IdentityFuncOp()
34:         (pre_mlp_layernorm): IdentityOp()
34:         (mlp): TEFusedMLP(
34:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
34:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
34:         )
34:       )
34:     )
34:     (final_layernorm): RMSNorm()
34:   )
34:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
66:         )
66:         (pre_cross_attn_layernorm): IdentityOp()
66:         (cross_attention): IdentityOp()
66:         (cross_attn_bda): IdentityFuncOp()
66:         (pre_mlp_layernorm): IdentityOp()
66:         (mlp): TEFusedMLP(
66:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
66:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
66:         )
66:       )
66:     )
66:     (final_layernorm): RMSNorm()
66:   )
66:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
66: )
50:         )
50:         (pre_cross_attn_layernorm): IdentityOp()
50:         (cross_attention): IdentityOp()
50:         (cross_attn_bda): IdentityFuncOp()
50:         (pre_mlp_layernorm): IdentityOp()
50:         (mlp): TEFusedMLP(
50:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
50:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
50:         )
50:       )
50:     )
50:     (final_layernorm): RMSNorm()
50:   )
50:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
31: )
62: GPTModel(
62:   (embedding): LanguageModelEmbedding(
62:     (word_embeddings): VocabParallelEmbedding()
62:     (embedding_dropout): Dropout(p=0.0, inplace=False)
62:   )
62:   (rotary_pos_emb): RotaryEmbedding()
62:   (decoder): TransformerBlock(
62:     (layers): ModuleList(
62:       (0-31): 32 x TransformerLayer(
62:         (input_layernorm): IdentityOp()
62:         (self_attention): SelfAttention(
62:           (core_attention): TEDotProductAttention(
62:             (flash_attention): FlashAttention()
62:             (fused_attention): FusedAttention()
62:             (unfused_attention): UnfusedDotProductAttention(
62:               (scale_mask_softmax): FusedScaleMaskSoftmax()
62:               (attention_dropout): Dropout(p=0.0, inplace=False)
62:             )
62:           )
62:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
62:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
62:           (q_layernorm): IdentityOp()
62:           (k_layernorm): IdentityOp()
24:         )
24:         (pre_cross_attn_layernorm): IdentityOp()
24:         (cross_attention): IdentityOp()
24:         (cross_attn_bda): IdentityFuncOp()
24:         (pre_mlp_layernorm): IdentityOp()
24:         (mlp): TEFusedMLP(
24:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
24:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
24:         )
24:       )
24:     )
24:     (final_layernorm): RMSNorm()
24:   )
24:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
24: )
47: )
 1: )
59:         )
59:         (pre_cross_attn_layernorm): IdentityOp()
59:         (cross_attention): IdentityOp()
59:         (cross_attn_bda): IdentityFuncOp()
59:         (pre_mlp_layernorm): IdentityOp()
59:         (mlp): TEFusedMLP(
59:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
59:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
59:         )
59:       )
59:     )
59:     (final_layernorm): RMSNorm()
59:   )
59:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
34: )
30: )
67: GPTModel(
67:   (embedding): LanguageModelEmbedding(
67:     (word_embeddings): VocabParallelEmbedding()
67:     (embedding_dropout): Dropout(p=0.0, inplace=False)
67:   )
67:   (rotary_pos_emb): RotaryEmbedding()
67:   (decoder): TransformerBlock(
67:     (layers): ModuleList(
67:       (0-31): 32 x TransformerLayer(
67:         (input_layernorm): IdentityOp()
67:         (self_attention): SelfAttention(
67:           (core_attention): TEDotProductAttention(
67:             (flash_attention): FlashAttention()
67:             (fused_attention): FusedAttention()
67:             (unfused_attention): UnfusedDotProductAttention(
67:               (scale_mask_softmax): FusedScaleMaskSoftmax()
67:               (attention_dropout): Dropout(p=0.0, inplace=False)
67:             )
67:           )
67:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
67:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
67:           (q_layernorm): IdentityOp()
67:           (k_layernorm): IdentityOp()
62:         )
62:         (pre_cross_attn_layernorm): IdentityOp()
62:         (cross_attention): IdentityOp()
62:         (cross_attn_bda): IdentityFuncOp()
62:         (pre_mlp_layernorm): IdentityOp()
62:         (mlp): TEFusedMLP(
62:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
62:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
62:         )
62:       )
62:     )
62:     (final_layernorm): RMSNorm()
62:   )
62:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
62: )
17:         )
17:         (pre_cross_attn_layernorm): IdentityOp()
17:         (cross_attention): IdentityOp()
17:         (cross_attn_bda): IdentityFuncOp()
17:         (pre_mlp_layernorm): IdentityOp()
17:         (mlp): TEFusedMLP(
17:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
17:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
17:         )
17:       )
17:     )
17:     (final_layernorm): RMSNorm()
17:   )
17:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
17: )
50: )
26: GPTModel(
26:   (embedding): LanguageModelEmbedding(
26:     (word_embeddings): VocabParallelEmbedding()
26:     (embedding_dropout): Dropout(p=0.0, inplace=False)
26:   )
26:   (rotary_pos_emb): RotaryEmbedding()
26:   (decoder): TransformerBlock(
26:     (layers): ModuleList(
26:       (0-31): 32 x TransformerLayer(
26:         (input_layernorm): IdentityOp()
26:         (self_attention): SelfAttention(
26:           (core_attention): TEDotProductAttention(
26:             (flash_attention): FlashAttention()
26:             (fused_attention): FusedAttention()
26:             (unfused_attention): UnfusedDotProductAttention(
26:               (scale_mask_softmax): FusedScaleMaskSoftmax()
26:               (attention_dropout): Dropout(p=0.0, inplace=False)
26:             )
26:           )
26:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
26:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
26:           (q_layernorm): IdentityOp()
26:           (k_layernorm): IdentityOp()
59: )
51: GPTModel(
51:   (embedding): LanguageModelEmbedding(
51:     (word_embeddings): VocabParallelEmbedding()
51:     (embedding_dropout): Dropout(p=0.0, inplace=False)
51:   )
51:   (rotary_pos_emb): RotaryEmbedding()
51:   (decoder): TransformerBlock(
51:     (layers): ModuleList(
51:       (0-31): 32 x TransformerLayer(
51:         (input_layernorm): IdentityOp()
51:         (self_attention): SelfAttention(
51:           (core_attention): TEDotProductAttention(
51:             (flash_attention): FlashAttention()
51:             (fused_attention): FusedAttention()
51:             (unfused_attention): UnfusedDotProductAttention(
51:               (scale_mask_softmax): FusedScaleMaskSoftmax()
51:               (attention_dropout): Dropout(p=0.0, inplace=False)
51:             )
51:           )
51:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
51:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
51:           (q_layernorm): IdentityOp()
51:           (k_layernorm): IdentityOp()
67:         )
67:         (pre_cross_attn_layernorm): IdentityOp()
67:         (cross_attention): IdentityOp()
67:         (cross_attn_bda): IdentityFuncOp()
67:         (pre_mlp_layernorm): IdentityOp()
67:         (mlp): TEFusedMLP(
67:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
67:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
67:         )
67:       )
67:     )
67:     (final_layernorm): RMSNorm()
67:   )
67:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
67: )
63: )
51:         )
51:         (pre_cross_attn_layernorm): IdentityOp()
51:         (cross_attention): IdentityOp()
51:         (cross_attn_bda): IdentityFuncOp()
51:         (pre_mlp_layernorm): IdentityOp()
51:         (mlp): TEFusedMLP(
51:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
51:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
51:         )
51:       )
51:     )
51:     (final_layernorm): RMSNorm()
51:   )
51:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
51: )
26:         )
26:         (pre_cross_attn_layernorm): IdentityOp()
26:         (cross_attention): IdentityOp()
26:         (cross_attn_bda): IdentityFuncOp()
26:         (pre_mlp_layernorm): IdentityOp()
26:         (mlp): TEFusedMLP(
26:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
26:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
26:         )
26:       )
26:     )
26:     (final_layernorm): RMSNorm()
26:   )
26:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
16: GPTModel(
16:   (embedding): LanguageModelEmbedding(
16:     (word_embeddings): VocabParallelEmbedding()
16:     (embedding_dropout): Dropout(p=0.0, inplace=False)
16:   )
16:   (rotary_pos_emb): RotaryEmbedding()
16:   (decoder): TransformerBlock(
16:     (layers): ModuleList(
16:       (0-31): 32 x TransformerLayer(
16:         (input_layernorm): IdentityOp()
16:         (self_attention): SelfAttention(
16:           (core_attention): TEDotProductAttention(
16:             (flash_attention): FlashAttention()
16:             (fused_attention): FusedAttention()
16:             (unfused_attention): UnfusedDotProductAttention(
16:               (scale_mask_softmax): FusedScaleMaskSoftmax()
16:               (attention_dropout): Dropout(p=0.0, inplace=False)
16:             )
16:           )
16:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
16:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
16:           (q_layernorm): IdentityOp()
16:           (k_layernorm): IdentityOp()
27: GPTModel(
27:   (embedding): LanguageModelEmbedding(
27:     (word_embeddings): VocabParallelEmbedding()
27:     (embedding_dropout): Dropout(p=0.0, inplace=False)
27:   )
27:   (rotary_pos_emb): RotaryEmbedding()
27:   (decoder): TransformerBlock(
27:     (layers): ModuleList(
27:       (0-31): 32 x TransformerLayer(
27:         (input_layernorm): IdentityOp()
27:         (self_attention): SelfAttention(
27:           (core_attention): TEDotProductAttention(
27:             (flash_attention): FlashAttention()
27:             (fused_attention): FusedAttention()
27:             (unfused_attention): UnfusedDotProductAttention(
27:               (scale_mask_softmax): FusedScaleMaskSoftmax()
27:               (attention_dropout): Dropout(p=0.0, inplace=False)
27:             )
27:           )
27:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
27:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
27:           (q_layernorm): IdentityOp()
27:           (k_layernorm): IdentityOp()
16:         )
16:         (pre_cross_attn_layernorm): IdentityOp()
16:         (cross_attention): IdentityOp()
16:         (cross_attn_bda): IdentityFuncOp()
16:         (pre_mlp_layernorm): IdentityOp()
16:         (mlp): TEFusedMLP(
16:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
16:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
16:         )
16:       )
16:     )
16:     (final_layernorm): RMSNorm()
16:   )
16:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
16: )
18: GPTModel(
18:   (embedding): LanguageModelEmbedding(
18:     (word_embeddings): VocabParallelEmbedding()
18:     (embedding_dropout): Dropout(p=0.0, inplace=False)
18:   )
18:   (rotary_pos_emb): RotaryEmbedding()
18:   (decoder): TransformerBlock(
18:     (layers): ModuleList(
18:       (0-31): 32 x TransformerLayer(
18:         (input_layernorm): IdentityOp()
18:         (self_attention): SelfAttention(
18:           (core_attention): TEDotProductAttention(
18:             (flash_attention): FlashAttention()
18:             (fused_attention): FusedAttention()
18:             (unfused_attention): UnfusedDotProductAttention(
18:               (scale_mask_softmax): FusedScaleMaskSoftmax()
18:               (attention_dropout): Dropout(p=0.0, inplace=False)
18:             )
18:           )
18:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
18:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
18:           (q_layernorm): IdentityOp()
18:           (k_layernorm): IdentityOp()
27:         )
27:         (pre_cross_attn_layernorm): IdentityOp()
27:         (cross_attention): IdentityOp()
27:         (cross_attn_bda): IdentityFuncOp()
27:         (pre_mlp_layernorm): IdentityOp()
27:         (mlp): TEFusedMLP(
27:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
27:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
27:         )
27:       )
27:     )
27:     (final_layernorm): RMSNorm()
27:   )
27:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
27: )
26: )
18:         )
18:         (pre_cross_attn_layernorm): IdentityOp()
18:         (cross_attention): IdentityOp()
18:         (cross_attn_bda): IdentityFuncOp()
18:         (pre_mlp_layernorm): IdentityOp()
18:         (mlp): TEFusedMLP(
18:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
18:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
18:         )
18:       )
18:     )
18:     (final_layernorm): RMSNorm()
18:   )
18:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
18: )
19: GPTModel(
19:   (embedding): LanguageModelEmbedding(
19:     (word_embeddings): VocabParallelEmbedding()
19:     (embedding_dropout): Dropout(p=0.0, inplace=False)
19:   )
19:   (rotary_pos_emb): RotaryEmbedding()
19:   (decoder): TransformerBlock(
19:     (layers): ModuleList(
19:       (0-31): 32 x TransformerLayer(
19:         (input_layernorm): IdentityOp()
19:         (self_attention): SelfAttention(
19:           (core_attention): TEDotProductAttention(
19:             (flash_attention): FlashAttention()
19:             (fused_attention): FusedAttention()
19:             (unfused_attention): UnfusedDotProductAttention(
19:               (scale_mask_softmax): FusedScaleMaskSoftmax()
19:               (attention_dropout): Dropout(p=0.0, inplace=False)
19:             )
19:           )
19:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
19:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
19:           (q_layernorm): IdentityOp()
19:           (k_layernorm): IdentityOp()
19:         )
19:         (pre_cross_attn_layernorm): IdentityOp()
19:         (cross_attention): IdentityOp()
19:         (cross_attn_bda): IdentityFuncOp()
19:         (pre_mlp_layernorm): IdentityOp()
19:         (mlp): TEFusedMLP(
19:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
19:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
19:         )
19:       )
19:     )
19:     (final_layernorm): RMSNorm()
19:   )
19:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
19: )
 0: Llama31Config8B(tensor_model_parallel_size=1,
 0:                 pipeline_model_parallel_comm_backend=None,
 0:                 pipeline_model_parallel_size=1,
 0:                 virtual_pipeline_model_parallel_size=None,
 0:                 sequence_parallel=False,
 0:                 context_parallel_size=2,
 0:                 hierarchical_context_parallel_sizes=None,
 0:                 expert_model_parallel_size=1,
 0:                 expert_tensor_parallel_size=2,
 0:                 moe_extended_tp=False,
 0:                 perform_initialization=True,
 0:                 use_cpu_initialization=False,
 0:                 fp16=False,
 0:                 bf16=True,
 0:                 params_dtype=torch.bfloat16,
 0:                 timers=<megatron.core.timers.Timers object at 0xe7a04b202990>,
 0:                 finalize_model_grads_func=<function MegatronOptimizerModule.on_fit_start.<locals>.finalize_model_grads_func at 0xe7a028134400>,
 0:                 grad_scale_func=None,
 0:                 no_sync_func=<bound method DistributedDataParallel.no_sync of DDP(
 0:   (module): Float16Module(
 0:     (module): GPTModel(
 0:       (embedding): LanguageModelEmbedding(
 0:         (word_embeddings): VocabParallelEmbedding()
 0:         (embedding_dropout): Dropout(p=0.0, inplace=False)
 0:       )
 0:       (rotary_pos_emb): RotaryEmbedding()
 0:       (decoder): TransformerBlock(
 0:         (layers): ModuleList(
 0:           (0-31): 32 x TransformerLayer(
 0:             (input_layernorm): IdentityOp()
 0:             (self_attention): SelfAttention(
 0:               (core_attention): TEDotProductAttention(
 0:                 (flash_attention): FlashAttention()
 0:                 (fused_attention): FusedAttention()
 0:                 (unfused_attention): UnfusedDotProductAttention(
 0:                   (scale_mask_softmax): FusedScaleMaskSoftmax()
 0:                   (attention_dropout): Dropout(p=0.0, inplace=False)
 0:                 )
 0:               )
 0:               (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 0:               (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 0:               (q_layernorm): IdentityOp()
 0:               (k_layernorm): IdentityOp()
 0:             )
 0:             (pre_cross_attn_layernorm): IdentityOp()
 0:             (cross_attention): IdentityOp()
 0:             (cross_attn_bda): IdentityFuncOp()
 0:             (pre_mlp_layernorm): IdentityOp()
 0:             (mlp): TEFusedMLP(
 0:               (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 0:               (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 0:             )
 0:           )
 0:         )
 0:         (final_layernorm): RMSNorm()
 0:       )
 0:       (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 0:     )
 0:   )
 0: )>,
 0:                 grad_sync_func=None,
 0:                 param_sync_func=None,
 0:                 deterministic_mode=False,
 0:                 enable_autocast=False,
 0:                 autocast_dtype=torch.bfloat16,
 0:                 num_microbatches_with_partial_activation_checkpoints=None,
 0:                 gradient_accumulation_fusion=True,
 0:                 async_tensor_model_parallel_allreduce=False,
 0:                 use_te_rng_tracker=(True,),
 0:                 tp_comm_overlap=False,
 0:                 tp_comm_bulk_wgrad=True,
 0:                 tp_comm_bulk_dgrad=True,
 0:                 tp_comm_overlap_ag=True,
 0:                 tp_comm_overlap_rs=True,
 0:                 tp_comm_overlap_rs_dgrad=False,
 0:                 tp_comm_split_ag=True,
 0:                 tp_comm_atomic_ag=False,
 0:                 tp_comm_split_rs=True,
 0:                 tp_comm_atomic_rs=False,
 0:                 cross_entropy_loss_fusion=True,
 0:                 cross_entropy_fusion_impl='te',
 0:                 tp_comm_overlap_disable_qkv=False,
 0:                 tp_comm_overlap_disable_fc1=False,
 0:                 tp_comm_bootstrap_backend=None,
 0:                 overlap_moe_expert_parallel_comm=False,
 0:                 delay_wgrad_compute=False,
 0:                 pipeline_dtype=torch.bfloat16,
 0:                 variable_seq_lengths=False,
 0:                 overlap_p2p_comm=True,
 0:                 batch_p2p_comm=False,
 0:                 batch_p2p_sync=True,
 0:                 use_ring_exchange_p2p=False,
 0:                 deallocate_pipeline_outputs=True,
 0:                 defer_embedding_wgrad_compute=False,
 0:                 wgrad_deferral_limit=50,
 0:                 overlap_p2p_comm_warmup_flush=False,
 0:                 microbatch_group_size_per_vp_stage=1,
 0:                 cpu_offloading=False,
 0:                 cpu_offloading_num_layers=0,
 0:                 _cpu_offloading_context=None,
 0:                 cpu_offloading_activations=True,
 0:                 cpu_offloading_weights=False,
 0:                 cpu_offloading_double_buffering=False,
 0:                 barrier_with_L1_time=True,
 0:                 num_layers=32,
 0:                 mtp_num_layers=None,
 0:                 mtp_loss_scaling_factor=None,
 0:                 num_layers_in_first_pipeline_stage=None,
 0:                 num_layers_in_last_pipeline_stage=None,
 0:                 pipeline_model_parallel_layout=None,
 0:                 account_for_embedding_in_pipeline_split=False,
 0:                 account_for_loss_in_pipeline_split=False,
 0:                 hidden_size=4096,
 0:                 num_attention_heads=32,
 0:                 attention_backend=<AttnBackend.auto: 5>,
 0:                 softmax_scale=None,
 0:                 softmax_type='vanilla',
 0:                 num_query_groups=8,
 0:                 ffn_hidden_size=14336,
 0:                 kv_channels=128,
 0:                 hidden_dropout=0.0,
 0:                 attention_dropout=0.0,
 0:                 fp32_residual_connection=False,
 0:                 apply_residual_connection_post_layernorm=False,
 0:                 layernorm_epsilon=1e-05,
 0:                 layernorm_zero_centered_gamma=False,
 0:                 add_bias_linear=False,
 0:                 add_qkv_bias=False,
 0:                 gated_linear_unit=True,
 0:                 activation_func=<function silu at 0xe7a3b15d6200>,
 0:                 activation_func_fp8_input_store=False,
 0:                 glu_linear_offset=0.0,
 0:                 activation_func_clamp_value=None,
 0:                 num_moe_experts=None,
 0:                 rotary_interleaved=False,
 0:                 window_size=None,
 0:                 window_attn_skip_freq=None,
 0:                 normalization='RMSNorm',
 0:                 qk_layernorm=False,
 0:                 test_mode=False,
 0:                 calculate_per_token_loss=False,
 0:                 multi_latent_attention=False,
 0:                 no_rope_freq=None,
 0:                 moe_deepep_num_sms=20,
 0:                 init_method=functools.partial(<function normal_ at 0xe7a3b1452b60>, mean=0.0, std=0.02),
 0:                 output_layer_init_method=functools.partial(<function normal_ at 0xe7a3b1452b60>, mean=0.0, std=0.0025),
 0:                 init_method_std=0.02,
 0:                 embedding_init_method=functools.partial(<function normal_ at 0xe7a3b1452b60>, mean=0.0, std=0.02),
 0:                 embedding_init_method_std=0.02,
 0:                 init_model_with_meta_device=False,
 0:                 apply_query_key_layer_scaling=False,
 0:                 attention_softmax_in_fp32=False,
 0:                 disable_bf16_reduced_precision_matmul=False,
 0:                 bias_activation_fusion=True,
 0:                 masked_softmax_fusion=True,
 0:                 persist_layer_norm=True,
 0:                 memory_efficient_layer_norm=False,
 0:                 bias_dropout_fusion=True,
 0:                 apply_rope_fusion=True,
 0:                 use_fused_weighted_squared_relu=False,
 0:                 fused_single_qkv_rope=True,
 0:                 recompute_granularity=None,
 0:                 recompute_method=None,
 0:                 recompute_num_layers=None,
 0:                 distribute_saved_activations=None,
 0:                 recompute_modules=['core_attn'],
 0:                 fp8='e4m3',
 0:                 fp8_recipe='tensorwise',
 0:                 fp8_param=True,
 0:                 fp8_margin=0,
 0:                 fp8_interval=1,
 0:                 fp8_amax_history_len=1,
 0:                 fp8_amax_compute_algo='most_recent',
 0:                 fp8_wgrad=True,
 0:                 fp8_dot_product_attention=False,
 0:                 fp8_multi_head_attention=False,
 0:                 tp_only_amax_red=True,
 0:                 first_last_layers_bf16=False,
 0:                 num_layers_at_start_in_bf16=0,
 0:                 num_layers_at_end_in_bf16=0,
 0:                 use_kitchen=False,
 0:                 fp4=None,
 0:                 fp4_recipe='nvfp4',
 0:                 fp4_param=False,
 0:                 moe_shared_expert_intermediate_size=None,
 0:                 moe_shared_expert_overlap=False,
 0:                 moe_layer_freq=1,
 0:                 moe_ffn_hidden_size=None,
 0:                 moe_router_load_balancing_type='aux_loss',
 0:                 moe_router_topk=2,
 0:                 moe_router_topk_limited_devices=None,
 0:                 moe_router_padding_for_fp8=False,
 0:                 moe_router_num_groups=None,
 0:                 moe_router_group_topk=None,
 0:                 moe_router_pre_softmax=False,
 0:                 moe_router_topk_scaling_factor=None,
 0:                 moe_router_score_function='softmax',
 0:                 moe_router_dtype=None,
 0:                 moe_router_enable_expert_bias=False,
 0:                 moe_router_bias_update_rate=0.001,
 0:                 moe_router_force_load_balancing=False,
 0:                 moe_grouped_gemm=False,
 0:                 moe_use_legacy_grouped_gemm=False,
 0:                 moe_aux_loss_coeff=0.0,
 0:                 moe_z_loss_coeff=None,
 0:                 moe_input_jitter_eps=None,
 0:                 moe_token_dropping=False,
 0:                 moe_token_dispatcher_type='allgather',
 0:                 moe_enable_deepep=False,
 0:                 moe_per_layer_logging=False,
 0:                 moe_expert_capacity_factor=None,
 0:                 moe_pad_expert_input_to_capacity=False,
 0:                 moe_token_drop_policy='probs',
 0:                 moe_layer_recompute=False,
 0:                 moe_permute_fusion=False,
 0:                 moe_router_fusion=False,
 0:                 moe_apply_probs_on_input=False,
 0:                 cp_comm_type=None,
 0:                 enable_cuda_graph=1,
 0:                 cuda_graph_use_single_mempool=False,
 0:                 cuda_graph_retain_backward_graph=False,
 0:                 cuda_graph_warmup_steps=3,
 0:                 external_cuda_graph=False,
 0:                 cuda_graph_scope='full_iteration',
 0:                 clone_scatter_output_in_embedding=True,
 0:                 disable_parameter_transpose_cache=False,
 0:                 config_logger_dir='',
 0:                 flash_decode=False,
 0:                 use_te_activation_func=False,
 0:                 inference_rng_tracker=False,
 0:                 inference_sampling_seed=42,
 0:                 symmetric_ar_type=None,
 0:                 mrope_section=None,
 0:                 is_hybrid_model=False,
 0:                 mamba_state_dim=128,
 0:                 mamba_head_dim=64,
 0:                 mamba_num_groups=8,
 0:                 mamba_num_heads=None,
 0:                 use_mamba_mem_eff_path=True,
 0:                 mlp_chunks_for_prefill=1,
 0:                 heterogeneous_block_specs=False,
 0:                 hetereogenous_dist_checkpoint=False,
 0:                 quant_recipe=None,
 0:                 transformer_impl='transformer_engine',
 0:                 fp16_lm_cross_entropy=False,
 0:                 parallel_output=True,
 0:                 share_embeddings_and_output_weights=False,
 0:                 make_vocab_size_divisible_by=128,
 0:                 position_embedding_type='rope',
 0:                 rotary_base=500000,
 0:                 rotary_percent=1.0,
 0:                 seq_len_interpolation_factor=None,
 0:                 seq_length=8192,
 0:                 scatter_embedding_sequence_parallel=True,
 0:                 use_transformer_engine_full_layer_spec=False,
 0:                 transformer_layer_spec=<function default_layer_spec at 0xe7a04bf2b920>,
 0:                 forward_step_fn=<function gpt_forward_step at 0xe7a04bf2a160>,
 0:                 data_step_fn=<function gpt_data_step at 0xe7a04bf29f80>,
 0:                 generation_config=None,
 0:                 vocab_size=None,
 0:                 tp_comm_overlap_cfg=None,
 0:                 use_transformer_engine_op_fuser=True,
 0:                 scale_factor=8.0,
 0:                 low_freq_factor=1.0,
 0:                 high_freq_factor=4.0,
 0:                 old_context_len=8192)
 0: [AUX I 2025-10-09 04:58:12 data:291] Instantiating MegatronPretrainingSampler with total_samples: 10000000 and consumed_samples: 0
44: [rank44]:[W1009 04:58:12.550862805 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 319 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
68: [rank68]:[W1009 04:58:12.220941991 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 391 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
69: [rank69]:[W1009 04:58:12.220941991 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 394 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
59: [rank59]:[W1009 04:58:12.558482173 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 364 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
42: [rank42]:[W1009 04:58:12.715225259 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 313 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
43: [rank43]:[W1009 04:58:12.716789781 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 316 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
33: [rank33]:[W1009 04:58:12.464296255 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 286 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
58: [rank58]:[W1009 04:58:12.561813887 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 361 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
49: [rank49]:[W1009 04:58:12.550295724 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 334 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
48: [rank48]:[W1009 04:58:12.550943154 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 331 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
65: [rank65]:[W1009 04:58:12.937284388 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 382 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
34: [rank34]:[W1009 04:58:12.466957564 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 289 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
35: [rank35]:[W1009 04:58:12.467449729 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 292 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
45: [rank45]:[W1009 04:58:12.557996945 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 322 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
66: [rank66]:[W1009 04:58:12.939923739 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 385 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
64: [rank64]:[W1009 04:58:12.940853604 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 379 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
67: [rank67]:[W1009 04:58:12.940896452 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 388 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
63: [rank63]:[W1009 04:58:12.512414457 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 376 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
41: [rank41]:[W1009 04:58:12.723723877 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 310 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
55: [rank55]:[W1009 04:58:12.287220711 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 352 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
62: [rank62]:[W1009 04:58:12.517613438 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 373 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
40: [rank40]:[W1009 04:58:12.730483699 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 307 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
54: [rank54]:[W1009 04:58:12.293883537 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 349 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
39: [rank39]:[W1009 04:58:12.226325597 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 304 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
53: [rank53]:[W1009 04:58:12.294607253 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 346 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
38: [rank38]:[W1009 04:58:12.226504863 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 301 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
52: [rank52]:[W1009 04:58:12.294793590 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 343 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
13: [rank13]:[W1009 04:58:12.921245289 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 226 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
61: [rank61]:[W1009 04:58:12.530668057 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 370 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
56: [rank56]:[W1009 04:58:12.587061024 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 355 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
60: [rank60]:[W1009 04:58:12.533410285 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 367 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
31: [rank31]:[W1009 04:58:12.780503119 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 280 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
25: [rank25]:[W1009 04:58:12.013782243 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 262 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
57: [rank57]:[W1009 04:58:12.592267701 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 358 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
24: [rank24]:[W1009 04:58:12.015628945 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 259 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
18: [rank18]:[W1009 04:58:12.008351444 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 241 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
30: [rank30]:[W1009 04:58:12.784655052 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 277 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
19: [rank19]:[W1009 04:58:12.010454435 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 244 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
21: [rank21]:[W1009 04:58:12.527207057 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 250 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
20: [rank20]:[W1009 04:58:12.527327282 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 247 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
50: [rank50]:[W1009 04:58:12.583837312 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 337 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
37: [rank37]:[W1009 04:58:12.244630488 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 298 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
36: [rank36]:[W1009 04:58:12.244778745 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 295 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
51: [rank51]:[W1009 04:58:12.587470081 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 340 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
46: [rank46]:[W1009 04:58:12.593414975 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 325 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
47: [rank47]:[W1009 04:58:12.594013348 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 328 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 1: [rank1]:[W1009 04:58:12.490056109 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 190 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 4: [rank4]:[W1009 04:58:12.487407978 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 199 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 6: [rank6]:[W1009 04:58:12.487470698 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 205 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 5: [rank5]:[W1009 04:58:12.487686156 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 202 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 7: [rank7]:[W1009 04:58:12.488883733 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 208 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
12: [rank12]:[W1009 04:58:12.941395987 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 223 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
70: [rank70]:[W1009 04:58:12.268731789 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 397 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
71: [rank71]:[W1009 04:58:12.269086799 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 400 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 8: [rank8]:[W1009 04:58:12.822741912 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 211 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 2: [rank2]:[W1009 04:58:12.497866623 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 193 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
16: [rank16]:[W1009 04:58:12.028106727 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 235 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
10: [rank10]:[W1009 04:58:12.829093776 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 217 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
11: [rank11]:[W1009 04:58:12.829663605 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 220 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
17: [rank17]:[W1009 04:58:12.034799897 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 238 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 9: [rank9]:[W1009 04:58:12.835178502 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 214 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
28: [rank28]:[W1009 04:58:12.812846444 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 271 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 3: [rank3]:[W1009 04:58:12.511322868 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 196 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
26: [rank26]:[W1009 04:58:12.047304967 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 265 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
29: [rank29]:[W1009 04:58:12.815390302 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 274 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 0: [rank0]:[W1009 04:58:12.513686019 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 187 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
27: [rank27]:[W1009 04:58:12.048969972 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 268 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
32: [rank32]:[W1009 04:58:12.532623345 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 283 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
15: [rank15]:[W1009 04:58:12.969188606 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 232 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
14: [rank14]:[W1009 04:58:12.969452192 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 229 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
23: [rank23]:[W1009 04:58:12.569498495 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 256 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
22: [rank22]:[W1009 04:58:12.569906659 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 253 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 0: [AUX I 2025-10-09 04:58:13 custom_callbacks:129] Starting training warmup
 0: [AUX I 2025-10-09 04:58:13 custom_callbacks:133]     Starting warmup step 0
66: [rank66]:[W1009 04:58:15.388453942 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
67: [rank67]:[W1009 04:58:15.389828418 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
10: [rank10]:[W1009 04:58:15.231751911 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
11: [rank11]:[W1009 04:58:15.234025915 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
70: [rank70]:[W1009 04:58:15.771583403 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
71: [rank71]:[W1009 04:58:15.772054574 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
18: [rank18]:[W1009 04:58:15.526079579 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
34: [rank34]:[W1009 04:58:15.016444594 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
35: [rank35]:[W1009 04:58:15.016764054 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
62: [rank62]:[W1009 04:58:15.059467422 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
63: [rank63]:[W1009 04:58:15.060477381 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
55: [rank55]:[W1009 04:58:15.832585152 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
19: [rank19]:[W1009 04:58:15.531067968 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
46: [rank46]:[W1009 04:58:15.110721290 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
47: [rank47]:[W1009 04:58:15.112288983 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
54: [rank54]:[W1009 04:58:15.836231479 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 8: [rank8]:[W1009 04:58:15.417883034 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
68: [rank68]:[W1009 04:58:15.872759293 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
69: [rank69]:[W1009 04:58:15.873022142 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
61: [rank61]:[W1009 04:58:15.155040894 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
30: [rank30]:[W1009 04:58:15.399136751 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
31: [rank31]:[W1009 04:58:15.399234416 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
65: [rank65]:[W1009 04:58:15.584010938 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
60: [rank60]:[W1009 04:58:15.155289760 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
33: [rank33]:[W1009 04:58:15.113824406 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
42: [rank42]:[W1009 04:58:15.367143083 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
43: [rank43]:[W1009 04:58:15.367255724 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
64: [rank64]:[W1009 04:58:15.586128876 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
53: [rank53]:[W1009 04:58:15.929509794 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
52: [rank52]:[W1009 04:58:15.930950379 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
57: [rank57]:[W1009 04:58:15.214687130 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 7: [rank7]:[W1009 04:58:15.100054282 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 6: [rank6]:[W1009 04:58:15.100392749 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 9: [rank9]:[W1009 04:58:15.429831588 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 3: [rank3]:[W1009 04:58:15.105781144 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 2: [rank2]:[W1009 04:58:15.111900479 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
32: [rank32]:[W1009 04:58:15.127716393 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
15: [rank15]:[W1009 04:58:15.562733651 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
14: [rank14]:[W1009 04:58:15.563297464 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
56: [rank56]:[W1009 04:58:15.228345702 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 0: [rank0]:[W1009 04:58:15.117930373 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
22: [rank22]:[W1009 04:58:15.164117241 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
23: [rank23]:[W1009 04:58:15.164341754 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 1: [rank1]:[W1009 04:58:16.189575340 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
39: [rank39]:[W1009 04:58:16.953970087 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
40: [rank40]:[W1009 04:58:16.463223513 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
44: [rank44]:[W1009 04:58:16.301590274 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
41: [rank41]:[W1009 04:58:16.468737214 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 4: [rank4]:[W1009 04:58:16.197021256 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
45: [rank45]:[W1009 04:58:16.306910127 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
38: [rank38]:[W1009 04:58:16.962257404 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
36: [rank36]:[W1009 04:58:16.962955171 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 5: [rank5]:[W1009 04:58:16.198623861 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
17: [rank17]:[W1009 04:58:16.729569285 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
37: [rank37]:[W1009 04:58:16.963533833 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
16: [rank16]:[W1009 04:58:16.736220727 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
13: [rank13]:[W1009 04:58:16.743804941 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
24: [rank24]:[W1009 04:58:16.832490015 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
25: [rank25]:[W1009 04:58:16.832812898 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
58: [rank58]:[W1009 04:58:16.410991470 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
49: [rank49]:[W1009 04:58:16.400456054 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
59: [rank59]:[W1009 04:58:16.412728927 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
48: [rank48]:[W1009 04:58:16.400651735 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
50: [rank50]:[W1009 04:58:16.402341287 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
20: [rank20]:[W1009 04:58:16.346149185 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
12: [rank12]:[W1009 04:58:16.751687823 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
21: [rank21]:[W1009 04:58:16.347939728 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
51: [rank51]:[W1009 04:58:16.405377539 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
26: [rank26]:[W1009 04:58:16.841550374 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
27: [rank27]:[W1009 04:58:16.843159058 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
29: [rank29]:[W1009 04:58:16.613224934 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
28: [rank28]:[W1009 04:58:16.613327271 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 0: [AUX I 2025-10-09 04:58:30 custom_callbacks:150]     Finished warmup step 0, takes 17.713576793670654 s
 0: [AUX I 2025-10-09 04:58:30 custom_callbacks:133]     Starting warmup step 1
 0: [NeMo I 2025-10-09 04:58:30 full_cuda_graph:164] Capture CUDA graph for training!!!
 0: [NeMo I 2025-10-09 04:58:36 full_cuda_graph:182] CUDA graph capture done!!!
 0: [AUX I 2025-10-09 04:58:37 custom_callbacks:150]     Finished warmup step 1, takes 6.302836894989014 s
 0: [AUX I 2025-10-09 04:58:37 custom_callbacks:157] Finished training warmup: 24.034668684005737 s. 
 0: [AUX I 2025-10-09 04:58:37 custom_callbacks:176] Starting validation warmups
 0: [NeMo I 2025-10-09 04:58:42 full_cuda_graph:164] Capture CUDA graph for validation!!!
 0: [NeMo I 2025-10-09 04:58:47 full_cuda_graph:182] CUDA graph capture done!!!
 0: [AUX I 2025-10-09 04:58:47 custom_callbacks:195] Finished validation warmup: 10.544648885726929 s. 
 0: [AUX I 2025-10-09 04:58:47 custom_callbacks:202] Finished training warmup: 10.546012163162231 s. 
 0: [AUX I 2025-10-09 04:58:47 custom_callbacks:212] Time spent in run_training_warmup: 10.550007820129395s
 0: :::MLLOG {"namespace": "", "time_ms": 1759985927597, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"warmup_time": 128.63845657799902}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985927650, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"init_finished": 0.05308731200057082}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985927650, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985927651, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985927651, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985932713, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15769386291503906, "reduced_train_loss": 8.31853199005127}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 31.0, "samples_count": 1152.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985937931, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15865492820739746, "reduced_train_loss": 7.403711318969727}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 63.0, "samples_count": 2304.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985943043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1605219841003418, "reduced_train_loss": 7.039641380310059}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 95.0, "samples_count": 3456.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985948161, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16218948364257812, "reduced_train_loss": 6.805975914001465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 127.0, "samples_count": 4608.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985953287, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16001677513122559, "reduced_train_loss": 6.677916049957275}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 159.0, "samples_count": 5760.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985958414, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16002225875854492, "reduced_train_loss": 6.451488971710205}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 191.0, "samples_count": 6912.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985963544, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15986347198486328, "reduced_train_loss": 6.275079250335693}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 223.0, "samples_count": 8064.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985968852, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16080808639526367, "reduced_train_loss": 6.1932806968688965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 255.0, "samples_count": 9216.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985973993, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16019535064697266, "reduced_train_loss": 6.051263809204102}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 287.0, "samples_count": 10368.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985979127, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1598803997039795, "reduced_train_loss": 5.9379801750183105}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 319.0, "samples_count": 11520.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985982797, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16124965790643658}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985982798, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 342}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985982798, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 12312, "step": 342}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985982869, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21137547492980957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 0, "samples_count": 36}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985982950, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.08169889450073242}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1, "samples_count": 72}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985982993, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043584346771240234}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2, "samples_count": 108}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983034, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040366172790527344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3, "samples_count": 144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983075, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04130077362060547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4, "samples_count": 180}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983117, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04193830490112305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5, "samples_count": 216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983158, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041321754455566406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6, "samples_count": 252}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983200, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041188716888427734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 7, "samples_count": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983241, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04153013229370117}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 8, "samples_count": 324}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983283, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04144573211669922}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 9, "samples_count": 360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983325, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042162179946899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 10, "samples_count": 396}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983365, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040802717208862305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 11, "samples_count": 432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983406, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040718793869018555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 12, "samples_count": 468}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983448, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04201245307922363}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 13, "samples_count": 504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983490, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041539907455444336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 14, "samples_count": 540}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983531, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041159868240356445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 15, "samples_count": 576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983573, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04171633720397949}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 16, "samples_count": 612}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983615, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0420076847076416}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 17, "samples_count": 648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983656, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04108572006225586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 18, "samples_count": 684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04055643081665039}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 19, "samples_count": 720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983738, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04134631156921387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 20, "samples_count": 756}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983779, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04141879081726074}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 21, "samples_count": 792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983821, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04161858558654785}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 22, "samples_count": 828}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983862, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041338205337524414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 23, "samples_count": 864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983904, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04182910919189453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 24, "samples_count": 900}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983945, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04090070724487305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 25, "samples_count": 936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985983986, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0410304069519043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 26, "samples_count": 972}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985984027, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04098796844482422}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 27, "samples_count": 1008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985984068, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04131579399108887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 28, "samples_count": 1044}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985984072, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 5.794810771942139, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985984072, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2749497069999052}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 342}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985984072, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 12312, "step": 342}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985984072, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 342}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985985699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16037893295288086, "reduced_train_loss": 5.673718452453613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 351.0, "samples_count": 12672.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985990842, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16064667701721191, "reduced_train_loss": 5.542383193969727}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 383.0, "samples_count": 13824.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759985996060, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16725373268127441, "reduced_train_loss": 5.4046101570129395}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 415.0, "samples_count": 14976.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986001334, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15937376022338867, "reduced_train_loss": 5.277372360229492}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 447.0, "samples_count": 16128.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986006470, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16181564331054688, "reduced_train_loss": 5.240664005279541}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 479.0, "samples_count": 17280.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986011623, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1608138084411621, "reduced_train_loss": 5.069000244140625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 511.0, "samples_count": 18432.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986016764, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1617434024810791, "reduced_train_loss": 4.964068412780762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 543.0, "samples_count": 19584.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986021906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16053032875061035, "reduced_train_loss": 4.894820213317871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 575.0, "samples_count": 20736.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986027203, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.17038226127624512, "reduced_train_loss": 4.763805389404297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 607.0, "samples_count": 21888.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986032419, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16083908081054688, "reduced_train_loss": 4.723349094390869}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 639.0, "samples_count": 23040.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986037565, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15919852256774902, "reduced_train_loss": 5.140076160430908}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 671.0, "samples_count": 24192.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039664, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16254824799122686}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039665, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039665, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 24624, "step": 684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039710, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21563458442687988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 29, "samples_count": 1080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039750, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04087114334106445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 30, "samples_count": 1116}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04085969924926758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 31, "samples_count": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039832, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04069828987121582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 32, "samples_count": 1188}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039873, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04095029830932617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 33, "samples_count": 1224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039913, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04062318801879883}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 34, "samples_count": 1260}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039954, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040517330169677734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 35, "samples_count": 1296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986039996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04213428497314453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 36, "samples_count": 1332}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040037, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04096555709838867}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 37, "samples_count": 1368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040079, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041890621185302734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 38, "samples_count": 1404}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040121, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04177379608154297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 39, "samples_count": 1440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040162, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04175877571105957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 40, "samples_count": 1476}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040205, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0423586368560791}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 41, "samples_count": 1512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040246, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04154205322265625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 42, "samples_count": 1548}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04162716865539551}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 43, "samples_count": 1584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040329, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041402578353881836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 44, "samples_count": 1620}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040371, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041704654693603516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 45, "samples_count": 1656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040413, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04155707359313965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 46, "samples_count": 1692}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040455, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042138099670410156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 47, "samples_count": 1728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040496, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04148459434509277}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 48, "samples_count": 1764}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040538, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041876792907714844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 49, "samples_count": 1800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040579, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04127621650695801}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 50, "samples_count": 1836}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040621, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04151463508605957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 51, "samples_count": 1872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040663, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04168200492858887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 52, "samples_count": 1908}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04126095771789551}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 53, "samples_count": 1944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040746, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04172539710998535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 54, "samples_count": 1980}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040786, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040834665298461914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 55, "samples_count": 2016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04154610633850098}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 56, "samples_count": 2052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040870, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041912078857421875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 57, "samples_count": 2088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040872, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 4.786420822143555, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 24624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040872, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2086331659993448}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040872, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 24624, "step": 684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986040872, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986044117, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1610567569732666, "reduced_train_loss": 4.612440586090088}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 703.0, "samples_count": 25344.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986049262, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16141128540039062, "reduced_train_loss": 4.578737735748291}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 735.0, "samples_count": 26496.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986054409, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16254472732543945, "reduced_train_loss": 4.4315876960754395}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 767.0, "samples_count": 27648.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986059764, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16284823417663574, "reduced_train_loss": 4.361165523529053}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 799.0, "samples_count": 28800.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986064912, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1594398021697998, "reduced_train_loss": 4.297022819519043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 831.0, "samples_count": 29952.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986070052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16105937957763672, "reduced_train_loss": 4.417103290557861}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 863.0, "samples_count": 31104.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986075194, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16010236740112305, "reduced_train_loss": 4.2760820388793945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 895.0, "samples_count": 32256.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986080346, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15975284576416016, "reduced_train_loss": 4.294965744018555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 927.0, "samples_count": 33408.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986085554, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16879582405090332, "reduced_train_loss": 4.1234917640686035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 959.0, "samples_count": 34560.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986090857, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15986323356628418, "reduced_train_loss": 4.258913040161133}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 991.0, "samples_count": 35712.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986095998, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15996026992797852, "reduced_train_loss": 4.118960380554199}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1023.0, "samples_count": 36864.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096496, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1626422570877223}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096497, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 1026}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096497, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 36936, "step": 1026}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096542, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.2154254913330078}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 58, "samples_count": 2124}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096583, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0406496524810791}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 59, "samples_count": 2160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096623, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04076194763183594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 60, "samples_count": 2196}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096664, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040970802307128906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 61, "samples_count": 2232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096705, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04058241844177246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 62, "samples_count": 2268}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096746, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04054522514343262}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 63, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096786, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04069042205810547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 64, "samples_count": 2340}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096827, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040717124938964844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 65, "samples_count": 2376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096868, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04155588150024414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 66, "samples_count": 2412}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096909, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040537357330322266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 67, "samples_count": 2448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096950, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040961503982543945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 68, "samples_count": 2484}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986096991, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041404008865356445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 69, "samples_count": 2520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097034, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04263496398925781}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 70, "samples_count": 2556}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097077, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04256129264831543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 71, "samples_count": 2592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097118, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04141974449157715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 72, "samples_count": 2628}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097159, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04097914695739746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 73, "samples_count": 2664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041817665100097656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 74, "samples_count": 2700}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097243, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04191327095031738}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 75, "samples_count": 2736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097284, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0414121150970459}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 76, "samples_count": 2772}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097325, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04108023643493652}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 77, "samples_count": 2808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097367, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041806697845458984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 78, "samples_count": 2844}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097409, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04210305213928223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 79, "samples_count": 2880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097450, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04127788543701172}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 80, "samples_count": 2916}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097492, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041144371032714844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 81, "samples_count": 2952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097534, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042314767837524414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 82, "samples_count": 2988}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097575, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04120469093322754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 83, "samples_count": 3024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097617, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04199528694152832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 84, "samples_count": 3060}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097658, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0409703254699707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 85, "samples_count": 3096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04147052764892578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 86, "samples_count": 3132}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097702, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 4.147262096405029, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 36936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097702, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.206537001999095}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1026}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097702, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 36936, "step": 1026}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986097702, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 1026}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986102560, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16005873680114746, "reduced_train_loss": 4.08230447769165}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1055.0, "samples_count": 38016.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986107706, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16162419319152832, "reduced_train_loss": 4.017210006713867}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1087.0, "samples_count": 39168.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986112861, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16048216819763184, "reduced_train_loss": 4.045434474945068}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1119.0, "samples_count": 40320.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986118210, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16717123985290527, "reduced_train_loss": 4.166985988616943}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1151.0, "samples_count": 41472.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986123365, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16071391105651855, "reduced_train_loss": 3.9776899814605713}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1183.0, "samples_count": 42624.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986128504, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16059494018554688, "reduced_train_loss": 3.962773561477661}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1215.0, "samples_count": 43776.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986133652, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16057276725769043, "reduced_train_loss": 3.9850850105285645}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1247.0, "samples_count": 44928.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986138795, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16017794609069824, "reduced_train_loss": 3.970386028289795}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1279.0, "samples_count": 46080.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986143935, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1599891185760498, "reduced_train_loss": 3.9801783561706543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1311.0, "samples_count": 47232.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986149298, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1616353988647461, "reduced_train_loss": 3.9233834743499756}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1343.0, "samples_count": 48384.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153327, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16264493000877517}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153328, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 1368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153328, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 49248, "step": 1368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153373, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.2164475917816162}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 87, "samples_count": 3168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153413, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040374040603637695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 88, "samples_count": 3204}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153454, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04038691520690918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 89, "samples_count": 3240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153494, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04050040245056152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 90, "samples_count": 3276}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153535, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04068398475646973}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 91, "samples_count": 3312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153576, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040996551513671875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 92, "samples_count": 3348}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153617, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04052114486694336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 93, "samples_count": 3384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153658, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0414280891418457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 94, "samples_count": 3420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04113340377807617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 95, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153741, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04228377342224121}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 96, "samples_count": 3492}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153782, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04059243202209473}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 97, "samples_count": 3528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153824, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041780948638916016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 98, "samples_count": 3564}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040784358978271484}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 99, "samples_count": 3600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04126715660095215}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 100, "samples_count": 3636}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153947, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041007041931152344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 101, "samples_count": 3672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986153989, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04189324378967285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 102, "samples_count": 3708}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154030, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04090523719787598}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 103, "samples_count": 3744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154072, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04191303253173828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 104, "samples_count": 3780}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041915178298950195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 105, "samples_count": 3816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041051626205444336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 106, "samples_count": 3852}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043022871017456055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 107, "samples_count": 3888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154239, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04164314270019531}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 108, "samples_count": 3924}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154280, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04107809066772461}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 109, "samples_count": 3960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154322, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04167819023132324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 110, "samples_count": 3996}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154364, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041748762130737305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 111, "samples_count": 4032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154405, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04100656509399414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 112, "samples_count": 4068}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154446, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041167497634887695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 113, "samples_count": 4104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154487, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041362762451171875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 114, "samples_count": 4140}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154529, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0416417121887207}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 115, "samples_count": 4176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154531, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.9200353622436523, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 49248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154531, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.204026501000044}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154531, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 49248, "step": 1368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986154531, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 1368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986155841, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16094493865966797, "reduced_train_loss": 3.978705883026123}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1375.0, "samples_count": 49536.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986161000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16246485710144043, "reduced_train_loss": 3.8485841751098633}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1407.0, "samples_count": 50688.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986166145, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15928888320922852, "reduced_train_loss": 3.8274574279785156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1439.0, "samples_count": 51840.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986171288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1603386402130127, "reduced_train_loss": 3.935176372528076}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1471.0, "samples_count": 52992.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986176550, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16791510581970215, "reduced_train_loss": 3.823779582977295}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1503.0, "samples_count": 54144.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986181805, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16056013107299805, "reduced_train_loss": 3.780501365661621}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1535.0, "samples_count": 55296.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986186958, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16043353080749512, "reduced_train_loss": 4.0156073570251465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1567.0, "samples_count": 56448.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986192098, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16057848930358887, "reduced_train_loss": 3.95651912689209}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1599.0, "samples_count": 57600.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986197244, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15977692604064941, "reduced_train_loss": 3.8766403198242188}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1631.0, "samples_count": 58752.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986202394, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1603846549987793, "reduced_train_loss": 3.8928213119506836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1663.0, "samples_count": 59904.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986207725, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16933178901672363, "reduced_train_loss": 3.8135905265808105}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1695.0, "samples_count": 61056.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210199, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16277160420760237}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210200, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 1710}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210200, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 61560, "step": 1710}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210245, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21858501434326172}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 116, "samples_count": 4212}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04065251350402832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 117, "samples_count": 4248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210326, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04071044921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 118, "samples_count": 4284}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210367, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04088854789733887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 119, "samples_count": 4320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210408, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040637969970703125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 120, "samples_count": 4356}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210449, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041224002838134766}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 121, "samples_count": 4392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210490, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04116201400756836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 122, "samples_count": 4428}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210532, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04143381118774414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 123, "samples_count": 4464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210574, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041802167892456055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 124, "samples_count": 4500}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210616, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04226875305175781}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 125, "samples_count": 4536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210657, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041335344314575195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 126, "samples_count": 4572}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0418848991394043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 127, "samples_count": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210741, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041536808013916016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 128, "samples_count": 4644}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210782, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041646718978881836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 129, "samples_count": 4680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210824, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04212331771850586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 130, "samples_count": 4716}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210866, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04125714302062988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 131, "samples_count": 4752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0411529541015625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 132, "samples_count": 4788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210948, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04111647605895996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 133, "samples_count": 4824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986210989, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04122018814086914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 134, "samples_count": 4860}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211031, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041474342346191406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 135, "samples_count": 4896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211073, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04234647750854492}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 136, "samples_count": 4932}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211115, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0420689582824707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 137, "samples_count": 4968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041947364807128906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 138, "samples_count": 5004}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04050040245056152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 139, "samples_count": 5040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211239, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04136919975280762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 140, "samples_count": 5076}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211282, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04293417930603027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 141, "samples_count": 5112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211324, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04179024696350098}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 142, "samples_count": 5148}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211365, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04142117500305176}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 143, "samples_count": 5184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211406, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04074811935424805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 144, "samples_count": 5220}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211408, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.8174076080322266, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 61560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211408, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2095605329996033}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1710}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211408, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 61560, "step": 1710}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986211409, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 1710}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986214335, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16123557090759277, "reduced_train_loss": 3.8182477951049805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1727.0, "samples_count": 62208.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986219491, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1596543788909912, "reduced_train_loss": 3.783330202102661}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1759.0, "samples_count": 63360.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986224644, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15974998474121094, "reduced_train_loss": 3.768162727355957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1791.0, "samples_count": 64512.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986229796, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16008830070495605, "reduced_train_loss": 3.719017505645752}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1823.0, "samples_count": 65664.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986234969, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16856861114501953, "reduced_train_loss": 3.701474189758301}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1855.0, "samples_count": 66816.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986240304, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16110563278198242, "reduced_train_loss": 3.718966245651245}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1887.0, "samples_count": 67968.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986245457, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16146254539489746, "reduced_train_loss": 3.732670783996582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1919.0, "samples_count": 69120.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986250611, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.159956693649292, "reduced_train_loss": 3.7241036891937256}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1951.0, "samples_count": 70272.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986255766, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1604008674621582, "reduced_train_loss": 3.710160970687866}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1983.0, "samples_count": 71424.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986260911, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1594986915588379, "reduced_train_loss": 3.7658207416534424}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2015.0, "samples_count": 72576.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986266152, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16847467422485352, "reduced_train_loss": 3.6461286544799805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2047.0, "samples_count": 73728.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267020, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16260770939766056}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267021, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 2052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267021, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 73872, "step": 2052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267067, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.2197892665863037}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 145, "samples_count": 5256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267107, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0408022403717041}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 146, "samples_count": 5292}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040400028228759766}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 147, "samples_count": 5328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267188, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04057669639587402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 148, "samples_count": 5364}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267230, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04136300086975098}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 149, "samples_count": 5400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267273, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04305219650268555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 150, "samples_count": 5436}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041615962982177734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 151, "samples_count": 5472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267357, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042669057846069336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 152, "samples_count": 5508}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267400, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04272890090942383}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 153, "samples_count": 5544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267443, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04291892051696777}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 154, "samples_count": 5580}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267486, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04360389709472656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 155, "samples_count": 5616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0417327880859375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 156, "samples_count": 5652}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267571, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042665958404541016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 157, "samples_count": 5688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267614, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043172597885131836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 158, "samples_count": 5724}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267656, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04235363006591797}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 159, "samples_count": 5760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267698, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042021989822387695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 160, "samples_count": 5796}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267740, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04179954528808594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 161, "samples_count": 5832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267782, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04165959358215332}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 162, "samples_count": 5868}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267823, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0413970947265625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 163, "samples_count": 5904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04237556457519531}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 164, "samples_count": 5940}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04152393341064453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 165, "samples_count": 5976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267948, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04149770736694336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 166, "samples_count": 6012}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986267989, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04070687294006348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 167, "samples_count": 6048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268030, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04138541221618652}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 168, "samples_count": 6084}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268072, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041486501693725586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 169, "samples_count": 6120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04147958755493164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 170, "samples_count": 6156}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268155, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04203987121582031}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 171, "samples_count": 6192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041544198989868164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 172, "samples_count": 6228}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268239, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04235959053039551}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 173, "samples_count": 6264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268241, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.6867825984954834, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 73872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268241, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2211149869981455}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 2052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268241, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 73872, "step": 2052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986268241, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 2052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986272785, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16050028800964355, "reduced_train_loss": 3.6841206550598145}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2079.0, "samples_count": 74880.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986277950, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16340279579162598, "reduced_train_loss": 3.6757054328918457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2111.0, "samples_count": 76032.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986283104, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1617720127105713, "reduced_train_loss": 3.634490966796875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2143.0, "samples_count": 77184.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986288259, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16046547889709473, "reduced_train_loss": 3.686131477355957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2175.0, "samples_count": 78336.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986293415, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1601855754852295, "reduced_train_loss": 3.597730875015259}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2207.0, "samples_count": 79488.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986298763, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16101837158203125, "reduced_train_loss": 3.538480758666992}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2239.0, "samples_count": 80640.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986303924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16078948974609375, "reduced_train_loss": 3.616856098175049}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2271.0, "samples_count": 81792.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986309079, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16001629829406738, "reduced_train_loss": 3.633255958557129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2303.0, "samples_count": 82944.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986314238, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16096019744873047, "reduced_train_loss": 3.6373350620269775}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2335.0, "samples_count": 84096.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986319399, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1616373062133789, "reduced_train_loss": 3.629584312438965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2367.0, "samples_count": 85248.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323758, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16232985642982492}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323759, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 2394}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323759, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 86184, "step": 2394}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323804, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21819067001342773}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 174, "samples_count": 6300}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040924787521362305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 175, "samples_count": 6336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323886, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04075908660888672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 176, "samples_count": 6372}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323927, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0408778190612793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 177, "samples_count": 6408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986323967, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040438175201416016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 178, "samples_count": 6444}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324007, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04026484489440918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 179, "samples_count": 6480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324049, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04161405563354492}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 180, "samples_count": 6516}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324091, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04189014434814453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 181, "samples_count": 6552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324132, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0413355827331543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 182, "samples_count": 6588}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324174, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04205608367919922}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 183, "samples_count": 6624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324216, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04152059555053711}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 184, "samples_count": 6660}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324258, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04207587242126465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 185, "samples_count": 6696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324300, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041841745376586914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 186, "samples_count": 6732}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324341, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04178333282470703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 187, "samples_count": 6768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324384, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04222512245178223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 188, "samples_count": 6804}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324425, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04149985313415527}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 189, "samples_count": 6840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324468, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042994022369384766}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 190, "samples_count": 6876}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324510, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0422666072845459}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 191, "samples_count": 6912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324553, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04281330108642578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 192, "samples_count": 6948}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324597, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043977975845336914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 193, "samples_count": 6984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324640, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043369293212890625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 194, "samples_count": 7020}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324684, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043410539627075195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 195, "samples_count": 7056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324727, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0427396297454834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 196, "samples_count": 7092}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324769, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042005062103271484}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 197, "samples_count": 7128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324811, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04219532012939453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 198, "samples_count": 7164}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324853, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042127370834350586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 199, "samples_count": 7200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324897, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04386019706726074}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 200, "samples_count": 7236}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324939, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0423736572265625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 201, "samples_count": 7272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324981, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04193282127380371}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 202, "samples_count": 7308}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324984, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.6120498180389404, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 86184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324984, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.225841016999766}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 2394}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324984, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 86184, "step": 2394}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986324984, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 2394}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986326026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16921472549438477, "reduced_train_loss": 3.582709789276123}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2399.0, "samples_count": 86400.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986331303, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16122961044311523, "reduced_train_loss": 3.6402370929718018}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2431.0, "samples_count": 87552.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986336466, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16115283966064453, "reduced_train_loss": 3.6623916625976562}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2463.0, "samples_count": 88704.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986341625, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16081690788269043, "reduced_train_loss": 3.609534740447998}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2495.0, "samples_count": 89856.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986346779, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1597433090209961, "reduced_train_loss": 3.6312777996063232}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2527.0, "samples_count": 91008.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986351932, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16052985191345215, "reduced_train_loss": 3.5981881618499756}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2559.0, "samples_count": 92160.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986357226, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16832304000854492, "reduced_train_loss": 3.604661226272583}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2591.0, "samples_count": 93312.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986362454, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16072893142700195, "reduced_train_loss": 3.607783317565918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2623.0, "samples_count": 94464.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986367607, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15952610969543457, "reduced_train_loss": 3.567823648452759}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2655.0, "samples_count": 95616.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986372768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16000008583068848, "reduced_train_loss": 3.4207403659820557}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2687.0, "samples_count": 96768.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986377920, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16014671325683594, "reduced_train_loss": 3.5435495376586914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2719.0, "samples_count": 97920.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380670, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1628254172397721}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380671, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 2736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380671, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 98496, "step": 2736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380717, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21704912185668945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 203, "samples_count": 7344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380757, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04077315330505371}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 204, "samples_count": 7380}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380797, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04005718231201172}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 205, "samples_count": 7416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380838, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04019808769226074}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 206, "samples_count": 7452}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380878, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04052400588989258}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 207, "samples_count": 7488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380919, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04116559028625488}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 208, "samples_count": 7524}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986380961, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041937828063964844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 209, "samples_count": 7560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381003, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042070627212524414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 210, "samples_count": 7596}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381045, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04122567176818848}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 211, "samples_count": 7632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381088, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043013811111450195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 212, "samples_count": 7668}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381128, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04092574119567871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 213, "samples_count": 7704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381169, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04086947441101074}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 214, "samples_count": 7740}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381210, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04109597206115723}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 215, "samples_count": 7776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381252, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041207075119018555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 216, "samples_count": 7812}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381294, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041906118392944336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 217, "samples_count": 7848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381336, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0425412654876709}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 218, "samples_count": 7884}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04172325134277344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 219, "samples_count": 7920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381419, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04108858108520508}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 220, "samples_count": 7956}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381461, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042077064514160156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 221, "samples_count": 7992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381503, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04222393035888672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 222, "samples_count": 8028}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381545, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041428327560424805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 223, "samples_count": 8064}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04188895225524902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 224, "samples_count": 8100}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381628, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04117941856384277}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 225, "samples_count": 8136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381669, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0414578914642334}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 226, "samples_count": 8172}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381711, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04139542579650879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 227, "samples_count": 8208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381752, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041577816009521484}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 228, "samples_count": 8244}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381793, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04122591018676758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 229, "samples_count": 8280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381835, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041556358337402344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 230, "samples_count": 8316}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381876, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04150724411010742}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 231, "samples_count": 8352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381879, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.5521061420440674, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 98496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381879, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2085333760005597}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 2736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381879, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 98496, "step": 2736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986381879, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 2736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986384481, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.164290189743042, "reduced_train_loss": 3.647268056869507}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2751.0, "samples_count": 99072.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986389845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16043496131896973, "reduced_train_loss": 3.5171494483947754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2783.0, "samples_count": 100224.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986395003, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1607964038848877, "reduced_train_loss": 3.5401437282562256}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2815.0, "samples_count": 101376.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986400166, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16190099716186523, "reduced_train_loss": 3.5376577377319336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2847.0, "samples_count": 102528.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986405326, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1614522933959961, "reduced_train_loss": 3.543779134750366}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2879.0, "samples_count": 103680.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986410482, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16005277633666992, "reduced_train_loss": 3.561648368835449}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2911.0, "samples_count": 104832.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986415702, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16901636123657227, "reduced_train_loss": 3.523120403289795}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2943.0, "samples_count": 105984.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986420992, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16031098365783691, "reduced_train_loss": 3.4363627433776855}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2975.0, "samples_count": 107136.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986426157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1616826057434082, "reduced_train_loss": 3.5205225944519043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3007.0, "samples_count": 108288.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986431313, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16155266761779785, "reduced_train_loss": 3.461136817932129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3039.0, "samples_count": 109440.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986436476, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16115212440490723, "reduced_train_loss": 3.4640955924987793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3071.0, "samples_count": 110592.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437623, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.162994308804085}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437624, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 3078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437624, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 110808, "step": 3078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437671, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.22282958030700684}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 232, "samples_count": 8388}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437712, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04147934913635254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 233, "samples_count": 8424}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437754, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041364431381225586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 234, "samples_count": 8460}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437795, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041329145431518555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 235, "samples_count": 8496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04130291938781738}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 236, "samples_count": 8532}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437878, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04161429405212402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 237, "samples_count": 8568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437919, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04114365577697754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 238, "samples_count": 8604}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986437960, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04078245162963867}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 239, "samples_count": 8640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438001, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04156684875488281}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 240, "samples_count": 8676}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04125857353210449}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 241, "samples_count": 8712}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438084, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04158782958984375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 242, "samples_count": 8748}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438126, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04144597053527832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 243, "samples_count": 8784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438167, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041780710220336914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 244, "samples_count": 8820}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438209, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041718244552612305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 245, "samples_count": 8856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438250, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04139089584350586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 246, "samples_count": 8892}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438292, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04128432273864746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 247, "samples_count": 8928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438333, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04135847091674805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 248, "samples_count": 8964}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438374, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041351318359375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 249, "samples_count": 9000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438416, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04108905792236328}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 250, "samples_count": 9036}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438458, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042005300521850586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 251, "samples_count": 9072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438500, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0423126220703125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 252, "samples_count": 9108}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438541, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041640520095825195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 253, "samples_count": 9144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438583, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04175853729248047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 254, "samples_count": 9180}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438625, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042203664779663086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 255, "samples_count": 9216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438667, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041533708572387695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 256, "samples_count": 9252}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438708, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04149770736694336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 257, "samples_count": 9288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438750, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04137611389160156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 258, "samples_count": 9324}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438792, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04167461395263672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 259, "samples_count": 9360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438832, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04095649719238281}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 260, "samples_count": 9396}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438834, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.4900505542755127, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 110808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438834, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2116090310009895}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438835, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 110808, "step": 3078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986438835, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 3078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986443049, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16029882431030273, "reduced_train_loss": 3.4558327198028564}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3103.0, "samples_count": 111744.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986448404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16212034225463867, "reduced_train_loss": 3.439800500869751}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3135.0, "samples_count": 112896.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986453560, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16086578369140625, "reduced_train_loss": 3.540287971496582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3167.0, "samples_count": 114048.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986458709, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16055059432983398, "reduced_train_loss": 3.5773494243621826}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3199.0, "samples_count": 115200.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986463866, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16058802604675293, "reduced_train_loss": 3.4209609031677246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3231.0, "samples_count": 116352.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986469011, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16112613677978516, "reduced_train_loss": 3.4110302925109863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3263.0, "samples_count": 117504.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986474165, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16061186790466309, "reduced_train_loss": 3.4318065643310547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3295.0, "samples_count": 118656.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986479519, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16204261779785156, "reduced_train_loss": 3.421942949295044}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3327.0, "samples_count": 119808.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986484674, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16088080406188965, "reduced_train_loss": 3.4706318378448486}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3359.0, "samples_count": 120960.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986489838, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16164541244506836, "reduced_train_loss": 3.3981425762176514}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3391.0, "samples_count": 122112.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494522, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16282954730116625}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494523, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 3420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494524, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 123120, "step": 3420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494569, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21967482566833496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 261, "samples_count": 9432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494609, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04070734977722168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 262, "samples_count": 9468}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494650, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04048466682434082}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 263, "samples_count": 9504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494691, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04059553146362305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 264, "samples_count": 9540}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494731, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04089832305908203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 265, "samples_count": 9576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494772, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040760040283203125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 266, "samples_count": 9612}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04071044921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 267, "samples_count": 9648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494854, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04094886779785156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 268, "samples_count": 9684}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494895, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040834903717041016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 269, "samples_count": 9720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494936, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04176163673400879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 270, "samples_count": 9756}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986494978, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041602373123168945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 271, "samples_count": 9792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495020, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04173898696899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 272, "samples_count": 9828}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495061, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04132390022277832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 273, "samples_count": 9864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495103, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04187631607055664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 274, "samples_count": 9900}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495145, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04186677932739258}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 275, "samples_count": 9936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495188, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043192148208618164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 276, "samples_count": 9972}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495229, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040960073471069336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 277, "samples_count": 10008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495271, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04185795783996582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 278, "samples_count": 10044}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495312, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04137825965881348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 279, "samples_count": 10080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495354, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04182267189025879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 280, "samples_count": 10116}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495396, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04148125648498535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 281, "samples_count": 10152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495437, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04163932800292969}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 282, "samples_count": 10188}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495479, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04224991798400879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 283, "samples_count": 10224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495521, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041101932525634766}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 284, "samples_count": 10260}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495562, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041696786880493164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 285, "samples_count": 10296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04132556915283203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 286, "samples_count": 10332}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495645, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04173898696899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 287, "samples_count": 10368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495687, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04183459281921387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 288, "samples_count": 10404}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495728, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04067420959472656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 289, "samples_count": 10440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495729, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.452694892883301, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 123120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495730, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.207482339003036}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495730, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 123120, "step": 3420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986495730, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 3420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986496394, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16145730018615723, "reduced_train_loss": 3.548495054244995}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3423.0, "samples_count": 123264.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986501553, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16017842292785645, "reduced_train_loss": 3.463829755783081}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3455.0, "samples_count": 124416.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986506836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1680288314819336, "reduced_train_loss": 3.4733996391296387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3487.0, "samples_count": 125568.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986512092, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16036105155944824, "reduced_train_loss": 3.396043539047241}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3519.0, "samples_count": 126720.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986517238, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1607973575592041, "reduced_train_loss": 3.4071907997131348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3551.0, "samples_count": 127872.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986522378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1618359088897705, "reduced_train_loss": 3.437567710876465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3583.0, "samples_count": 129024.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986527517, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1594562530517578, "reduced_train_loss": 3.4295482635498047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3615.0, "samples_count": 130176.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986532661, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16017985343933105, "reduced_train_loss": 3.4114577770233154}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3647.0, "samples_count": 131328.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986538006, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1702861785888672, "reduced_train_loss": 3.371960401535034}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3679.0, "samples_count": 132480.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986543172, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16077804565429688, "reduced_train_loss": 3.3560705184936523}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3711.0, "samples_count": 133632.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986548329, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16092419624328613, "reduced_train_loss": 3.373405933380127}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3743.0, "samples_count": 134784.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1627895999327417}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551405, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 3762}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551405, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 135432, "step": 3762}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551451, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.22214126586914062}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 290, "samples_count": 10476}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551492, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04152107238769531}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 291, "samples_count": 10512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551533, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04080033302307129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 292, "samples_count": 10548}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551573, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04075789451599121}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 293, "samples_count": 10584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551614, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04103350639343262}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 294, "samples_count": 10620}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551655, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04066944122314453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 295, "samples_count": 10656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04108452796936035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 296, "samples_count": 10692}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551737, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04104304313659668}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 297, "samples_count": 10728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551778, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04075002670288086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 298, "samples_count": 10764}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551819, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04081249237060547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 299, "samples_count": 10800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551860, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04114985466003418}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 300, "samples_count": 10836}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551902, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04184865951538086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 301, "samples_count": 10872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551944, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04172849655151367}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 302, "samples_count": 10908}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986551985, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04138994216918945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 303, "samples_count": 10944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04104876518249512}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 304, "samples_count": 10980}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552068, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0424802303314209}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 305, "samples_count": 11016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552110, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0414433479309082}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 306, "samples_count": 11052}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552151, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041428565979003906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 307, "samples_count": 11088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552193, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04186844825744629}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 308, "samples_count": 11124}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552234, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04102206230163574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 309, "samples_count": 11160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552275, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04098963737487793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 310, "samples_count": 11196}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552316, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04086780548095703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 311, "samples_count": 11232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552358, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04207348823547363}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 312, "samples_count": 11268}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552400, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04198789596557617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 313, "samples_count": 11304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552442, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041910409927368164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 314, "samples_count": 11340}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552484, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041588544845581055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 315, "samples_count": 11376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552525, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04153275489807129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 316, "samples_count": 11412}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552566, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04094505310058594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 317, "samples_count": 11448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552608, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042000532150268555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 318, "samples_count": 11484}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552610, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.4054837226867676, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 135432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552610, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2061510240018833}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3762}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552610, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 135432, "step": 3762}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986552610, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 3762}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986554891, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16055965423583984, "reduced_train_loss": 3.3421194553375244}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3775.0, "samples_count": 135936.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986560042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16302776336669922, "reduced_train_loss": 3.383761405944824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3807.0, "samples_count": 137088.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986565230, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16925048828125, "reduced_train_loss": 3.385758399963379}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3839.0, "samples_count": 138240.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986570551, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16177654266357422, "reduced_train_loss": 3.3887531757354736}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3871.0, "samples_count": 139392.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986575707, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16096162796020508, "reduced_train_loss": 3.431513786315918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3903.0, "samples_count": 140544.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986580864, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16211247444152832, "reduced_train_loss": 3.333517074584961}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3935.0, "samples_count": 141696.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986586018, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16010427474975586, "reduced_train_loss": 3.30582332611084}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3967.0, "samples_count": 142848.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986591164, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16186833381652832, "reduced_train_loss": 3.370669364929199}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3999.0, "samples_count": 144000.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986596406, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16897797584533691, "reduced_train_loss": 3.277297019958496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4031.0, "samples_count": 145152.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986601673, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16017627716064453, "reduced_train_loss": 3.250486135482788}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4063.0, "samples_count": 146304.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986606828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16063380241394043, "reduced_train_loss": 3.4357190132141113}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4095.0, "samples_count": 147456.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608296, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16282354219590178}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608296, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 4104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608297, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 147744, "step": 4104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608342, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.2191481590270996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 319, "samples_count": 11520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608382, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04067063331604004}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 320, "samples_count": 11556}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608423, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040541887283325195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 321, "samples_count": 11592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608464, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041005849838256836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 322, "samples_count": 11628}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608505, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041124582290649414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 323, "samples_count": 11664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608546, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0408477783203125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 324, "samples_count": 11700}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04070162773132324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 325, "samples_count": 11736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608627, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0408327579498291}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 326, "samples_count": 11772}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608668, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040802955627441406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 327, "samples_count": 11808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608709, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041170358657836914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 328, "samples_count": 11844}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608751, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041207075119018555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 329, "samples_count": 11880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608792, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04127025604248047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 330, "samples_count": 11916}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608834, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04198002815246582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 331, "samples_count": 11952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608875, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04137158393859863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 332, "samples_count": 11988}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608916, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04117584228515625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 333, "samples_count": 12024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986608958, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04185605049133301}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 334, "samples_count": 12060}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0416264533996582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 335, "samples_count": 12096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041835784912109375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 336, "samples_count": 12132}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609083, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04124593734741211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 337, "samples_count": 12168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609125, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04256176948547363}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 338, "samples_count": 12204}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609167, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04129362106323242}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 339, "samples_count": 12240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609208, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041535139083862305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 340, "samples_count": 12276}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609250, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04131031036376953}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 341, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609292, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0419771671295166}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 342, "samples_count": 12348}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609333, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04184412956237793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 343, "samples_count": 12384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04171490669250488}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 344, "samples_count": 12420}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609416, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0411686897277832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 345, "samples_count": 12456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609458, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04147505760192871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 346, "samples_count": 12492}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609500, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041950225830078125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 347, "samples_count": 12528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609503, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3754944801330566, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 147744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609503, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2073884320016077}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609503, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 147744, "step": 4104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986609503, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 4104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986613391, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16158056259155273, "reduced_train_loss": 3.3163747787475586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4127.0, "samples_count": 148608.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986618545, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16209006309509277, "reduced_train_loss": 3.348315715789795}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4159.0, "samples_count": 149760.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986623696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16117095947265625, "reduced_train_loss": 3.3405258655548096}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4191.0, "samples_count": 150912.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986629044, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16137933731079102, "reduced_train_loss": 3.3589916229248047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4223.0, "samples_count": 152064.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986634194, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16007375717163086, "reduced_train_loss": 3.3676187992095947}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4255.0, "samples_count": 153216.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986639339, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16085433959960938, "reduced_train_loss": 3.3466715812683105}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4287.0, "samples_count": 154368.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986644489, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16008472442626953, "reduced_train_loss": 3.347337484359741}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4319.0, "samples_count": 155520.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986649636, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1595745086669922, "reduced_train_loss": 3.311983585357666}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4351.0, "samples_count": 156672.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986654796, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1687178611755371, "reduced_train_loss": 3.2947561740875244}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4383.0, "samples_count": 157824.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986660146, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16100692749023438, "reduced_train_loss": 3.4444632530212402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4415.0, "samples_count": 158976.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1627045842777728}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665149, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 4446}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665149, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 160056, "step": 4446}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665194, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.2202606201171875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 348, "samples_count": 12564}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665235, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04133248329162598}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 349, "samples_count": 12600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665276, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04052996635437012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 350, "samples_count": 12636}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665317, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04101085662841797}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 351, "samples_count": 12672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665358, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04092001914978027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 352, "samples_count": 12708}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665399, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04058218002319336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 353, "samples_count": 12744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665440, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04152178764343262}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 354, "samples_count": 12780}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665482, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041947126388549805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 355, "samples_count": 12816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665525, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04293560981750488}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 356, "samples_count": 12852}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665567, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041652679443359375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 357, "samples_count": 12888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665608, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04167604446411133}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 358, "samples_count": 12924}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665650, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04183006286621094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 359, "samples_count": 12960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665692, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04158830642700195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 360, "samples_count": 12996}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665733, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04162859916687012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 361, "samples_count": 13032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665775, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04197835922241211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 362, "samples_count": 13068}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665817, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04159855842590332}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 363, "samples_count": 13104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665859, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042083024978637695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 364, "samples_count": 13140}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04110383987426758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 365, "samples_count": 13176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665942, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0418858528137207}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 366, "samples_count": 13212}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986665983, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041534423828125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 367, "samples_count": 13248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666025, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04187273979187012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 368, "samples_count": 13284}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666067, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04165768623352051}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 369, "samples_count": 13320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666109, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041903018951416016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 370, "samples_count": 13356}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666150, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04110598564147949}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 371, "samples_count": 13392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04162454605102539}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 372, "samples_count": 13428}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666233, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04146146774291992}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 373, "samples_count": 13464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666274, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041329383850097656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 374, "samples_count": 13500}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666316, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041161537170410156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 375, "samples_count": 13536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666357, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04127359390258789}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 376, "samples_count": 13572}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666359, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3473520278930664, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 160056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666359, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2113326079997933}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4446}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666359, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 160056, "step": 4446}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666359, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 4446}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986666699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16216015815734863, "reduced_train_loss": 3.349106788635254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4447.0, "samples_count": 160128.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986671861, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16040396690368652, "reduced_train_loss": 3.2971320152282715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4479.0, "samples_count": 161280.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986677010, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1606767177581787, "reduced_train_loss": 3.3684334754943848}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4511.0, "samples_count": 162432.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986682166, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16072678565979004, "reduced_train_loss": 3.2907471656799316}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4543.0, "samples_count": 163584.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986687491, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1687781810760498, "reduced_train_loss": 3.3540585041046143}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4575.0, "samples_count": 164736.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986692699, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16115713119506836, "reduced_train_loss": 3.342539072036743}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4607.0, "samples_count": 165888.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986697852, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16094708442687988, "reduced_train_loss": 3.3478951454162598}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4639.0, "samples_count": 167040.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986702994, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1598198413848877, "reduced_train_loss": 3.323009729385376}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4671.0, "samples_count": 168192.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986708142, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16283702850341797, "reduced_train_loss": 3.3684282302856445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4703.0, "samples_count": 169344.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986713291, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16043663024902344, "reduced_train_loss": 3.38057804107666}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4735.0, "samples_count": 170496.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986718654, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1616666316986084, "reduced_train_loss": 3.3361244201660156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4767.0, "samples_count": 171648.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1628447845672547}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722053, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 4788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722053, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 172368, "step": 4788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722099, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.21898555755615234}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 377, "samples_count": 13608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722139, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04037928581237793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 378, "samples_count": 13644}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722179, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040381431579589844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 379, "samples_count": 13680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722220, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04035043716430664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 380, "samples_count": 13716}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722260, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040474891662597656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 381, "samples_count": 13752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722301, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04053497314453125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 382, "samples_count": 13788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722342, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04131317138671875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 383, "samples_count": 13824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722383, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04101371765136719}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 384, "samples_count": 13860}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722425, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04181480407714844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 385, "samples_count": 13896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722467, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04243898391723633}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 386, "samples_count": 13932}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722509, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04138445854187012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 387, "samples_count": 13968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722551, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04232192039489746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 388, "samples_count": 14004}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722592, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0411834716796875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 389, "samples_count": 14040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722633, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041219472885131836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 390, "samples_count": 14076}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722675, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04137015342712402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 391, "samples_count": 14112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722716, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04097151756286621}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 392, "samples_count": 14148}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722757, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04149222373962402}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 393, "samples_count": 14184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722800, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04261469841003418}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 394, "samples_count": 14220}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722842, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041895151138305664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 395, "samples_count": 14256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722884, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04199814796447754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 396, "samples_count": 14292}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722926, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042269229888916016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 397, "samples_count": 14328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986722968, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04187917709350586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 398, "samples_count": 14364}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723009, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04147839546203613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 399, "samples_count": 14400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723051, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041504859924316406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 400, "samples_count": 14436}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04194521903991699}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 401, "samples_count": 14472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723134, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04122281074523926}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 402, "samples_count": 14508}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723175, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04132890701293945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 403, "samples_count": 14544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723216, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04110217094421387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 404, "samples_count": 14580}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723259, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04208111763000488}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 405, "samples_count": 14616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723261, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3241822719573975, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 172368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723261, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2087188879995665}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723261, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 172368, "step": 4788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986723261, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 4788}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986725217, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16145658493041992, "reduced_train_loss": 3.447876214981079}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4799.0, "samples_count": 172800.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986730375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16034817695617676, "reduced_train_loss": 3.397533655166626}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4831.0, "samples_count": 173952.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986735532, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.15993356704711914, "reduced_train_loss": 3.6113667488098145}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4863.0, "samples_count": 175104.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986740683, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16096925735473633, "reduced_train_loss": 3.506849765777588}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4895.0, "samples_count": 176256.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986745903, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1679849624633789, "reduced_train_loss": 3.323129177093506}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4927.0, "samples_count": 177408.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986751200, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1607801914215088, "reduced_train_loss": 3.3669347763061523}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4959.0, "samples_count": 178560.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986756349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1603074073791504, "reduced_train_loss": 3.3397018909454346}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4991.0, "samples_count": 179712.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986761500, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16086721420288086, "reduced_train_loss": 3.3458354473114014}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5023.0, "samples_count": 180864.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986766647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1618192195892334, "reduced_train_loss": 3.2854197025299072}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5055.0, "samples_count": 182016.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986771800, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16141247749328613, "reduced_train_loss": 3.423963785171509}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5087.0, "samples_count": 183168.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986777092, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16993355751037598, "reduced_train_loss": 3.234398603439331}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5119.0, "samples_count": 184320.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986778957, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16285428829532497}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986778958, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 5130}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986778958, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 184680, "step": 5130}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779004, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.22491741180419922}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 406, "samples_count": 14652}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779044, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04070425033569336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 407, "samples_count": 14688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779084, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04024219512939453}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 408, "samples_count": 14724}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779126, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04170870780944824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 409, "samples_count": 14760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779167, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04126596450805664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 410, "samples_count": 14796}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779209, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041205644607543945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 411, "samples_count": 14832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779250, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04087972640991211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 412, "samples_count": 14868}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779290, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04093647003173828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 413, "samples_count": 14904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779331, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040761709213256836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 414, "samples_count": 14940}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779372, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041216135025024414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 415, "samples_count": 14976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779414, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04165935516357422}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 416, "samples_count": 15012}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779455, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041352033615112305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 417, "samples_count": 15048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779497, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04195570945739746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 418, "samples_count": 15084}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779539, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04170966148376465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 419, "samples_count": 15120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779581, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04165959358215332}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 420, "samples_count": 15156}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779622, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04114246368408203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 421, "samples_count": 15192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779664, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04196977615356445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 422, "samples_count": 15228}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779705, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041486501693725586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 423, "samples_count": 15264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779747, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04202461242675781}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 424, "samples_count": 15300}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779788, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040877580642700195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 425, "samples_count": 15336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779829, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041053056716918945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 426, "samples_count": 15372}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779871, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041768550872802734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 427, "samples_count": 15408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779913, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04163861274719238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 428, "samples_count": 15444}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779954, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04171586036682129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 429, "samples_count": 15480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986779995, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.040703535079956055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 430, "samples_count": 15516}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780037, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04174947738647461}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 431, "samples_count": 15552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780079, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04159855842590332}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 432, "samples_count": 15588}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780120, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04190778732299805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 433, "samples_count": 15624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780162, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04145193099975586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 434, "samples_count": 15660}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780164, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3055078983306885, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 184680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780164, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2073256580006273}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 5130}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780164, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 184680, "step": 5130}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986780164, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12312, "step": 5130}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986783737, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16336774826049805, "reduced_train_loss": 3.210555076599121}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5151.0, "samples_count": 185472.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986788891, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16025781631469727, "reduced_train_loss": 3.306574821472168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5183.0, "samples_count": 186624.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986794048, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16178417205810547, "reduced_train_loss": 3.336984872817993}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5215.0, "samples_count": 187776.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986799192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16031312942504883, "reduced_train_loss": 3.2656514644622803}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5247.0, "samples_count": 188928.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986804338, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16073369979858398, "reduced_train_loss": 3.3516037464141846}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5279.0, "samples_count": 190080.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986809689, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1608438491821289, "reduced_train_loss": 3.3011348247528076}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5311.0, "samples_count": 191232.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986814835, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16129469871520996, "reduced_train_loss": 3.274688959121704}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5343.0, "samples_count": 192384.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986819983, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16008353233337402, "reduced_train_loss": 3.3021786212921143}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5375.0, "samples_count": 193536.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986825130, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1601550579071045, "reduced_train_loss": 3.2888858318328857}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5407.0, "samples_count": 194688.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986830278, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16072940826416016, "reduced_train_loss": 3.15531063079834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5439.0, "samples_count": 195840.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835485, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1701359748840332, "reduced_train_loss": 3.140791416168213}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5471.0, "samples_count": 196992.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835662, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.16227480230116387}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835663, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12312, "step": 5472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835664, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 196992, "step": 5472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835709, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.22463464736938477}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 435, "samples_count": 15696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835750, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04154682159423828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 436, "samples_count": 15732}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04043984413146973}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 437, "samples_count": 15768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835833, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0417020320892334}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 438, "samples_count": 15804}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835875, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042818307876586914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 439, "samples_count": 15840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835920, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04476594924926758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 440, "samples_count": 15876}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986835963, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042662620544433594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 441, "samples_count": 15912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836006, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04348015785217285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 442, "samples_count": 15948}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836049, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042943716049194336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 443, "samples_count": 15984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043498992919921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 444, "samples_count": 16020}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836135, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04232454299926758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 445, "samples_count": 16056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836178, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04279971122741699}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 446, "samples_count": 16092}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836219, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041393280029296875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 447, "samples_count": 16128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836263, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04337334632873535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 448, "samples_count": 16164}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836306, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04304337501525879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 449, "samples_count": 16200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0431671142578125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 450, "samples_count": 16236}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836391, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.041710853576660156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 451, "samples_count": 16272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836434, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04335308074951172}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 452, "samples_count": 16308}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836477, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04323554039001465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 453, "samples_count": 16344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836520, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04272770881652832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 454, "samples_count": 16380}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836563, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04352116584777832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 455, "samples_count": 16416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836605, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042057037353515625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 456, "samples_count": 16452}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836649, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.043233633041381836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 457, "samples_count": 16488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836691, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042525291442871094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 458, "samples_count": 16524}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836734, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042427778244018555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 459, "samples_count": 16560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836775, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04162168502807617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 460, "samples_count": 16596}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836817, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042258501052856445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 461, "samples_count": 16632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836860, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042330026626586914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 462, "samples_count": 16668}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836903, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.042733192443847656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 463, "samples_count": 16704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836906, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.2776505947113037, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 196992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.2435935870016692}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 5472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836906, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 196992, "step": 5472}}
 0: Average train_step_time 0.16212442878916947
 0: :::MLLOG {"namespace": "", "time_ms": 1759986836925, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 106, "step": 5472, "samples_count": 196992, "status": "success"}}
++ date +%s
+ echo 'RUNANDTIME_STOP 1759986885'
RUNANDTIME_STOP 1759986885
+ set -e
