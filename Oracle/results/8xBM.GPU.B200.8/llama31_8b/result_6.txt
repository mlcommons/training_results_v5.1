+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/llama3_8b_20251008.sqsh 103 8 GPU-[1767,5857,4943,4715,239,2633,1335,6290] '\''unknown'\'' DGXB200_8x8x1xtp1pp1cp2_8b'
:::DLPAL /mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/llama3_8b_20251008.sqsh 103 8 GPU-[1767,5857,4943,4715,239,2633,1335,6290] 'unknown' DGXB200_8x8x1xtp1pp1cp2_8b
++ srun -N1 -n1 --container-name=llama31_8b_103 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8592+","host_processor_core_count":"64","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"2.27.7","cublas_version":"13.0.2.14","cudnn_version":"9.13.1.26","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.8.0-1026-oracle","nvidia_kernel_driver":"580.65.06"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8592+","host_processor_core_count":"64","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.09","framework_name":"","other_software_stack":{"cuda_version":"13.0.1.012","cuda_driver_version":"580.82.07","nccl_version":"2.27.7","cublas_version":"13.0.2.14","cudnn_version":"9.13.1.26","trt_version":"10.13.3.9","dali_version":"1.51.2","mofed_version":"5.4-rdmacore56.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.8.0-1026-oracle","nvidia_kernel_driver":"580.65.06"},"operating_system":"Ubuntu 24.04.3 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=llama31_8b_103 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID  
+ export SEED=21838
+ SEED=21838
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c '
                 host=$(hostname)
                 echo "$host sync_start"
                 sync && echo "$host sync_done"
                 cache_before=$(awk "/^Cached:/ {print \$2}" /proc/meminfo)
                 sudo /sbin/sysctl vm.drop_caches=3
                 cache_after=$(awk "/^Cached:/ {print \$2}" /proc/meminfo)
                 echo "$host cache_cleared ${cache_before}kB to ${cache_after}kB"
            '
GPU-1767 sync_start
GPU-4715 sync_start
GPU-1335 sync_start
GPU-2633 sync_start
GPU-1767 sync_done
GPU-5857 sync_start
GPU-4943 sync_start
GPU-6290 sync_start
GPU-4715 sync_done
GPU-2633 sync_done
GPU-1335 sync_done
GPU-5857 sync_done
GPU-6290 sync_done
GPU-4943 sync_done
GPU-239 sync_start
GPU-239 sync_done
vm.drop_caches = 3
vm.drop_caches = 3
GPU-1767 cache_cleared 33030916kB to 632352kB
vm.drop_caches = 3
GPU-4715 cache_cleared 32972432kB to 632340kB
vm.drop_caches = 3
GPU-6290 cache_cleared 33123520kB to 633816kB
vm.drop_caches = 3
vm.drop_caches = 3
GPU-1335 cache_cleared 32896344kB to 588472kB
vm.drop_caches = 3
GPU-5857 cache_cleared 33618964kB to 628560kB
GPU-4943 cache_cleared 33573240kB to 631668kB
GPU-2633 cache_cleared 33224440kB to 589940kB
vm.drop_caches = 3
GPU-239 cache_cleared 33033828kB to 632796kB
+ srun --ntasks-per-node=1 --container-name=llama31_8b_103 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_common.callbacks import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
:::MLLOG {"namespace": "", "time_ms": 1760051382088, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382091, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382159, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382184, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382216, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382254, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1760051382272, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1760051382'
RUNANDTIME_START 1760051382
+ SLURM_HOSTFILE=/mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/logs/hostfile.103.l9dq
+ NV_MLPERF_DEBUG=1
+ srun -l --mpi=pmi2 --ntasks-per-node=8 --distribution=arbitrary --time=30 --container-name=llama31_8b_103 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/logs:/results,/mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/logs/251009230844793017776_npy_index:/npy_index,/mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/logs/mem_dump:/mem_dump,/mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/data/8b/tokenizer:/workspace/llm/nemo_tokenizer:ro,/mnt/lfs/sce/mlperf_train_v51/llama31_8b/b200/data/8b:/preproc_data:ro --container-workdir=/workspace/llm --container-env=MASTER_PORT,MASTER_ADDR,NCCL_SHARP_GROUP_SIZE_THRESH,NCCL_NVLS_ENABLE slurm2pytorch ./run_and_time.sh
 0: slurm2pytorch: MASTER_ADDR=GPU-1335 MASTER_PORT=29500 WORLD_SIZE=64
62: LOAD_CHECKPOINT=
29: LOAD_CHECKPOINT=
15: LOAD_CHECKPOINT=
52: LOAD_CHECKPOINT=
60: LOAD_CHECKPOINT=
19: LOAD_CHECKPOINT=
58: LOAD_CHECKPOINT=
59: LOAD_CHECKPOINT=
13: LOAD_CHECKPOINT=
24: LOAD_CHECKPOINT=
26: LOAD_CHECKPOINT=
24: Hello from: GPU-2633
49: LOAD_CHECKPOINT=
53: LOAD_CHECKPOINT=
51: LOAD_CHECKPOINT=
 7: LOAD_CHECKPOINT=
34: LOAD_CHECKPOINT=
40: LOAD_CHECKPOINT=
11: LOAD_CHECKPOINT=
40: Hello from: GPU-4943
56: LOAD_CHECKPOINT=
41: LOAD_CHECKPOINT=
56: Hello from: GPU-6290
50: LOAD_CHECKPOINT=
25: LOAD_CHECKPOINT=
28: LOAD_CHECKPOINT=
54: LOAD_CHECKPOINT=
57: LOAD_CHECKPOINT=
10: LOAD_CHECKPOINT=
35: LOAD_CHECKPOINT=
39: LOAD_CHECKPOINT=
27: LOAD_CHECKPOINT=
 8: LOAD_CHECKPOINT=
 9: LOAD_CHECKPOINT=
45: LOAD_CHECKPOINT=
 8: Hello from: GPU-1767
31: LOAD_CHECKPOINT=
42: LOAD_CHECKPOINT=
30: LOAD_CHECKPOINT=
48: LOAD_CHECKPOINT=
48: Hello from: GPU-5857
36: LOAD_CHECKPOINT=
 2: LOAD_CHECKPOINT=
43: LOAD_CHECKPOINT=
12: LOAD_CHECKPOINT=
33: LOAD_CHECKPOINT=
21: LOAD_CHECKPOINT=
61: LOAD_CHECKPOINT=
44: LOAD_CHECKPOINT=
 1: LOAD_CHECKPOINT=
38: LOAD_CHECKPOINT=
14: LOAD_CHECKPOINT=
63: LOAD_CHECKPOINT=
47: LOAD_CHECKPOINT=
55: LOAD_CHECKPOINT=
 3: LOAD_CHECKPOINT=
 4: LOAD_CHECKPOINT=
17: LOAD_CHECKPOINT=
16: LOAD_CHECKPOINT=
16: Hello from: GPU-239
20: LOAD_CHECKPOINT=
37: LOAD_CHECKPOINT=
32: LOAD_CHECKPOINT=
 0: LOAD_CHECKPOINT=
32: Hello from: GPU-4715
22: LOAD_CHECKPOINT=
46: LOAD_CHECKPOINT=
18: LOAD_CHECKPOINT=
 0: Hello from: GPU-1335
 0: running LLM benchmark
 0: Extra args:  exp_manager.explicit_log_dir="/results/251009230844793017776"
 5: LOAD_CHECKPOINT=
 6: LOAD_CHECKPOINT=
23: LOAD_CHECKPOINT=
62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
62: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
62:   import pynvml  # type: ignore[import]
60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
19: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
19:   import pynvml  # type: ignore[import]
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
29: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
29:   import pynvml  # type: ignore[import]
63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
54: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
54:   import pynvml  # type: ignore[import]
52: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
52:   import pynvml  # type: ignore[import]
48: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
48:   import pynvml  # type: ignore[import]
35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
32: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
32:   import pynvml  # type: ignore[import]
34: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
34:   import pynvml  # type: ignore[import]
37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
40: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
40:   import pynvml  # type: ignore[import]
36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=64
 7: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 7:   import pynvml  # type: ignore[import]
 4: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 4:   import pynvml  # type: ignore[import]
41: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
41:   import pynvml  # type: ignore[import]
21: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
21:   import pynvml  # type: ignore[import]
45: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
45:   import pynvml  # type: ignore[import]
43: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
43:   import pynvml  # type: ignore[import]
25: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
25:   import pynvml  # type: ignore[import]
57: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
57:   import pynvml  # type: ignore[import]
59: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
59:   import pynvml  # type: ignore[import]
46: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
46:   import pynvml  # type: ignore[import]
47: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
47:   import pynvml  # type: ignore[import]
44: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
44:   import pynvml  # type: ignore[import]
22: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
22:   import pynvml  # type: ignore[import]
18: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
18:   import pynvml  # type: ignore[import]
 1: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 1:   import pynvml  # type: ignore[import]
20: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
20:   import pynvml  # type: ignore[import]
23: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
23:   import pynvml  # type: ignore[import]
55: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
55:   import pynvml  # type: ignore[import]
42: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
42:   import pynvml  # type: ignore[import]
 3: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 3:   import pynvml  # type: ignore[import]
61: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
61:   import pynvml  # type: ignore[import]
 0: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 0:   import pynvml  # type: ignore[import]
60: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
60:   import pynvml  # type: ignore[import]
63: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
63:   import pynvml  # type: ignore[import]
 2: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 2:   import pynvml  # type: ignore[import]
 5: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 5:   import pynvml  # type: ignore[import]
 6: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 6:   import pynvml  # type: ignore[import]
56: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
56:   import pynvml  # type: ignore[import]
58: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
58:   import pynvml  # type: ignore[import]
13: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
13:   import pynvml  # type: ignore[import]
11: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
11:   import pynvml  # type: ignore[import]
50: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
50:   import pynvml  # type: ignore[import]
51: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
51:   import pynvml  # type: ignore[import]
53: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
53:   import pynvml  # type: ignore[import]
49: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
49:   import pynvml  # type: ignore[import]
17: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
17:   import pynvml  # type: ignore[import]
16: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
16:   import pynvml  # type: ignore[import]
35: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
35:   import pynvml  # type: ignore[import]
15: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
15:   import pynvml  # type: ignore[import]
33: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
33:   import pynvml  # type: ignore[import]
10: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
10:   import pynvml  # type: ignore[import]
 9: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 9:   import pynvml  # type: ignore[import]
12: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
12:   import pynvml  # type: ignore[import]
14: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
14:   import pynvml  # type: ignore[import]
 8: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
 8:   import pynvml  # type: ignore[import]
37: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
37:   import pynvml  # type: ignore[import]
38: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
38:   import pynvml  # type: ignore[import]
36: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
36:   import pynvml  # type: ignore[import]
39: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
39:   import pynvml  # type: ignore[import]
27: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
27:   import pynvml  # type: ignore[import]
28: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
28:   import pynvml  # type: ignore[import]
31: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
31:   import pynvml  # type: ignore[import]
26: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
26:   import pynvml  # type: ignore[import]
24: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
24:   import pynvml  # type: ignore[import]
30: /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
30:   import pynvml  # type: ignore[import]
62: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
57: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
61: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
59: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
63: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
60: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
18: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
20: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
22: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
23: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
17: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
16: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
21: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
19: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
56: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
58: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
24: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
27: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
25: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
30: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
29: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
26: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
50: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
49: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
48: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
52: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
51: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
54: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
36: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
32: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
39: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
33: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
34: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
35: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
28: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
31: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
53: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
55: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
37: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
38: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
40: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
44: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
46: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
43: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
47: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
41: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
45: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
42: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 5: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 2: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 6: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 4: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 3: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 0: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 7: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 1: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 8: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
14: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
11: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
10: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 9: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
15: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
12: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
13: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
 0: :::MLLOG {"namespace": "", "time_ms": 1760051405835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 750}}
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] 
 0:     
 0:     **************** Experiment configuration ****************
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] 
 0:     model:
 0:       data:
 0:         data_prefix:
 0:           train:
 0:           - 0.5
 0:           - /preproc_data/c4_en_6_c4_spm_text_document
 0:           - 0.5
 0:           - /preproc_data/c4_en_7_c4_spm_text_document
 0:           validation:
 0:           - /preproc_data/c4_en_validation_subset_c4_spm_text_document
 0:           test:
 0:           - /preproc_data/c4_en_validation_subset_c4_spm_text_document
 0:         index_mapping_dir: /npy_index
 0:         splits_string: null
 0:         validation_drop_last: false
 0:         pad_samples_to_global_batch_size: true
 0:         shuffle_documents: false
 0:         legacy_dataset: true
 0:         delay_data_init: true
 0:         delay_data_mmap: true
 0:         no_seqlen_plus_one_input_tokens: true
 0:         exchange_indices_distributed: true
 0:         mock_dataset: false
 0:         mock_tokenizer_vocab_size: 32000
 0:       mcore_gpt: true
 0:       name: megatron_gpt_full_te_layer_autocast
 0:       micro_batch_size: 1
 0:       tensor_model_parallel_size: 1
 0:       pipeline_model_parallel_size: 1
 0:       virtual_pipeline_model_parallel_size: null
 0:       context_parallel_size: 2
 0:       expert_model_parallel_size: 1
 0:       global_batch_size: 32
 0:       use_tp_pp_dp_mapping: true
 0:       base_config: 8b
 0:       overwritten_attributes:
 0:         num_layers: 32
 0:         enable_cuda_graph: 1
 0:         cuda_graph_scope: full_iteration
 0:       encoder_seq_length: 8192
 0:       overlap_p2p_comm: true
 0:       batch_p2p_comm: false
 0:       account_for_embedding_in_pipeline_split: false
 0:       account_for_loss_in_pipeline_split: false
 0:       external_cuda_graph: false
 0:       defer_embedding_wgrad_compute: false
 0:       wgrad_deferral_limit: 50
 0:       tokenizer:
 0:         model: /workspace/llm/nemo_tokenizer
 0:       gradient_accumulation_fusion: true
 0:       fused_single_qkv_rope: true
 0:       cross_entropy_loss_fusion: true
 0:       deterministic_mode: false
 0:       seed: 21838
 0:       resume_from_checkpoint: null
 0:       dist_ckpt_format: torch_dist
 0:       dist_ckpt_parallel_load: true
 0:       sync_batch_comm: false
 0:       activations_checkpoint_granularity: null
 0:       activations_checkpoint_method: null
 0:       activations_checkpoint_num_layers: null
 0:       sequence_parallel: false
 0:       transformer_engine: true
 0:       fp8: true
 0:       fp8_hybrid: true
 0:       fp8_recipe: tensorwise
 0:       fp8_amax_history_len: 1
 0:       fp8_amax_compute_algo: most_recent
 0:       fp4: false
 0:       fp4_recipe: nvfp4
 0:       reduce_amax: true
 0:       tp_only_amax_red: true
 0:       first_last_layers_bf16: false
 0:       num_layers_at_start_in_bf16: 0
 0:       num_layers_at_end_in_bf16: 0
 0:       fp8_dot_product_attention: false
 0:       use_te_rng_tracker: true
 0:       use_transformer_engine_op_fuser: true
 0:       cross_entropy_fusion_impl: te
 0:       ub_tp_comm_overlap: false
 0:       tp_comm_overlap_ag: true
 0:       tp_comm_overlap_rs: true
 0:       nccl_communicator_config_path: /workspace/llm/conf/nccl/custom_communicator_cta.yaml
 0:       sharp: false
 0:       optim:
 0:         overlap_grad_reduce: true
 0:         overlap_param_gather: true
 0:         align_param_gather: false
 0:         use_distributed_optimizer: true
 0:         bucket_size: 768000000
 0:         fp8_param_gather: true
 0:         overlap_param_gather_with_optim_step: false
 0:         lr: 0.0008
 0:         sched:
 0:           min_lr: 8.0e-05
 0:           warmup_steps: 96
 0:           max_steps_for_lr_sched: 43200000.0
 0:         lock_timeout: null
 0:       gc_interval_train: 10000
 0:       gc_interval_valid: 10000
 0:       nsys_profile:
 0:         enabled: false
 0:         start_step: 10
 0:         end_step: 10
 0:         ranks:
 0:         - 0
 0:         gen_shape: false
 0:         nvtx_ranges: false
 0:       custom:
 0:         log_metrics: NEMO
 0:         init_global_step: 0
 0:         target_log_ppl: 3.3
 0:         use_distributed_checkpointing: 1
 0:         run_warmup_on_synth_data: 1
 0:         reset_fp8_stats_after_warmup: 1
 0:         pre_validate: 0
 0:         override_zero_consumed_samples: 1
 0:         force_success_status: 0
 0:         warmup_train_steps: 2
 0:         warmup_validation_steps: 2
 0:         extend_run_evals: 0
 0:         disable_nemo_logs: true
 0:     proxy_gbs: 32
 0:     is_proxy_run: false
 0:     skip_evals: 12
 0:     default_val_check_interval: 384
 0:     trainer:
 0:       devices: 8
 0:       num_nodes: 8
 0:       precision: bf16
 0:       max_steps: 1200000
 0:       max_epochs: 1
 0:       log_every_n_steps: 32
 0:       val_check_interval: 384
 0:       limit_val_batches: 32
 0:       limit_test_batches: 1
 0:       limit_train_batches: null
 0:       enable_progress_bar: false
 0:       num_sanity_val_steps: 0
 0:     exp_manager:
 0:       explicit_log_dir: /results/251009230844793017776
 0:       resume_if_exists: 0
 0:       create_checkpoint_callback: 0
 0:       checkpoint_callback_params:
 0:         save_top_k: 1
 0:         mode: max
 0:         every_n_epochs: 0
 0:         save_last: true
 0:       log_step_timing: true
 0:       create_tensorboard_logger: false
 0:       log_global_rank_0_only: true
 0:     misc:
 0:       print_config: false
 0:       memory_profiler:
 0:         enable: false
 0:         file_prefix: memdump
 0:         max_entries: 1000000
 0:         rank_0_only: true
 0:         start_location: init
 0:         end_location: train_start
 0:         force_oom_before_stop: false
 0:         possible_oom: false
 0:     
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] 
 0:     TP: 1; PP: 1; VP: None; CP: 2
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] ======== Benchmarked setups ========
 7: [W1009 23:10:06.335873602 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.336030919 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.336196427 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.336364998 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.336527364 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.337531833 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.337705541 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.337867127 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.338045195 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.338211075 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.339194529 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.339354321 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.339513995 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.339679265 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.339846215 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.340827570 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.340988728 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.341155374 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.341317948 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.341477472 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.342478792 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.342650944 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.342873383 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.343052094 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.343262878 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.344354506 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.344516060 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.344685713 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.344856633 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.345065874 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.346100451 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.346266994 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.346434657 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.346604943 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.346934643 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.347803626 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.348110141 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.348279963 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.348451989 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.348613583 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.348944031 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.349771188 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 7: [W1009 23:10:06.350104266 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 6: [W1009 23:10:06.350262802 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 5: [W1009 23:10:06.350431968 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 1: [W1009 23:10:06.350598241 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 4: [W1009 23:10:06.350938129 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.351110684 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.351537004 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.351929359 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.352341499 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.352758651 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.353156902 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 2: [W1009 23:10:06.353578264 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 3: [W1009 23:10:06.441905719 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 3: [W1009 23:10:06.442296300 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 3: [W1009 23:10:06.442674688 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 3: [W1009 23:10:06.443052318 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 0: GPU available: True (cuda), used: True
 3: [W1009 23:10:06.443424151 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 0: TPU available: False, using: 0 TPU cores
 0: HPU available: False, using: 0 HPUs
 3: [W1009 23:10:06.443783513 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 0: `Trainer(limit_test_batches=1)` was configured so 1 batch will be used.
 3: [W1009 23:10:06.444149021 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 3: [W1009 23:10:06.444500283 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 3: [W1009 23:10:06.444886569 socket.cpp:755] [c10d] The client socket has failed to connect to [GPU-1335]:29500 (errno: 22 - Invalid argument).
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406920, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama31_8b", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406920, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406921, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406921, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406921, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 566}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406921, "event_type": "POINT_IN_TIME", "key": "seed", "value": 21838, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406921, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 32, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406921, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1.0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406922, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 8192, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406922, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 1024, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406922, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1574207408, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406922, "event_type": "POINT_IN_TIME", "key": "init_checkpoint_step", "value": 0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406922, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406923, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0008, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406923, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406923, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.95, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406923, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-05, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406923, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.1, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406923, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 1.0, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406924, "event_type": "POINT_IN_TIME", "key": "opt_end_learning_rate", "value": 8e-05, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406924, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 96, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406924, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 1199904, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406924, "event_type": "POINT_IN_TIME", "key": "max_steps", "value": 1200000, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406924, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_schedule", "value": "cosine with linear warmup", "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051406925, "event_type": "POINT_IN_TIME", "key": "target_accuracy", "value": 3.3, "metadata": {"file": "/workspace/llm/pretrain.py", "lineno": 601}}
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] ======== Benchmarked fit ========
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Experiments will be logged at /workspace/llm/nemo_experiments/default/2025-10-09_23-10-06
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has data parallel group : [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has context parallel group: [0, 1]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23], [24, 25], [26, 27], [28, 29], [30, 31], [32, 33], [34, 35], [36, 37], [38, 39], [40, 41], [42, 43], [44, 45], [46, 47], [48, 49], [50, 51], [52, 53], [54, 55], [56, 57], [58, 59], [60, 61], [62, 63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has model parallel group: [0]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All expert tensor parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23], [24, 25], [26, 27], [28, 29], [30, 31], [32, 33], [34, 35], [36, 37], [38, 39], [40, 41], [42, 43], [44, 45], [46, 47], [48, 49], [50, 51], [52, 53], [54, 55], [56, 57], [58, 59], [60, 61], [62, 63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has expert tensor parallel rank: 0
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has embedding group: [0]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2025-10-09 23:10:06 nemo_logging:393] Rank 0 has embedding rank: 0
 0: [AUX I 2025-10-09 23:10:06 megatron_strategy:607] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
32: [W1009 23:10:06.666587535 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
38: [W1009 23:10:06.672235253 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
48: [W1009 23:10:06.052370667 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
29: [W1009 23:10:06.257265885 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
31: [W1009 23:10:06.270726745 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
58: [W1009 23:10:06.807184472 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
49: [W1009 23:10:07.093417604 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
34: [W1009 23:10:07.728188634 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
25: [W1009 23:10:07.337925984 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
24: [W1009 23:10:07.339159688 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
51: [W1009 23:10:07.140813859 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
46: [W1009 23:10:07.129342386 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
44: [W1009 23:10:07.130961144 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
16: [W1009 23:10:07.831037177 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
63: [W1009 23:10:07.886790775 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
33: [W1009 23:10:07.785134964 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
28: [W1009 23:10:07.376735667 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 6: [W1009 23:10:07.615274569 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
18: [W1009 23:10:07.870810889 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
35: [W1009 23:10:07.821142686 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
10: [W1009 23:10:07.307867881 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
11: [W1009 23:10:07.307861376 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
12: [W1009 23:10:07.307860637 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
14: [W1009 23:10:07.307860203 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
15: [W1009 23:10:07.307867209 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 8: [W1009 23:10:07.307879811 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 9: [W1009 23:10:07.307882066 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
13: [W1009 23:10:07.307884978 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
37: [W1009 23:10:07.840284351 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
27: [W1009 23:10:07.435214316 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
56: [W1009 23:10:07.988422532 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
52: [W1009 23:10:07.261714536 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
39: [W1009 23:10:07.895265845 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
53: [W1009 23:10:07.284650374 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
36: [W1009 23:10:07.921122175 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
45: [W1009 23:10:07.285059500 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
21: [W1009 23:10:07.979835240 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 5: [W1009 23:10:07.732450418 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
20: [W1009 23:10:07.980781764 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
17: [W1009 23:10:07.981923151 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
47: [W1009 23:10:07.294120185 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
19: [W1009 23:10:07.009830529 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
60: [W1009 23:10:07.070982228 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
22: [W1009 23:10:07.026700342 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 7: [W1009 23:10:07.793050233 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
55: [W1009 23:10:07.369532842 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
42: [W1009 23:10:07.363046399 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
40: [W1009 23:10:07.364961390 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
43: [W1009 23:10:07.392982998 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 4: [W1009 23:10:07.852851181 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
54: [W1009 23:10:07.424050287 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
50: [W1009 23:10:07.437568685 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
41: [W1009 23:10:07.468001351 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 3: [W1009 23:10:07.982802712 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
61: [W1009 23:10:07.314062319 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
26: [W1009 23:10:07.789626874 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 1: [W1009 23:10:07.052457332 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 2: [W1009 23:10:07.067472916 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
57: [W1009 23:10:07.422971952 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
30: [W1009 23:10:07.913807054 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
62: [W1009 23:10:07.488345001 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
23: [W1009 23:10:07.453631560 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
59: [W1009 23:10:07.680177541 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 0: [W1009 23:10:07.388325450 ProcessGroupNCCL.cpp:927] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 64 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 8: [Gloo] Rank 8 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 9: [Gloo] Rank 9 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
10: [Gloo] Rank 10 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
11: [Gloo] Rank 11 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
12: [Gloo] Rank 12 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
24: [Gloo] Rank 24 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 0: [Gloo] Rank 0 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
25: [Gloo] Rank 25 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 1: [Gloo] Rank 1 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 2: [Gloo] Rank 2 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
26: [Gloo] Rank 26 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 3: [Gloo] Rank 3 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
27: [Gloo] Rank 27 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
28: [Gloo] Rank 28 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 4: [Gloo] Rank 4 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
29: [Gloo] Rank 29 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 6: [Gloo] Rank 6 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
30: [Gloo] Rank 30 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 5: [Gloo] Rank 5 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
32: [Gloo] Rank 32 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
31: [Gloo] Rank 31 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
33: [Gloo] Rank 33 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 7: [Gloo] Rank 7 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
34: [Gloo] Rank 34 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
35: [Gloo] Rank 35 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
14: [Gloo] Rank 14 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
13: [Gloo] Rank 13 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
15: [Gloo] Rank 15 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
36: [Gloo] Rank 36 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
16: [Gloo] Rank 16 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
19: [Gloo] Rank 19 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
40: [Gloo] Rank 40 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
20: [Gloo] Rank 20 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
21: [Gloo] Rank 21 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
17: [Gloo] Rank 17 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
37: [Gloo] Rank 37 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
18: [Gloo] Rank 18 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
22: [Gloo] Rank 22 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
41: [Gloo] Rank 41 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
23: [Gloo] Rank 23 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
38: [Gloo] Rank 38 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
39: [Gloo] Rank 39 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
42: [Gloo] Rank 42 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
43: [Gloo] Rank 43 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
44: [Gloo] Rank 44 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
45: [Gloo] Rank 45 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
46: [Gloo] Rank 46 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
47: [Gloo] Rank 47 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
57: [Gloo] Rank 57 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
48: [Gloo] Rank 48 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
56: [Gloo] Rank 56 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
59: [Gloo] Rank 59 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
49: [Gloo] Rank 49 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
50: [Gloo] Rank 50 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
58: [Gloo] Rank 58 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
60: [Gloo] Rank 60 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
51: [Gloo] Rank 51 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
62: [Gloo] Rank 62 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
52: [Gloo] Rank 52 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
53: [Gloo] Rank 53 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
54: [Gloo] Rank 54 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
55: [Gloo] Rank 55 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
61: [Gloo] Rank 61 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
63: [Gloo] Rank 63 is connected to 63 peer ranks. Expected number of connected peer ranks is : 63
 2: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 4: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 6: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 8: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
10: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
12: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
14: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 1: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 3: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 5: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
49: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
51: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 7: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 9: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
24: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
55: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
11: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
26: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
57: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
61: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
59: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
63: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
13: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
28: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
58: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
15: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
56: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
60: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
62: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
30: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
17: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
32: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
19: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
34: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
21: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
36: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
23: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
38: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
43: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
25: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
40: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
42: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
27: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
53: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
29: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
44: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
47: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
31: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
46: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
48: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
50: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
52: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
54: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
33: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
35: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
37: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
39: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
41: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
45: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 0: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
16: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
20: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
18: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
22: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
59: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 1: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 3: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 5: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 7: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 9: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
11: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
13: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
15: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
17: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
61: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
19: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
43: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
21: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
23: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
47: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
25: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
49: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
27: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
51: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
29: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
53: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
55: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
31: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
57: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
63: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
33: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
35: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
37: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
39: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
41: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
45: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
24: [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 0: [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
26: [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 2: [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
28: [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 4: [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
30: [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
16: [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 8: [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 6: [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
10: [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
32: [Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
14: [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
12: [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
34: [Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
20: [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
18: [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
22: [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
36: [Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
38: [Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
40: [Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
42: [Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
44: [Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
46: [Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
48: [Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
50: [Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
52: [Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
54: [Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
56: [Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
58: [Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
62: [Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
60: [Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
 0: NCCL version 2.27.7+cuda13.0
 0: [NeMo I 2025-10-09 23:10:11 utils:662] Building GPTDataset splits with sizes=[38400000, 3201024, 32] and config=GPTDatasetConfig(random_seed=21838, sequence_length=8192, blend=None, blend_per_split=[(['/preproc_data/c4-train.en_6_text_document'], [50.0]), (['/preproc_data/c4-validation-91205-samples.en_text_document'], None), (['/preproc_data/c4-validation-91205-samples.en_text_document'], None)], multiple_validation_sets=None, full_validation=None, split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache='/npy_index', mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x770205eca4e0>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
 0: [NeMo I 2025-10-09 23:10:11 utils:662] Load the _IndexReader from /preproc_data/c4-train.en_6_text_document.idx
 0: [NeMo I 2025-10-09 23:10:11 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 23:10:11 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 23:10:11 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 23:10:11 utils:662] > total number of sequences: 45608611
 0: [NeMo I 2025-10-09 23:10:11 utils:662] > total number of documents: 45608611
 0: [NeMo I 2025-10-09 23:10:11 utils:662] Build and save the GPTDataset train indices
 0: [NeMo I 2025-10-09 23:10:59 utils:662] > total number of samples: 38441516
 0: [NeMo I 2025-10-09 23:10:59 utils:662] > total number of epochs: 15
 0: [NeMo I 2025-10-09 23:10:59 utils:662] Load the _IndexReader from /preproc_data/c4-validation-91205-samples.en_text_document.idx
 0: [NeMo I 2025-10-09 23:10:59 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 23:10:59 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 23:10:59 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 23:10:59 utils:662] > total number of sequences: 91205
 0: [NeMo I 2025-10-09 23:10:59 utils:662] > total number of documents: 91205
 0: [NeMo I 2025-10-09 23:10:59 utils:662] Build and save the GPTDataset valid indices
 0: [NeMo I 2025-10-09 23:11:02 utils:662] > total number of samples: 3202455
 0: [NeMo I 2025-10-09 23:11:02 utils:662] > total number of epochs: 630
 0: [NeMo I 2025-10-09 23:11:02 utils:662] Load the _IndexReader from /preproc_data/c4-validation-91205-samples.en_text_document.idx
 0: [NeMo I 2025-10-09 23:11:02 utils:662] 	Extract the sequence lengths
 0: [NeMo I 2025-10-09 23:11:02 utils:662] 	Extract the sequence pointers
 0: [NeMo I 2025-10-09 23:11:02 utils:662] 	Extract the document indices
 0: [NeMo I 2025-10-09 23:11:02 utils:662] > total number of sequences: 91205
 0: [NeMo I 2025-10-09 23:11:02 utils:662] > total number of documents: 91205
 0: [NeMo I 2025-10-09 23:11:02 utils:662] Build and save the GPTDataset test indices
 0: [NeMo I 2025-10-09 23:11:02 utils:662] > total number of samples: 5083
 0: [NeMo I 2025-10-09 23:11:02 utils:662] > total number of epochs: 1
 0: [NeMo W 2025-10-09 23:11:02 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=32 for best performance                         but get 1
 0: [NeMo I 2025-10-09 23:11:02 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-10-09 23:11:07 nemo_logging:393] Apply rope scaling with factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, old_context_len=8192.
41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-10-09 23:11:07 nemo_logging:393] Copying Trainer's 'max_steps' (1200000) to LR scheduler's 'max_steps'.
 0: [NeMo I 2025-10-09 23:11:07 num_microbatches_calculator:228] setting number of microbatches to constant 1
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-10-09 23:11:07 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 8030261248
 0: [NeMo I 2025-10-09 23:11:07 utils:662] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=768000000, pad_buckets_for_high_nccl_busbw=False, average_in_collective=True, fp8_param_gather=True, reuse_grad_buf_for_mxfp8_param_ag=False, use_megatron_fsdp=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache=False, nccl_ub=False, fsdp_double_buffer=False, outer_dp_sharding_strategy='no_shard', disable_symmetric_registration=False, delay_wgrad_compute=False)
44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2025-10-09 23:11:07 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 1
 0:     Params for bucket 1 (1050939392 elements, 1050939392 padded size):
 0:     	module.output_layer.weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.final_layernorm.weight
 0:     	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
 0:     	module.embedding.word_embeddings.weight
 0:     	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
 0:     	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
 0: [NeMo I 2025-10-09 23:11:07 utils:695] Number of buckets for gradient all-reduce / reduce-scatter: 9
 0:     Params for bucket 1 (830472192 elements, 830472192 padded size):
 0:     	module.decoder.layers.31.self_attention.linear_proj.weight
 0:     	module.decoder.layers.30.self_attention.linear_proj.weight
 0:     	module.decoder.layers.29.self_attention.linear_proj.weight
 0:     	module.decoder.layers.31.mlp.linear_fc2.weight
 0:     	module.decoder.layers.30.mlp.linear_fc2.weight
 0:     	module.decoder.layers.31.mlp.linear_fc1.weight
 0:     	module.decoder.layers.31.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.30.mlp.linear_fc1.weight
 0:     	module.decoder.layers.30.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.29.mlp.linear_fc2.weight
 0:     	module.decoder.layers.29.mlp.linear_fc1.weight
 0:     	module.decoder.layers.29.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.28.mlp.linear_fc2.weight
 0:     	module.decoder.layers.28.mlp.linear_fc1.weight
 0:     Params for bucket 2 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.28.self_attention.linear_proj.weight
 0:     	module.decoder.layers.27.self_attention.linear_proj.weight
 0:     	module.decoder.layers.26.self_attention.linear_proj.weight
 0:     	module.decoder.layers.25.self_attention.linear_proj.weight
 0:     	module.decoder.layers.24.mlp.linear_fc1.weight
 0:     	module.decoder.layers.24.mlp.linear_fc2.weight
 0:     	module.decoder.layers.28.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.27.mlp.linear_fc2.weight
 0:     	module.decoder.layers.27.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.27.mlp.linear_fc1.weight
 0:     	module.decoder.layers.26.mlp.linear_fc2.weight
 0:     	module.decoder.layers.26.mlp.linear_fc1.weight
 0:     	module.decoder.layers.26.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.25.mlp.linear_fc2.weight
 0:     	module.decoder.layers.25.mlp.linear_fc1.weight
 0:     	module.decoder.layers.25.self_attention.linear_qkv.weight
 0:     Params for bucket 3 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.24.self_attention.linear_proj.weight
 0:     	module.decoder.layers.23.self_attention.linear_proj.weight
 0:     	module.decoder.layers.22.self_attention.linear_proj.weight
 0:     	module.decoder.layers.21.self_attention.linear_proj.weight
 0:     	module.decoder.layers.24.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.23.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.23.mlp.linear_fc2.weight
 0:     	module.decoder.layers.23.mlp.linear_fc1.weight
 0:     	module.decoder.layers.22.mlp.linear_fc2.weight
 0:     	module.decoder.layers.22.mlp.linear_fc1.weight
 0:     	module.decoder.layers.22.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.21.mlp.linear_fc2.weight
 0:     	module.decoder.layers.21.mlp.linear_fc1.weight
 0:     	module.decoder.layers.21.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.20.mlp.linear_fc1.weight
 0:     	module.decoder.layers.20.mlp.linear_fc2.weight
 0:     Params for bucket 4 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.20.self_attention.linear_proj.weight
 0:     	module.decoder.layers.19.self_attention.linear_proj.weight
 0:     	module.decoder.layers.18.self_attention.linear_proj.weight
 0:     	module.decoder.layers.17.self_attention.linear_proj.weight
 0:     	module.decoder.layers.16.mlp.linear_fc1.weight
 0:     	module.decoder.layers.16.mlp.linear_fc2.weight
 0:     	module.decoder.layers.20.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.19.mlp.linear_fc1.weight
 0:     	module.decoder.layers.19.mlp.linear_fc2.weight
 0:     	module.decoder.layers.19.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.18.mlp.linear_fc2.weight
 0:     	module.decoder.layers.18.mlp.linear_fc1.weight
 0:     	module.decoder.layers.18.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.17.mlp.linear_fc2.weight
 0:     	module.decoder.layers.17.mlp.linear_fc1.weight
 0:     	module.decoder.layers.17.self_attention.linear_qkv.weight
 0:     Params for bucket 5 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.16.self_attention.linear_proj.weight
 0:     	module.decoder.layers.15.self_attention.linear_proj.weight
 0:     	module.decoder.layers.14.self_attention.linear_proj.weight
 0:     	module.decoder.layers.13.self_attention.linear_proj.weight
 0:     	module.decoder.layers.12.mlp.linear_fc1.weight
 0:     	module.decoder.layers.16.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.15.mlp.linear_fc1.weight
 0:     	module.decoder.layers.15.mlp.linear_fc2.weight
 0:     	module.decoder.layers.15.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.14.mlp.linear_fc2.weight
 0:     	module.decoder.layers.14.mlp.linear_fc1.weight
 0:     	module.decoder.layers.14.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.13.mlp.linear_fc2.weight
 0:     	module.decoder.layers.13.mlp.linear_fc1.weight
 0:     	module.decoder.layers.13.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.12.mlp.linear_fc2.weight
 0:     Params for bucket 6 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.12.self_attention.linear_proj.weight
 0:     	module.decoder.layers.11.self_attention.linear_proj.weight
 0:     	module.decoder.layers.10.self_attention.linear_proj.weight
 0:     	module.decoder.layers.9.self_attention.linear_proj.weight
 0:     	module.decoder.layers.8.mlp.linear_fc1.weight
 0:     	module.decoder.layers.8.mlp.linear_fc2.weight
 0:     	module.decoder.layers.12.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.11.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.11.mlp.linear_fc1.weight
 0:     	module.decoder.layers.11.mlp.linear_fc2.weight
 0:     	module.decoder.layers.10.mlp.linear_fc2.weight
 0:     	module.decoder.layers.10.mlp.linear_fc1.weight
 0:     	module.decoder.layers.10.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.9.mlp.linear_fc2.weight
 0:     	module.decoder.layers.9.mlp.linear_fc1.weight
 0:     	module.decoder.layers.9.self_attention.linear_qkv.weight
 0:     Params for bucket 7 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.8.self_attention.linear_proj.weight
 0:     	module.decoder.layers.7.self_attention.linear_proj.weight
 0:     	module.decoder.layers.6.self_attention.linear_proj.weight
 0:     	module.decoder.layers.5.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.5.self_attention.linear_proj.weight
 0:     	module.decoder.layers.8.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.7.mlp.linear_fc2.weight
 0:     	module.decoder.layers.7.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.7.mlp.linear_fc1.weight
 0:     	module.decoder.layers.6.mlp.linear_fc2.weight
 0:     	module.decoder.layers.6.mlp.linear_fc1.weight
 0:     	module.decoder.layers.6.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.5.mlp.linear_fc2.weight
 0:     	module.decoder.layers.5.mlp.linear_fc1.weight
 0:     	module.decoder.layers.4.mlp.linear_fc2.weight
 0:     	module.decoder.layers.4.mlp.linear_fc1.weight
 0:     Params for bucket 8 (872415232 elements, 872415232 padded size):
 0:     	module.decoder.layers.4.self_attention.linear_proj.weight
 0:     	module.decoder.layers.3.self_attention.linear_proj.weight
 0:     	module.decoder.layers.2.mlp.linear_fc1.weight
 0:     	module.decoder.layers.2.self_attention.linear_proj.weight
 0:     	module.decoder.layers.1.mlp.linear_fc2.weight
 0:     	module.decoder.layers.1.mlp.linear_fc1.weight
 0:     	module.decoder.layers.1.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.0.mlp.linear_fc2.weight
 0:     	module.decoder.layers.0.mlp.linear_fc1.weight
 0:     	module.decoder.layers.4.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.3.mlp.linear_fc1.weight
 0:     	module.decoder.layers.3.mlp.linear_fc2.weight
 0:     	module.decoder.layers.3.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.2.mlp.linear_fc2.weight
 0:     	module.decoder.layers.2.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.1.self_attention.linear_proj.weight
 0:     Params for bucket 9 (41943040 elements, 41943040 padded size):
 0:     	module.decoder.layers.0.self_attention.linear_qkv.weight
 0:     	module.decoder.layers.0.self_attention.linear_proj.weight
 0: [NeMo I 2025-10-09 23:11:07 utils:662] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0008, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='tensorwise', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_ti
 0: me=False, timers=None, config_logger_dir='')
 0: 
 0:   | Name   | Type | Params | Mode 
 0: ----------------------------------------
 0: 0 | module | DDP  | 8.0 B  | train
 0: ----------------------------------------
 0: 8.0 B     Trainable params
 0: 0         Non-trainable params
 0: 8.0 B     Total params
 0: 32,121.045Total estimated model params size (MB)
 0: 651       Modules in train mode
 0: 0         Modules in eval mode
16: SLURM auto-requeueing enabled. Setting signal handlers.
 0: SLURM auto-requeueing enabled. Setting signal handlers.
34: SLURM auto-requeueing enabled. Setting signal handlers.
35: SLURM auto-requeueing enabled. Setting signal handlers.
36: SLURM auto-requeueing enabled. Setting signal handlers.
37: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
38: SLURM auto-requeueing enabled. Setting signal handlers.
56: SLURM auto-requeueing enabled. Setting signal handlers.
48: SLURM auto-requeueing enabled. Setting signal handlers.
39: SLURM auto-requeueing enabled. Setting signal handlers.
51: SLURM auto-requeueing enabled. Setting signal handlers.
32: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
33: SLURM auto-requeueing enabled. Setting signal handlers.
40: SLURM auto-requeueing enabled. Setting signal handlers.
52: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
53: SLURM auto-requeueing enabled. Setting signal handlers.
17: SLURM auto-requeueing enabled. Setting signal handlers.
54: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
55: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
57: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
49: SLURM auto-requeueing enabled. Setting signal handlers.
50: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
41: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
43: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
42: SLURM auto-requeueing enabled. Setting signal handlers.
44: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
58: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
45: SLURM auto-requeueing enabled. Setting signal handlers.
59: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
60: SLURM auto-requeueing enabled. Setting signal handlers.
61: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
46: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
62: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
47: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
63: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [AUX I 2025-10-09 23:11:07 data:291] Instantiating MegatronPretrainingSampler with total_samples: 38441516 and consumed_samples: 0
 0: [AUX I 2025-10-09 23:11:07 data:291] Instantiating MegatronPretrainingSampler with total_samples: 3202455 and consumed_samples: 0
 8: GPTModel(
 8:   (embedding): LanguageModelEmbedding(
 8:     (word_embeddings): VocabParallelEmbedding()
 8:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 8:   )
 8:   (rotary_pos_emb): RotaryEmbedding()
 8:   (decoder): TransformerBlock(
 8:     (layers): ModuleList(
 8:       (0-31): 32 x TransformerLayer(
 8:         (input_layernorm): IdentityOp()
 8:         (self_attention): SelfAttention(
 8:           (core_attention): TEDotProductAttention(
 8:             (flash_attention): FlashAttention()
 8:             (fused_attention): FusedAttention()
 8:             (unfused_attention): UnfusedDotProductAttention(
 8:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 8:               (attention_dropout): Dropout(p=0.0, inplace=False)
 8:             )
 8:           )
 8:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 8:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 8:           (q_layernorm): IdentityOp()
 8:           (k_layernorm): IdentityOp()
 8:         )
 8:         (pre_cross_attn_layernorm): IdentityOp()
 8:         (cross_attention): IdentityOp()
 8:         (cross_attn_bda): IdentityFuncOp()
 8:         (pre_mlp_layernorm): IdentityOp()
 8:         (mlp): TEFusedMLP(
 8:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 8:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 8:         )
 8:       )
 8:     )
 8:     (final_layernorm): RMSNorm()
 8:   )
 8:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 8: )
 9: GPTModel(
 9:   (embedding): LanguageModelEmbedding(
 9:     (word_embeddings): VocabParallelEmbedding()
 9:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 9:   )
 9:   (rotary_pos_emb): RotaryEmbedding()
 9:   (decoder): TransformerBlock(
 9:     (layers): ModuleList(
 9:       (0-31): 32 x TransformerLayer(
 9:         (input_layernorm): IdentityOp()
 9:         (self_attention): SelfAttention(
 9:           (core_attention): TEDotProductAttention(
 9:             (flash_attention): FlashAttention()
 9:             (fused_attention): FusedAttention()
 9:             (unfused_attention): UnfusedDotProductAttention(
 9:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 9:               (attention_dropout): Dropout(p=0.0, inplace=False)
 9:             )
 9:           )
 9:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 9:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 9:           (q_layernorm): IdentityOp()
 9:           (k_layernorm): IdentityOp()
48: GPTModel(
48:   (embedding): LanguageModelEmbedding(
48:     (word_embeddings): VocabParallelEmbedding()
48:     (embedding_dropout): Dropout(p=0.0, inplace=False)
48:   )
48:   (rotary_pos_emb): RotaryEmbedding()
48:   (decoder): TransformerBlock(
48:     (layers): ModuleList(
48:       (0-31): 32 x TransformerLayer(
48:         (input_layernorm): IdentityOp()
48:         (self_attention): SelfAttention(
48:           (core_attention): TEDotProductAttention(
48:             (flash_attention): FlashAttention()
48:             (fused_attention): FusedAttention()
48:             (unfused_attention): UnfusedDotProductAttention(
48:               (scale_mask_softmax): FusedScaleMaskSoftmax()
48:               (attention_dropout): Dropout(p=0.0, inplace=False)
48:             )
48:           )
48:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
48:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
48:           (q_layernorm): IdentityOp()
48:           (k_layernorm): IdentityOp()
 9:         )
 9:         (pre_cross_attn_layernorm): IdentityOp()
 9:         (cross_attention): IdentityOp()
 9:         (cross_attn_bda): IdentityFuncOp()
 9:         (pre_mlp_layernorm): IdentityOp()
 9:         (mlp): TEFusedMLP(
 9:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 9:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 9:         )
 9:       )
 9:     )
 9:     (final_layernorm): RMSNorm()
 9:   )
 9:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
48:         )
48:         (pre_cross_attn_layernorm): IdentityOp()
48:         (cross_attention): IdentityOp()
48:         (cross_attn_bda): IdentityFuncOp()
48:         (pre_mlp_layernorm): IdentityOp()
48:         (mlp): TEFusedMLP(
48:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
48:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
48:         )
48:       )
48:     )
48:     (final_layernorm): RMSNorm()
48:   )
48:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 9: )
16: GPTModel(
16:   (embedding): LanguageModelEmbedding(
16:     (word_embeddings): VocabParallelEmbedding()
16:     (embedding_dropout): Dropout(p=0.0, inplace=False)
16:   )
16:   (rotary_pos_emb): RotaryEmbedding()
16:   (decoder): TransformerBlock(
16:     (layers): ModuleList(
16:       (0-31): 32 x TransformerLayer(
16:         (input_layernorm): IdentityOp()
16:         (self_attention): SelfAttention(
16:           (core_attention): TEDotProductAttention(
16:             (flash_attention): FlashAttention()
16:             (fused_attention): FusedAttention()
16:             (unfused_attention): UnfusedDotProductAttention(
16:               (scale_mask_softmax): FusedScaleMaskSoftmax()
16:               (attention_dropout): Dropout(p=0.0, inplace=False)
16:             )
16:           )
16:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
16:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
16:           (q_layernorm): IdentityOp()
16:           (k_layernorm): IdentityOp()
48: )
16:         )
16:         (pre_cross_attn_layernorm): IdentityOp()
16:         (cross_attention): IdentityOp()
16:         (cross_attn_bda): IdentityFuncOp()
16:         (pre_mlp_layernorm): IdentityOp()
16:         (mlp): TEFusedMLP(
16:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
16:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
16:         )
16:       )
16:     )
16:     (final_layernorm): RMSNorm()
16:   )
16:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
16: )
56: GPTModel(
56:   (embedding): LanguageModelEmbedding(
56:     (word_embeddings): VocabParallelEmbedding()
56:     (embedding_dropout): Dropout(p=0.0, inplace=False)
56:   )
56:   (rotary_pos_emb): RotaryEmbedding()
56:   (decoder): TransformerBlock(
56:     (layers): ModuleList(
56:       (0-31): 32 x TransformerLayer(
56:         (input_layernorm): IdentityOp()
56:         (self_attention): SelfAttention(
56:           (core_attention): TEDotProductAttention(
56:             (flash_attention): FlashAttention()
56:             (fused_attention): FusedAttention()
56:             (unfused_attention): UnfusedDotProductAttention(
56:               (scale_mask_softmax): FusedScaleMaskSoftmax()
56:               (attention_dropout): Dropout(p=0.0, inplace=False)
56:             )
56:           )
56:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
56:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
56:           (q_layernorm): IdentityOp()
56:           (k_layernorm): IdentityOp()
17: GPTModel(
17:   (embedding): LanguageModelEmbedding(
17:     (word_embeddings): VocabParallelEmbedding()
17:     (embedding_dropout): Dropout(p=0.0, inplace=False)
17:   )
17:   (rotary_pos_emb): RotaryEmbedding()
17:   (decoder): TransformerBlock(
17:     (layers): ModuleList(
17:       (0-31): 32 x TransformerLayer(
17:         (input_layernorm): IdentityOp()
17:         (self_attention): SelfAttention(
17:           (core_attention): TEDotProductAttention(
17:             (flash_attention): FlashAttention()
17:             (fused_attention): FusedAttention()
17:             (unfused_attention): UnfusedDotProductAttention(
17:               (scale_mask_softmax): FusedScaleMaskSoftmax()
17:               (attention_dropout): Dropout(p=0.0, inplace=False)
17:             )
17:           )
17:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
17:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
17:           (q_layernorm): IdentityOp()
17:           (k_layernorm): IdentityOp()
49: GPTModel(
49:   (embedding): LanguageModelEmbedding(
49:     (word_embeddings): VocabParallelEmbedding()
49:     (embedding_dropout): Dropout(p=0.0, inplace=False)
49:   )
49:   (rotary_pos_emb): RotaryEmbedding()
49:   (decoder): TransformerBlock(
49:     (layers): ModuleList(
49:       (0-31): 32 x TransformerLayer(
49:         (input_layernorm): IdentityOp()
49:         (self_attention): SelfAttention(
49:           (core_attention): TEDotProductAttention(
49:             (flash_attention): FlashAttention()
49:             (fused_attention): FusedAttention()
49:             (unfused_attention): UnfusedDotProductAttention(
49:               (scale_mask_softmax): FusedScaleMaskSoftmax()
49:               (attention_dropout): Dropout(p=0.0, inplace=False)
49:             )
49:           )
49:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
49:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
49:           (q_layernorm): IdentityOp()
49:           (k_layernorm): IdentityOp()
41: GPTModel(
41:   (embedding): LanguageModelEmbedding(
41:     (word_embeddings): VocabParallelEmbedding()
41:     (embedding_dropout): Dropout(p=0.0, inplace=False)
41:   )
41:   (rotary_pos_emb): RotaryEmbedding()
41:   (decoder): TransformerBlock(
41:     (layers): ModuleList(
41:       (0-31): 32 x TransformerLayer(
41:         (input_layernorm): IdentityOp()
41:         (self_attention): SelfAttention(
41:           (core_attention): TEDotProductAttention(
41:             (flash_attention): FlashAttention()
41:             (fused_attention): FusedAttention()
41:             (unfused_attention): UnfusedDotProductAttention(
41:               (scale_mask_softmax): FusedScaleMaskSoftmax()
41:               (attention_dropout): Dropout(p=0.0, inplace=False)
41:             )
41:           )
41:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
41:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
41:           (q_layernorm): IdentityOp()
41:           (k_layernorm): IdentityOp()
32: GPTModel(
32:   (embedding): LanguageModelEmbedding(
32:     (word_embeddings): VocabParallelEmbedding()
32:     (embedding_dropout): Dropout(p=0.0, inplace=False)
32:   )
32:   (rotary_pos_emb): RotaryEmbedding()
32:   (decoder): TransformerBlock(
32:     (layers): ModuleList(
32:       (0-31): 32 x TransformerLayer(
32:         (input_layernorm): IdentityOp()
32:         (self_attention): SelfAttention(
32:           (core_attention): TEDotProductAttention(
32:             (flash_attention): FlashAttention()
32:             (fused_attention): FusedAttention()
32:             (unfused_attention): UnfusedDotProductAttention(
32:               (scale_mask_softmax): FusedScaleMaskSoftmax()
32:               (attention_dropout): Dropout(p=0.0, inplace=False)
32:             )
32:           )
32:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
32:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
32:           (q_layernorm): IdentityOp()
32:           (k_layernorm): IdentityOp()
 1: GPTModel(
 1:   (embedding): LanguageModelEmbedding(
 1:     (word_embeddings): VocabParallelEmbedding()
 1:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 1:   )
 1:   (rotary_pos_emb): RotaryEmbedding()
 1:   (decoder): TransformerBlock(
 1:     (layers): ModuleList(
 1:       (0-31): 32 x TransformerLayer(
 1:         (input_layernorm): IdentityOp()
 1:         (self_attention): SelfAttention(
 1:           (core_attention): TEDotProductAttention(
 1:             (flash_attention): FlashAttention()
 1:             (fused_attention): FusedAttention()
 1:             (unfused_attention): UnfusedDotProductAttention(
 1:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 1:               (attention_dropout): Dropout(p=0.0, inplace=False)
 1:             )
 1:           )
 1:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 1:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 1:           (q_layernorm): IdentityOp()
 1:           (k_layernorm): IdentityOp()
24: GPTModel(
24:   (embedding): LanguageModelEmbedding(
24:     (word_embeddings): VocabParallelEmbedding()
24:     (embedding_dropout): Dropout(p=0.0, inplace=False)
24:   )
24:   (rotary_pos_emb): RotaryEmbedding()
24:   (decoder): TransformerBlock(
24:     (layers): ModuleList(
24:       (0-31): 32 x TransformerLayer(
24:         (input_layernorm): IdentityOp()
24:         (self_attention): SelfAttention(
24:           (core_attention): TEDotProductAttention(
24:             (flash_attention): FlashAttention()
24:             (fused_attention): FusedAttention()
24:             (unfused_attention): UnfusedDotProductAttention(
24:               (scale_mask_softmax): FusedScaleMaskSoftmax()
24:               (attention_dropout): Dropout(p=0.0, inplace=False)
24:             )
24:           )
24:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
24:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
24:           (q_layernorm): IdentityOp()
24:           (k_layernorm): IdentityOp()
56:         )
56:         (pre_cross_attn_layernorm): IdentityOp()
56:         (cross_attention): IdentityOp()
56:         (cross_attn_bda): IdentityFuncOp()
56:         (pre_mlp_layernorm): IdentityOp()
56:         (mlp): TEFusedMLP(
56:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
56:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
56:         )
56:       )
56:     )
56:     (final_layernorm): RMSNorm()
56:   )
56:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
10: GPTModel(
10:   (embedding): LanguageModelEmbedding(
10:     (word_embeddings): VocabParallelEmbedding()
10:     (embedding_dropout): Dropout(p=0.0, inplace=False)
10:   )
10:   (rotary_pos_emb): RotaryEmbedding()
10:   (decoder): TransformerBlock(
10:     (layers): ModuleList(
10:       (0-31): 32 x TransformerLayer(
10:         (input_layernorm): IdentityOp()
10:         (self_attention): SelfAttention(
10:           (core_attention): TEDotProductAttention(
10:             (flash_attention): FlashAttention()
10:             (fused_attention): FusedAttention()
10:             (unfused_attention): UnfusedDotProductAttention(
10:               (scale_mask_softmax): FusedScaleMaskSoftmax()
10:               (attention_dropout): Dropout(p=0.0, inplace=False)
10:             )
10:           )
10:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
10:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
10:           (q_layernorm): IdentityOp()
10:           (k_layernorm): IdentityOp()
17:         )
17:         (pre_cross_attn_layernorm): IdentityOp()
17:         (cross_attention): IdentityOp()
17:         (cross_attn_bda): IdentityFuncOp()
17:         (pre_mlp_layernorm): IdentityOp()
17:         (mlp): TEFusedMLP(
17:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
17:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
17:         )
17:       )
17:     )
17:     (final_layernorm): RMSNorm()
17:   )
17:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
49:         )
49:         (pre_cross_attn_layernorm): IdentityOp()
49:         (cross_attention): IdentityOp()
49:         (cross_attn_bda): IdentityFuncOp()
49:         (pre_mlp_layernorm): IdentityOp()
49:         (mlp): TEFusedMLP(
49:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
49:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
49:         )
49:       )
49:     )
49:     (final_layernorm): RMSNorm()
49:   )
49:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
41:         )
41:         (pre_cross_attn_layernorm): IdentityOp()
41:         (cross_attention): IdentityOp()
41:         (cross_attn_bda): IdentityFuncOp()
41:         (pre_mlp_layernorm): IdentityOp()
41:         (mlp): TEFusedMLP(
41:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
41:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
41:         )
41:       )
41:     )
41:     (final_layernorm): RMSNorm()
41:   )
41:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
41: )
32:         )
32:         (pre_cross_attn_layernorm): IdentityOp()
32:         (cross_attention): IdentityOp()
32:         (cross_attn_bda): IdentityFuncOp()
32:         (pre_mlp_layernorm): IdentityOp()
32:         (mlp): TEFusedMLP(
32:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
32:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
32:         )
32:       )
32:     )
32:     (final_layernorm): RMSNorm()
32:   )
32:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
32: )
 1:         )
 1:         (pre_cross_attn_layernorm): IdentityOp()
 1:         (cross_attention): IdentityOp()
 1:         (cross_attn_bda): IdentityFuncOp()
 1:         (pre_mlp_layernorm): IdentityOp()
 1:         (mlp): TEFusedMLP(
 1:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 1:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 1:         )
 1:       )
 1:     )
 1:     (final_layernorm): RMSNorm()
 1:   )
 1:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
24:         )
24:         (pre_cross_attn_layernorm): IdentityOp()
24:         (cross_attention): IdentityOp()
24:         (cross_attn_bda): IdentityFuncOp()
24:         (pre_mlp_layernorm): IdentityOp()
24:         (mlp): TEFusedMLP(
24:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
24:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
24:         )
24:       )
24:     )
24:     (final_layernorm): RMSNorm()
24:   )
24:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
24: )
56: )
10:         )
10:         (pre_cross_attn_layernorm): IdentityOp()
10:         (cross_attention): IdentityOp()
10:         (cross_attn_bda): IdentityFuncOp()
10:         (pre_mlp_layernorm): IdentityOp()
10:         (mlp): TEFusedMLP(
10:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
10:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
10:         )
10:       )
10:     )
10:     (final_layernorm): RMSNorm()
10:   )
10:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
17: )
50: GPTModel(
50:   (embedding): LanguageModelEmbedding(
50:     (word_embeddings): VocabParallelEmbedding()
50:     (embedding_dropout): Dropout(p=0.0, inplace=False)
50:   )
50:   (rotary_pos_emb): RotaryEmbedding()
50:   (decoder): TransformerBlock(
50:     (layers): ModuleList(
50:       (0-31): 32 x TransformerLayer(
50:         (input_layernorm): IdentityOp()
50:         (self_attention): SelfAttention(
50:           (core_attention): TEDotProductAttention(
50:             (flash_attention): FlashAttention()
50:             (fused_attention): FusedAttention()
50:             (unfused_attention): UnfusedDotProductAttention(
50:               (scale_mask_softmax): FusedScaleMaskSoftmax()
50:               (attention_dropout): Dropout(p=0.0, inplace=False)
50:             )
50:           )
50:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
50:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
50:           (q_layernorm): IdentityOp()
50:           (k_layernorm): IdentityOp()
40: GPTModel(
40:   (embedding): LanguageModelEmbedding(
40:     (word_embeddings): VocabParallelEmbedding()
40:     (embedding_dropout): Dropout(p=0.0, inplace=False)
40:   )
40:   (rotary_pos_emb): RotaryEmbedding()
40:   (decoder): TransformerBlock(
40:     (layers): ModuleList(
40:       (0-31): 32 x TransformerLayer(
40:         (input_layernorm): IdentityOp()
40:         (self_attention): SelfAttention(
40:           (core_attention): TEDotProductAttention(
40:             (flash_attention): FlashAttention()
40:             (fused_attention): FusedAttention()
40:             (unfused_attention): UnfusedDotProductAttention(
40:               (scale_mask_softmax): FusedScaleMaskSoftmax()
40:               (attention_dropout): Dropout(p=0.0, inplace=False)
40:             )
40:           )
40:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
40:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
40:           (q_layernorm): IdentityOp()
40:           (k_layernorm): IdentityOp()
33: GPTModel(
33:   (embedding): LanguageModelEmbedding(
33:     (word_embeddings): VocabParallelEmbedding()
33:     (embedding_dropout): Dropout(p=0.0, inplace=False)
33:   )
33:   (rotary_pos_emb): RotaryEmbedding()
33:   (decoder): TransformerBlock(
33:     (layers): ModuleList(
33:       (0-31): 32 x TransformerLayer(
33:         (input_layernorm): IdentityOp()
33:         (self_attention): SelfAttention(
33:           (core_attention): TEDotProductAttention(
33:             (flash_attention): FlashAttention()
33:             (fused_attention): FusedAttention()
33:             (unfused_attention): UnfusedDotProductAttention(
33:               (scale_mask_softmax): FusedScaleMaskSoftmax()
33:               (attention_dropout): Dropout(p=0.0, inplace=False)
33:             )
33:           )
33:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
33:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
33:           (q_layernorm): IdentityOp()
33:           (k_layernorm): IdentityOp()
25: GPTModel(
25:   (embedding): LanguageModelEmbedding(
25:     (word_embeddings): VocabParallelEmbedding()
25:     (embedding_dropout): Dropout(p=0.0, inplace=False)
25:   )
25:   (rotary_pos_emb): RotaryEmbedding()
25:   (decoder): TransformerBlock(
25:     (layers): ModuleList(
25:       (0-31): 32 x TransformerLayer(
25:         (input_layernorm): IdentityOp()
25:         (self_attention): SelfAttention(
25:           (core_attention): TEDotProductAttention(
25:             (flash_attention): FlashAttention()
25:             (fused_attention): FusedAttention()
25:             (unfused_attention): UnfusedDotProductAttention(
25:               (scale_mask_softmax): FusedScaleMaskSoftmax()
25:               (attention_dropout): Dropout(p=0.0, inplace=False)
25:             )
25:           )
25:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
25:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
25:           (q_layernorm): IdentityOp()
25:           (k_layernorm): IdentityOp()
57: GPTModel(
57:   (embedding): LanguageModelEmbedding(
57:     (word_embeddings): VocabParallelEmbedding()
57:     (embedding_dropout): Dropout(p=0.0, inplace=False)
57:   )
57:   (rotary_pos_emb): RotaryEmbedding()
57:   (decoder): TransformerBlock(
57:     (layers): ModuleList(
57:       (0-31): 32 x TransformerLayer(
57:         (input_layernorm): IdentityOp()
57:         (self_attention): SelfAttention(
57:           (core_attention): TEDotProductAttention(
57:             (flash_attention): FlashAttention()
57:             (fused_attention): FusedAttention()
57:             (unfused_attention): UnfusedDotProductAttention(
57:               (scale_mask_softmax): FusedScaleMaskSoftmax()
57:               (attention_dropout): Dropout(p=0.0, inplace=False)
57:             )
57:           )
57:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
57:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
57:           (q_layernorm): IdentityOp()
57:           (k_layernorm): IdentityOp()
10: )
18: GPTModel(
18:   (embedding): LanguageModelEmbedding(
18:     (word_embeddings): VocabParallelEmbedding()
18:     (embedding_dropout): Dropout(p=0.0, inplace=False)
18:   )
18:   (rotary_pos_emb): RotaryEmbedding()
18:   (decoder): TransformerBlock(
18:     (layers): ModuleList(
18:       (0-31): 32 x TransformerLayer(
18:         (input_layernorm): IdentityOp()
18:         (self_attention): SelfAttention(
18:           (core_attention): TEDotProductAttention(
18:             (flash_attention): FlashAttention()
18:             (fused_attention): FusedAttention()
18:             (unfused_attention): UnfusedDotProductAttention(
18:               (scale_mask_softmax): FusedScaleMaskSoftmax()
18:               (attention_dropout): Dropout(p=0.0, inplace=False)
18:             )
18:           )
18:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
18:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
18:           (q_layernorm): IdentityOp()
18:           (k_layernorm): IdentityOp()
50:         )
50:         (pre_cross_attn_layernorm): IdentityOp()
50:         (cross_attention): IdentityOp()
50:         (cross_attn_bda): IdentityFuncOp()
50:         (pre_mlp_layernorm): IdentityOp()
50:         (mlp): TEFusedMLP(
50:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
50:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
50:         )
50:       )
50:     )
50:     (final_layernorm): RMSNorm()
50:   )
50:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
40:         )
40:         (pre_cross_attn_layernorm): IdentityOp()
40:         (cross_attention): IdentityOp()
40:         (cross_attn_bda): IdentityFuncOp()
40:         (pre_mlp_layernorm): IdentityOp()
40:         (mlp): TEFusedMLP(
40:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
40:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
40:         )
40:       )
40:     )
40:     (final_layernorm): RMSNorm()
40:   )
40:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
40: )
33:         )
33:         (pre_cross_attn_layernorm): IdentityOp()
33:         (cross_attention): IdentityOp()
33:         (cross_attn_bda): IdentityFuncOp()
33:         (pre_mlp_layernorm): IdentityOp()
33:         (mlp): TEFusedMLP(
33:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
33:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
33:         )
33:       )
33:     )
33:     (final_layernorm): RMSNorm()
33:   )
33:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
25:         )
25:         (pre_cross_attn_layernorm): IdentityOp()
25:         (cross_attention): IdentityOp()
25:         (cross_attn_bda): IdentityFuncOp()
25:         (pre_mlp_layernorm): IdentityOp()
25:         (mlp): TEFusedMLP(
25:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
25:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
25:         )
25:       )
25:     )
25:     (final_layernorm): RMSNorm()
25:   )
25:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
57:         )
57:         (pre_cross_attn_layernorm): IdentityOp()
57:         (cross_attention): IdentityOp()
57:         (cross_attn_bda): IdentityFuncOp()
57:         (pre_mlp_layernorm): IdentityOp()
57:         (mlp): TEFusedMLP(
57:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
57:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
57:         )
57:       )
57:     )
57:     (final_layernorm): RMSNorm()
57:   )
57:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
57: )
11: GPTModel(
11:   (embedding): LanguageModelEmbedding(
11:     (word_embeddings): VocabParallelEmbedding()
11:     (embedding_dropout): Dropout(p=0.0, inplace=False)
11:   )
11:   (rotary_pos_emb): RotaryEmbedding()
11:   (decoder): TransformerBlock(
11:     (layers): ModuleList(
11:       (0-31): 32 x TransformerLayer(
11:         (input_layernorm): IdentityOp()
11:         (self_attention): SelfAttention(
11:           (core_attention): TEDotProductAttention(
11:             (flash_attention): FlashAttention()
11:             (fused_attention): FusedAttention()
11:             (unfused_attention): UnfusedDotProductAttention(
11:               (scale_mask_softmax): FusedScaleMaskSoftmax()
11:               (attention_dropout): Dropout(p=0.0, inplace=False)
11:             )
11:           )
11:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
11:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
11:           (q_layernorm): IdentityOp()
11:           (k_layernorm): IdentityOp()
18:         )
18:         (pre_cross_attn_layernorm): IdentityOp()
18:         (cross_attention): IdentityOp()
18:         (cross_attn_bda): IdentityFuncOp()
18:         (pre_mlp_layernorm): IdentityOp()
18:         (mlp): TEFusedMLP(
18:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
18:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
18:         )
18:       )
18:     )
18:     (final_layernorm): RMSNorm()
18:   )
18:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
49: )
42: GPTModel(
42:   (embedding): LanguageModelEmbedding(
42:     (word_embeddings): VocabParallelEmbedding()
42:     (embedding_dropout): Dropout(p=0.0, inplace=False)
42:   )
42:   (rotary_pos_emb): RotaryEmbedding()
42:   (decoder): TransformerBlock(
42:     (layers): ModuleList(
42:       (0-31): 32 x TransformerLayer(
42:         (input_layernorm): IdentityOp()
42:         (self_attention): SelfAttention(
42:           (core_attention): TEDotProductAttention(
42:             (flash_attention): FlashAttention()
42:             (fused_attention): FusedAttention()
42:             (unfused_attention): UnfusedDotProductAttention(
42:               (scale_mask_softmax): FusedScaleMaskSoftmax()
42:               (attention_dropout): Dropout(p=0.0, inplace=False)
42:             )
42:           )
42:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
42:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
42:           (q_layernorm): IdentityOp()
42:           (k_layernorm): IdentityOp()
33: )
 1: )
25: )
58: GPTModel(
58:   (embedding): LanguageModelEmbedding(
58:     (word_embeddings): VocabParallelEmbedding()
58:     (embedding_dropout): Dropout(p=0.0, inplace=False)
58:   )
58:   (rotary_pos_emb): RotaryEmbedding()
58:   (decoder): TransformerBlock(
58:     (layers): ModuleList(
58:       (0-31): 32 x TransformerLayer(
58:         (input_layernorm): IdentityOp()
58:         (self_attention): SelfAttention(
58:           (core_attention): TEDotProductAttention(
58:             (flash_attention): FlashAttention()
58:             (fused_attention): FusedAttention()
58:             (unfused_attention): UnfusedDotProductAttention(
58:               (scale_mask_softmax): FusedScaleMaskSoftmax()
58:               (attention_dropout): Dropout(p=0.0, inplace=False)
58:             )
58:           )
58:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
58:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
58:           (q_layernorm): IdentityOp()
58:           (k_layernorm): IdentityOp()
11:         )
11:         (pre_cross_attn_layernorm): IdentityOp()
11:         (cross_attention): IdentityOp()
11:         (cross_attn_bda): IdentityFuncOp()
11:         (pre_mlp_layernorm): IdentityOp()
11:         (mlp): TEFusedMLP(
11:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
11:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
11:         )
11:       )
11:     )
11:     (final_layernorm): RMSNorm()
11:   )
11:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
18: )
50: )
42:         )
42:         (pre_cross_attn_layernorm): IdentityOp()
42:         (cross_attention): IdentityOp()
42:         (cross_attn_bda): IdentityFuncOp()
42:         (pre_mlp_layernorm): IdentityOp()
42:         (mlp): TEFusedMLP(
42:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
42:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
42:         )
42:       )
42:     )
42:     (final_layernorm): RMSNorm()
42:   )
42:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
34: GPTModel(
34:   (embedding): LanguageModelEmbedding(
34:     (word_embeddings): VocabParallelEmbedding()
34:     (embedding_dropout): Dropout(p=0.0, inplace=False)
34:   )
34:   (rotary_pos_emb): RotaryEmbedding()
34:   (decoder): TransformerBlock(
34:     (layers): ModuleList(
34:       (0-31): 32 x TransformerLayer(
34:         (input_layernorm): IdentityOp()
34:         (self_attention): SelfAttention(
34:           (core_attention): TEDotProductAttention(
34:             (flash_attention): FlashAttention()
34:             (fused_attention): FusedAttention()
34:             (unfused_attention): UnfusedDotProductAttention(
34:               (scale_mask_softmax): FusedScaleMaskSoftmax()
34:               (attention_dropout): Dropout(p=0.0, inplace=False)
34:             )
34:           )
34:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
34:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
34:           (q_layernorm): IdentityOp()
34:           (k_layernorm): IdentityOp()
27: GPTModel(
27:   (embedding): LanguageModelEmbedding(
27:     (word_embeddings): VocabParallelEmbedding()
27:     (embedding_dropout): Dropout(p=0.0, inplace=False)
27:   )
27:   (rotary_pos_emb): RotaryEmbedding()
27:   (decoder): TransformerBlock(
27:     (layers): ModuleList(
27:       (0-31): 32 x TransformerLayer(
27:         (input_layernorm): IdentityOp()
27:         (self_attention): SelfAttention(
27:           (core_attention): TEDotProductAttention(
27:             (flash_attention): FlashAttention()
27:             (fused_attention): FusedAttention()
27:             (unfused_attention): UnfusedDotProductAttention(
27:               (scale_mask_softmax): FusedScaleMaskSoftmax()
27:               (attention_dropout): Dropout(p=0.0, inplace=False)
27:             )
27:           )
27:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
27:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
27:           (q_layernorm): IdentityOp()
27:           (k_layernorm): IdentityOp()
58:         )
58:         (pre_cross_attn_layernorm): IdentityOp()
58:         (cross_attention): IdentityOp()
58:         (cross_attn_bda): IdentityFuncOp()
58:         (pre_mlp_layernorm): IdentityOp()
58:         (mlp): TEFusedMLP(
58:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
58:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
58:         )
58:       )
58:     )
58:     (final_layernorm): RMSNorm()
58:   )
58:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
11: )
19: GPTModel(
19:   (embedding): LanguageModelEmbedding(
19:     (word_embeddings): VocabParallelEmbedding()
19:     (embedding_dropout): Dropout(p=0.0, inplace=False)
19:   )
19:   (rotary_pos_emb): RotaryEmbedding()
19:   (decoder): TransformerBlock(
19:     (layers): ModuleList(
19:       (0-31): 32 x TransformerLayer(
19:         (input_layernorm): IdentityOp()
19:         (self_attention): SelfAttention(
19:           (core_attention): TEDotProductAttention(
19:             (flash_attention): FlashAttention()
19:             (fused_attention): FusedAttention()
19:             (unfused_attention): UnfusedDotProductAttention(
19:               (scale_mask_softmax): FusedScaleMaskSoftmax()
19:               (attention_dropout): Dropout(p=0.0, inplace=False)
19:             )
19:           )
19:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
19:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
19:           (q_layernorm): IdentityOp()
19:           (k_layernorm): IdentityOp()
51: GPTModel(
51:   (embedding): LanguageModelEmbedding(
51:     (word_embeddings): VocabParallelEmbedding()
51:     (embedding_dropout): Dropout(p=0.0, inplace=False)
51:   )
51:   (rotary_pos_emb): RotaryEmbedding()
51:   (decoder): TransformerBlock(
51:     (layers): ModuleList(
51:       (0-31): 32 x TransformerLayer(
51:         (input_layernorm): IdentityOp()
51:         (self_attention): SelfAttention(
51:           (core_attention): TEDotProductAttention(
51:             (flash_attention): FlashAttention()
51:             (fused_attention): FusedAttention()
51:             (unfused_attention): UnfusedDotProductAttention(
51:               (scale_mask_softmax): FusedScaleMaskSoftmax()
51:               (attention_dropout): Dropout(p=0.0, inplace=False)
51:             )
51:           )
51:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
51:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
51:           (q_layernorm): IdentityOp()
51:           (k_layernorm): IdentityOp()
43: GPTModel(
43:   (embedding): LanguageModelEmbedding(
43:     (word_embeddings): VocabParallelEmbedding()
43:     (embedding_dropout): Dropout(p=0.0, inplace=False)
43:   )
43:   (rotary_pos_emb): RotaryEmbedding()
43:   (decoder): TransformerBlock(
43:     (layers): ModuleList(
43:       (0-31): 32 x TransformerLayer(
43:         (input_layernorm): IdentityOp()
43:         (self_attention): SelfAttention(
43:           (core_attention): TEDotProductAttention(
43:             (flash_attention): FlashAttention()
43:             (fused_attention): FusedAttention()
43:             (unfused_attention): UnfusedDotProductAttention(
43:               (scale_mask_softmax): FusedScaleMaskSoftmax()
43:               (attention_dropout): Dropout(p=0.0, inplace=False)
43:             )
43:           )
43:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
43:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
43:           (q_layernorm): IdentityOp()
43:           (k_layernorm): IdentityOp()
34:         )
34:         (pre_cross_attn_layernorm): IdentityOp()
34:         (cross_attention): IdentityOp()
34:         (cross_attn_bda): IdentityFuncOp()
34:         (pre_mlp_layernorm): IdentityOp()
34:         (mlp): TEFusedMLP(
34:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
34:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
34:         )
34:       )
34:     )
34:     (final_layernorm): RMSNorm()
34:   )
34:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 4: GPTModel(
 4:   (embedding): LanguageModelEmbedding(
 4:     (word_embeddings): VocabParallelEmbedding()
 4:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 4:   )
 4:   (rotary_pos_emb): RotaryEmbedding()
 4:   (decoder): TransformerBlock(
 4:     (layers): ModuleList(
 4:       (0-31): 32 x TransformerLayer(
 4:         (input_layernorm): IdentityOp()
 4:         (self_attention): SelfAttention(
 4:           (core_attention): TEDotProductAttention(
 4:             (flash_attention): FlashAttention()
 4:             (fused_attention): FusedAttention()
 4:             (unfused_attention): UnfusedDotProductAttention(
 4:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 4:               (attention_dropout): Dropout(p=0.0, inplace=False)
 4:             )
 4:           )
 4:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 4:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 4:           (q_layernorm): IdentityOp()
 4:           (k_layernorm): IdentityOp()
27:         )
27:         (pre_cross_attn_layernorm): IdentityOp()
27:         (cross_attention): IdentityOp()
27:         (cross_attn_bda): IdentityFuncOp()
27:         (pre_mlp_layernorm): IdentityOp()
27:         (mlp): TEFusedMLP(
27:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
27:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
27:         )
27:       )
27:     )
27:     (final_layernorm): RMSNorm()
27:   )
27:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
58: )
12: GPTModel(
12:   (embedding): LanguageModelEmbedding(
12:     (word_embeddings): VocabParallelEmbedding()
12:     (embedding_dropout): Dropout(p=0.0, inplace=False)
12:   )
12:   (rotary_pos_emb): RotaryEmbedding()
12:   (decoder): TransformerBlock(
12:     (layers): ModuleList(
12:       (0-31): 32 x TransformerLayer(
12:         (input_layernorm): IdentityOp()
12:         (self_attention): SelfAttention(
12:           (core_attention): TEDotProductAttention(
12:             (flash_attention): FlashAttention()
12:             (fused_attention): FusedAttention()
12:             (unfused_attention): UnfusedDotProductAttention(
12:               (scale_mask_softmax): FusedScaleMaskSoftmax()
12:               (attention_dropout): Dropout(p=0.0, inplace=False)
12:             )
12:           )
12:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
12:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
12:           (q_layernorm): IdentityOp()
12:           (k_layernorm): IdentityOp()
19:         )
19:         (pre_cross_attn_layernorm): IdentityOp()
19:         (cross_attention): IdentityOp()
19:         (cross_attn_bda): IdentityFuncOp()
19:         (pre_mlp_layernorm): IdentityOp()
19:         (mlp): TEFusedMLP(
19:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
19:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
19:         )
19:       )
19:     )
19:     (final_layernorm): RMSNorm()
19:   )
19:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
51:         )
51:         (pre_cross_attn_layernorm): IdentityOp()
51:         (cross_attention): IdentityOp()
51:         (cross_attn_bda): IdentityFuncOp()
51:         (pre_mlp_layernorm): IdentityOp()
51:         (mlp): TEFusedMLP(
51:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
51:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
51:         )
51:       )
51:     )
51:     (final_layernorm): RMSNorm()
51:   )
51:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
43:         )
43:         (pre_cross_attn_layernorm): IdentityOp()
43:         (cross_attention): IdentityOp()
43:         (cross_attn_bda): IdentityFuncOp()
43:         (pre_mlp_layernorm): IdentityOp()
43:         (mlp): TEFusedMLP(
43:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
43:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
43:         )
43:       )
43:     )
43:     (final_layernorm): RMSNorm()
43:   )
43:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
34: )
 4:         )
 4:         (pre_cross_attn_layernorm): IdentityOp()
 4:         (cross_attention): IdentityOp()
 4:         (cross_attn_bda): IdentityFuncOp()
 4:         (pre_mlp_layernorm): IdentityOp()
 4:         (mlp): TEFusedMLP(
 4:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 4:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 4:         )
 4:       )
 4:     )
 4:     (final_layernorm): RMSNorm()
 4:   )
 4:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
26: GPTModel(
26:   (embedding): LanguageModelEmbedding(
26:     (word_embeddings): VocabParallelEmbedding()
26:     (embedding_dropout): Dropout(p=0.0, inplace=False)
26:   )
26:   (rotary_pos_emb): RotaryEmbedding()
26:   (decoder): TransformerBlock(
26:     (layers): ModuleList(
26:       (0-31): 32 x TransformerLayer(
26:         (input_layernorm): IdentityOp()
26:         (self_attention): SelfAttention(
26:           (core_attention): TEDotProductAttention(
26:             (flash_attention): FlashAttention()
26:             (fused_attention): FusedAttention()
26:             (unfused_attention): UnfusedDotProductAttention(
26:               (scale_mask_softmax): FusedScaleMaskSoftmax()
26:               (attention_dropout): Dropout(p=0.0, inplace=False)
26:             )
26:           )
26:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
26:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
26:           (q_layernorm): IdentityOp()
26:           (k_layernorm): IdentityOp()
59: GPTModel(
59:   (embedding): LanguageModelEmbedding(
59:     (word_embeddings): VocabParallelEmbedding()
59:     (embedding_dropout): Dropout(p=0.0, inplace=False)
59:   )
59:   (rotary_pos_emb): RotaryEmbedding()
59:   (decoder): TransformerBlock(
59:     (layers): ModuleList(
59:       (0-31): 32 x TransformerLayer(
59:         (input_layernorm): IdentityOp()
59:         (self_attention): SelfAttention(
59:           (core_attention): TEDotProductAttention(
59:             (flash_attention): FlashAttention()
59:             (fused_attention): FusedAttention()
59:             (unfused_attention): UnfusedDotProductAttention(
59:               (scale_mask_softmax): FusedScaleMaskSoftmax()
59:               (attention_dropout): Dropout(p=0.0, inplace=False)
59:             )
59:           )
59:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
59:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
59:           (q_layernorm): IdentityOp()
59:           (k_layernorm): IdentityOp()
12:         )
12:         (pre_cross_attn_layernorm): IdentityOp()
12:         (cross_attention): IdentityOp()
12:         (cross_attn_bda): IdentityFuncOp()
12:         (pre_mlp_layernorm): IdentityOp()
12:         (mlp): TEFusedMLP(
12:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
12:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
12:         )
12:       )
12:     )
12:     (final_layernorm): RMSNorm()
12:   )
12:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
19: )
51: )
42: )
35: GPTModel(
35:   (embedding): LanguageModelEmbedding(
35:     (word_embeddings): VocabParallelEmbedding()
35:     (embedding_dropout): Dropout(p=0.0, inplace=False)
35:   )
35:   (rotary_pos_emb): RotaryEmbedding()
35:   (decoder): TransformerBlock(
35:     (layers): ModuleList(
35:       (0-31): 32 x TransformerLayer(
35:         (input_layernorm): IdentityOp()
35:         (self_attention): SelfAttention(
35:           (core_attention): TEDotProductAttention(
35:             (flash_attention): FlashAttention()
35:             (fused_attention): FusedAttention()
35:             (unfused_attention): UnfusedDotProductAttention(
35:               (scale_mask_softmax): FusedScaleMaskSoftmax()
35:               (attention_dropout): Dropout(p=0.0, inplace=False)
35:             )
35:           )
35:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
35:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
35:           (q_layernorm): IdentityOp()
35:           (k_layernorm): IdentityOp()
 4: )
26:         )
26:         (pre_cross_attn_layernorm): IdentityOp()
26:         (cross_attention): IdentityOp()
26:         (cross_attn_bda): IdentityFuncOp()
26:         (pre_mlp_layernorm): IdentityOp()
26:         (mlp): TEFusedMLP(
26:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
26:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
26:         )
26:       )
26:     )
26:     (final_layernorm): RMSNorm()
26:   )
26:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
59:         )
59:         (pre_cross_attn_layernorm): IdentityOp()
59:         (cross_attention): IdentityOp()
59:         (cross_attn_bda): IdentityFuncOp()
59:         (pre_mlp_layernorm): IdentityOp()
59:         (mlp): TEFusedMLP(
59:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
59:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
59:         )
59:       )
59:     )
59:     (final_layernorm): RMSNorm()
59:   )
59:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
12: )
20: GPTModel(
20:   (embedding): LanguageModelEmbedding(
20:     (word_embeddings): VocabParallelEmbedding()
20:     (embedding_dropout): Dropout(p=0.0, inplace=False)
20:   )
20:   (rotary_pos_emb): RotaryEmbedding()
20:   (decoder): TransformerBlock(
20:     (layers): ModuleList(
20:       (0-31): 32 x TransformerLayer(
20:         (input_layernorm): IdentityOp()
20:         (self_attention): SelfAttention(
20:           (core_attention): TEDotProductAttention(
20:             (flash_attention): FlashAttention()
20:             (fused_attention): FusedAttention()
20:             (unfused_attention): UnfusedDotProductAttention(
20:               (scale_mask_softmax): FusedScaleMaskSoftmax()
20:               (attention_dropout): Dropout(p=0.0, inplace=False)
20:             )
20:           )
20:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
20:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
20:           (q_layernorm): IdentityOp()
20:           (k_layernorm): IdentityOp()
52: GPTModel(
52:   (embedding): LanguageModelEmbedding(
52:     (word_embeddings): VocabParallelEmbedding()
52:     (embedding_dropout): Dropout(p=0.0, inplace=False)
52:   )
52:   (rotary_pos_emb): RotaryEmbedding()
52:   (decoder): TransformerBlock(
52:     (layers): ModuleList(
52:       (0-31): 32 x TransformerLayer(
52:         (input_layernorm): IdentityOp()
52:         (self_attention): SelfAttention(
52:           (core_attention): TEDotProductAttention(
52:             (flash_attention): FlashAttention()
52:             (fused_attention): FusedAttention()
52:             (unfused_attention): UnfusedDotProductAttention(
52:               (scale_mask_softmax): FusedScaleMaskSoftmax()
52:               (attention_dropout): Dropout(p=0.0, inplace=False)
52:             )
52:           )
52:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
52:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
52:           (q_layernorm): IdentityOp()
52:           (k_layernorm): IdentityOp()
43: )
35:         )
35:         (pre_cross_attn_layernorm): IdentityOp()
35:         (cross_attention): IdentityOp()
35:         (cross_attn_bda): IdentityFuncOp()
35:         (pre_mlp_layernorm): IdentityOp()
35:         (mlp): TEFusedMLP(
35:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
35:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
35:         )
35:       )
35:     )
35:     (final_layernorm): RMSNorm()
35:   )
35:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 3: GPTModel(
 3:   (embedding): LanguageModelEmbedding(
 3:     (word_embeddings): VocabParallelEmbedding()
 3:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 3:   )
 3:   (rotary_pos_emb): RotaryEmbedding()
 3:   (decoder): TransformerBlock(
 3:     (layers): ModuleList(
 3:       (0-31): 32 x TransformerLayer(
 3:         (input_layernorm): IdentityOp()
 3:         (self_attention): SelfAttention(
 3:           (core_attention): TEDotProductAttention(
 3:             (flash_attention): FlashAttention()
 3:             (fused_attention): FusedAttention()
 3:             (unfused_attention): UnfusedDotProductAttention(
 3:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 3:               (attention_dropout): Dropout(p=0.0, inplace=False)
 3:             )
 3:           )
 3:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 3:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 3:           (q_layernorm): IdentityOp()
 3:           (k_layernorm): IdentityOp()
27: )
59: )
20:         )
20:         (pre_cross_attn_layernorm): IdentityOp()
20:         (cross_attention): IdentityOp()
20:         (cross_attn_bda): IdentityFuncOp()
20:         (pre_mlp_layernorm): IdentityOp()
20:         (mlp): TEFusedMLP(
20:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
20:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
20:         )
20:       )
20:     )
20:     (final_layernorm): RMSNorm()
20:   )
20:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
52:         )
52:         (pre_cross_attn_layernorm): IdentityOp()
52:         (cross_attention): IdentityOp()
52:         (cross_attn_bda): IdentityFuncOp()
52:         (pre_mlp_layernorm): IdentityOp()
52:         (mlp): TEFusedMLP(
52:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
52:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
52:         )
52:       )
52:     )
52:     (final_layernorm): RMSNorm()
52:   )
52:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
44: GPTModel(
44:   (embedding): LanguageModelEmbedding(
44:     (word_embeddings): VocabParallelEmbedding()
44:     (embedding_dropout): Dropout(p=0.0, inplace=False)
44:   )
44:   (rotary_pos_emb): RotaryEmbedding()
44:   (decoder): TransformerBlock(
44:     (layers): ModuleList(
44:       (0-31): 32 x TransformerLayer(
44:         (input_layernorm): IdentityOp()
44:         (self_attention): SelfAttention(
44:           (core_attention): TEDotProductAttention(
44:             (flash_attention): FlashAttention()
44:             (fused_attention): FusedAttention()
44:             (unfused_attention): UnfusedDotProductAttention(
44:               (scale_mask_softmax): FusedScaleMaskSoftmax()
44:               (attention_dropout): Dropout(p=0.0, inplace=False)
44:             )
44:           )
44:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
44:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
44:           (q_layernorm): IdentityOp()
44:           (k_layernorm): IdentityOp()
35: )
 3:         )
 3:         (pre_cross_attn_layernorm): IdentityOp()
 3:         (cross_attention): IdentityOp()
 3:         (cross_attn_bda): IdentityFuncOp()
 3:         (pre_mlp_layernorm): IdentityOp()
 3:         (mlp): TEFusedMLP(
 3:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 3:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 3:         )
 3:       )
 3:     )
 3:     (final_layernorm): RMSNorm()
 3:   )
 3:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
26: )
60: GPTModel(
60:   (embedding): LanguageModelEmbedding(
60:     (word_embeddings): VocabParallelEmbedding()
60:     (embedding_dropout): Dropout(p=0.0, inplace=False)
60:   )
60:   (rotary_pos_emb): RotaryEmbedding()
60:   (decoder): TransformerBlock(
60:     (layers): ModuleList(
60:       (0-31): 32 x TransformerLayer(
60:         (input_layernorm): IdentityOp()
60:         (self_attention): SelfAttention(
60:           (core_attention): TEDotProductAttention(
60:             (flash_attention): FlashAttention()
60:             (fused_attention): FusedAttention()
60:             (unfused_attention): UnfusedDotProductAttention(
60:               (scale_mask_softmax): FusedScaleMaskSoftmax()
60:               (attention_dropout): Dropout(p=0.0, inplace=False)
60:             )
60:           )
60:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
60:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
60:           (q_layernorm): IdentityOp()
60:           (k_layernorm): IdentityOp()
13: GPTModel(
13:   (embedding): LanguageModelEmbedding(
13:     (word_embeddings): VocabParallelEmbedding()
13:     (embedding_dropout): Dropout(p=0.0, inplace=False)
13:   )
13:   (rotary_pos_emb): RotaryEmbedding()
13:   (decoder): TransformerBlock(
13:     (layers): ModuleList(
13:       (0-31): 32 x TransformerLayer(
13:         (input_layernorm): IdentityOp()
13:         (self_attention): SelfAttention(
13:           (core_attention): TEDotProductAttention(
13:             (flash_attention): FlashAttention()
13:             (fused_attention): FusedAttention()
13:             (unfused_attention): UnfusedDotProductAttention(
13:               (scale_mask_softmax): FusedScaleMaskSoftmax()
13:               (attention_dropout): Dropout(p=0.0, inplace=False)
13:             )
13:           )
13:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
13:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
13:           (q_layernorm): IdentityOp()
13:           (k_layernorm): IdentityOp()
21: GPTModel(
21:   (embedding): LanguageModelEmbedding(
21:     (word_embeddings): VocabParallelEmbedding()
21:     (embedding_dropout): Dropout(p=0.0, inplace=False)
21:   )
21:   (rotary_pos_emb): RotaryEmbedding()
21:   (decoder): TransformerBlock(
21:     (layers): ModuleList(
21:       (0-31): 32 x TransformerLayer(
21:         (input_layernorm): IdentityOp()
21:         (self_attention): SelfAttention(
21:           (core_attention): TEDotProductAttention(
21:             (flash_attention): FlashAttention()
21:             (fused_attention): FusedAttention()
21:             (unfused_attention): UnfusedDotProductAttention(
21:               (scale_mask_softmax): FusedScaleMaskSoftmax()
21:               (attention_dropout): Dropout(p=0.0, inplace=False)
21:             )
21:           )
21:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
21:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
21:           (q_layernorm): IdentityOp()
21:           (k_layernorm): IdentityOp()
52: )
44:         )
44:         (pre_cross_attn_layernorm): IdentityOp()
44:         (cross_attention): IdentityOp()
44:         (cross_attn_bda): IdentityFuncOp()
44:         (pre_mlp_layernorm): IdentityOp()
44:         (mlp): TEFusedMLP(
44:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
44:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
44:         )
44:       )
44:     )
44:     (final_layernorm): RMSNorm()
44:   )
44:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
36: GPTModel(
36:   (embedding): LanguageModelEmbedding(
36:     (word_embeddings): VocabParallelEmbedding()
36:     (embedding_dropout): Dropout(p=0.0, inplace=False)
36:   )
36:   (rotary_pos_emb): RotaryEmbedding()
36:   (decoder): TransformerBlock(
36:     (layers): ModuleList(
36:       (0-31): 32 x TransformerLayer(
36:         (input_layernorm): IdentityOp()
36:         (self_attention): SelfAttention(
36:           (core_attention): TEDotProductAttention(
36:             (flash_attention): FlashAttention()
36:             (fused_attention): FusedAttention()
36:             (unfused_attention): UnfusedDotProductAttention(
36:               (scale_mask_softmax): FusedScaleMaskSoftmax()
36:               (attention_dropout): Dropout(p=0.0, inplace=False)
36:             )
36:           )
36:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
36:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
36:           (q_layernorm): IdentityOp()
36:           (k_layernorm): IdentityOp()
 3: )
28: GPTModel(
28:   (embedding): LanguageModelEmbedding(
28:     (word_embeddings): VocabParallelEmbedding()
28:     (embedding_dropout): Dropout(p=0.0, inplace=False)
28:   )
28:   (rotary_pos_emb): RotaryEmbedding()
28:   (decoder): TransformerBlock(
28:     (layers): ModuleList(
28:       (0-31): 32 x TransformerLayer(
28:         (input_layernorm): IdentityOp()
28:         (self_attention): SelfAttention(
28:           (core_attention): TEDotProductAttention(
28:             (flash_attention): FlashAttention()
28:             (fused_attention): FusedAttention()
28:             (unfused_attention): UnfusedDotProductAttention(
28:               (scale_mask_softmax): FusedScaleMaskSoftmax()
28:               (attention_dropout): Dropout(p=0.0, inplace=False)
28:             )
28:           )
28:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
28:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
28:           (q_layernorm): IdentityOp()
28:           (k_layernorm): IdentityOp()
60:         )
60:         (pre_cross_attn_layernorm): IdentityOp()
60:         (cross_attention): IdentityOp()
60:         (cross_attn_bda): IdentityFuncOp()
60:         (pre_mlp_layernorm): IdentityOp()
60:         (mlp): TEFusedMLP(
60:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
60:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
60:         )
60:       )
60:     )
60:     (final_layernorm): RMSNorm()
60:   )
60:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
13:         )
13:         (pre_cross_attn_layernorm): IdentityOp()
13:         (cross_attention): IdentityOp()
13:         (cross_attn_bda): IdentityFuncOp()
13:         (pre_mlp_layernorm): IdentityOp()
13:         (mlp): TEFusedMLP(
13:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
13:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
13:         )
13:       )
13:     )
13:     (final_layernorm): RMSNorm()
13:   )
13:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
21:         )
21:         (pre_cross_attn_layernorm): IdentityOp()
21:         (cross_attention): IdentityOp()
21:         (cross_attn_bda): IdentityFuncOp()
21:         (pre_mlp_layernorm): IdentityOp()
21:         (mlp): TEFusedMLP(
21:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
21:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
21:         )
21:       )
21:     )
21:     (final_layernorm): RMSNorm()
21:   )
21:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
53: GPTModel(
53:   (embedding): LanguageModelEmbedding(
53:     (word_embeddings): VocabParallelEmbedding()
53:     (embedding_dropout): Dropout(p=0.0, inplace=False)
53:   )
53:   (rotary_pos_emb): RotaryEmbedding()
53:   (decoder): TransformerBlock(
53:     (layers): ModuleList(
53:       (0-31): 32 x TransformerLayer(
53:         (input_layernorm): IdentityOp()
53:         (self_attention): SelfAttention(
53:           (core_attention): TEDotProductAttention(
53:             (flash_attention): FlashAttention()
53:             (fused_attention): FusedAttention()
53:             (unfused_attention): UnfusedDotProductAttention(
53:               (scale_mask_softmax): FusedScaleMaskSoftmax()
53:               (attention_dropout): Dropout(p=0.0, inplace=False)
53:             )
53:           )
53:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
53:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
53:           (q_layernorm): IdentityOp()
53:           (k_layernorm): IdentityOp()
44: )
36:         )
36:         (pre_cross_attn_layernorm): IdentityOp()
36:         (cross_attention): IdentityOp()
36:         (cross_attn_bda): IdentityFuncOp()
36:         (pre_mlp_layernorm): IdentityOp()
36:         (mlp): TEFusedMLP(
36:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
36:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
36:         )
36:       )
36:     )
36:     (final_layernorm): RMSNorm()
36:   )
36:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 5: GPTModel(
 5:   (embedding): LanguageModelEmbedding(
 5:     (word_embeddings): VocabParallelEmbedding()
 5:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 5:   )
 5:   (rotary_pos_emb): RotaryEmbedding()
 5:   (decoder): TransformerBlock(
 5:     (layers): ModuleList(
 5:       (0-31): 32 x TransformerLayer(
 5:         (input_layernorm): IdentityOp()
 5:         (self_attention): SelfAttention(
 5:           (core_attention): TEDotProductAttention(
 5:             (flash_attention): FlashAttention()
 5:             (fused_attention): FusedAttention()
 5:             (unfused_attention): UnfusedDotProductAttention(
 5:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 5:               (attention_dropout): Dropout(p=0.0, inplace=False)
 5:             )
 5:           )
 5:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 5:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 5:           (q_layernorm): IdentityOp()
 5:           (k_layernorm): IdentityOp()
28:         )
28:         (pre_cross_attn_layernorm): IdentityOp()
28:         (cross_attention): IdentityOp()
28:         (cross_attn_bda): IdentityFuncOp()
28:         (pre_mlp_layernorm): IdentityOp()
28:         (mlp): TEFusedMLP(
28:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
28:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
28:         )
28:       )
28:     )
28:     (final_layernorm): RMSNorm()
28:   )
28:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
60: )
15: GPTModel(
15:   (embedding): LanguageModelEmbedding(
15:     (word_embeddings): VocabParallelEmbedding()
15:     (embedding_dropout): Dropout(p=0.0, inplace=False)
15:   )
15:   (rotary_pos_emb): RotaryEmbedding()
15:   (decoder): TransformerBlock(
15:     (layers): ModuleList(
15:       (0-31): 32 x TransformerLayer(
15:         (input_layernorm): IdentityOp()
15:         (self_attention): SelfAttention(
15:           (core_attention): TEDotProductAttention(
15:             (flash_attention): FlashAttention()
15:             (fused_attention): FusedAttention()
15:             (unfused_attention): UnfusedDotProductAttention(
15:               (scale_mask_softmax): FusedScaleMaskSoftmax()
15:               (attention_dropout): Dropout(p=0.0, inplace=False)
15:             )
15:           )
15:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
15:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
15:           (q_layernorm): IdentityOp()
15:           (k_layernorm): IdentityOp()
20: )
53:         )
53:         (pre_cross_attn_layernorm): IdentityOp()
53:         (cross_attention): IdentityOp()
53:         (cross_attn_bda): IdentityFuncOp()
53:         (pre_mlp_layernorm): IdentityOp()
53:         (mlp): TEFusedMLP(
53:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
53:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
53:         )
53:       )
53:     )
53:     (final_layernorm): RMSNorm()
53:   )
53:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
45: GPTModel(
45:   (embedding): LanguageModelEmbedding(
45:     (word_embeddings): VocabParallelEmbedding()
45:     (embedding_dropout): Dropout(p=0.0, inplace=False)
45:   )
45:   (rotary_pos_emb): RotaryEmbedding()
45:   (decoder): TransformerBlock(
45:     (layers): ModuleList(
45:       (0-31): 32 x TransformerLayer(
45:         (input_layernorm): IdentityOp()
45:         (self_attention): SelfAttention(
45:           (core_attention): TEDotProductAttention(
45:             (flash_attention): FlashAttention()
45:             (fused_attention): FusedAttention()
45:             (unfused_attention): UnfusedDotProductAttention(
45:               (scale_mask_softmax): FusedScaleMaskSoftmax()
45:               (attention_dropout): Dropout(p=0.0, inplace=False)
45:             )
45:           )
45:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
45:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
45:           (q_layernorm): IdentityOp()
45:           (k_layernorm): IdentityOp()
36: )
 5:         )
 5:         (pre_cross_attn_layernorm): IdentityOp()
 5:         (cross_attention): IdentityOp()
 5:         (cross_attn_bda): IdentityFuncOp()
 5:         (pre_mlp_layernorm): IdentityOp()
 5:         (mlp): TEFusedMLP(
 5:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 5:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 5:         )
 5:       )
 5:     )
 5:     (final_layernorm): RMSNorm()
 5:   )
 5:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
28: )
61: GPTModel(
61:   (embedding): LanguageModelEmbedding(
61:     (word_embeddings): VocabParallelEmbedding()
61:     (embedding_dropout): Dropout(p=0.0, inplace=False)
61:   )
61:   (rotary_pos_emb): RotaryEmbedding()
61:   (decoder): TransformerBlock(
61:     (layers): ModuleList(
61:       (0-31): 32 x TransformerLayer(
61:         (input_layernorm): IdentityOp()
61:         (self_attention): SelfAttention(
61:           (core_attention): TEDotProductAttention(
61:             (flash_attention): FlashAttention()
61:             (fused_attention): FusedAttention()
61:             (unfused_attention): UnfusedDotProductAttention(
61:               (scale_mask_softmax): FusedScaleMaskSoftmax()
61:               (attention_dropout): Dropout(p=0.0, inplace=False)
61:             )
61:           )
61:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
61:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
61:           (q_layernorm): IdentityOp()
61:           (k_layernorm): IdentityOp()
15:         )
15:         (pre_cross_attn_layernorm): IdentityOp()
15:         (cross_attention): IdentityOp()
15:         (cross_attn_bda): IdentityFuncOp()
15:         (pre_mlp_layernorm): IdentityOp()
15:         (mlp): TEFusedMLP(
15:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
15:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
15:         )
15:       )
15:     )
15:     (final_layernorm): RMSNorm()
15:   )
15:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
15: )
21: )
53: )
45:         )
45:         (pre_cross_attn_layernorm): IdentityOp()
45:         (cross_attention): IdentityOp()
45:         (cross_attn_bda): IdentityFuncOp()
45:         (pre_mlp_layernorm): IdentityOp()
45:         (mlp): TEFusedMLP(
45:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
45:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
45:         )
45:       )
45:     )
45:     (final_layernorm): RMSNorm()
45:   )
45:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
39: GPTModel(
39:   (embedding): LanguageModelEmbedding(
39:     (word_embeddings): VocabParallelEmbedding()
39:     (embedding_dropout): Dropout(p=0.0, inplace=False)
39:   )
39:   (rotary_pos_emb): RotaryEmbedding()
39:   (decoder): TransformerBlock(
39:     (layers): ModuleList(
39:       (0-31): 32 x TransformerLayer(
39:         (input_layernorm): IdentityOp()
39:         (self_attention): SelfAttention(
39:           (core_attention): TEDotProductAttention(
39:             (flash_attention): FlashAttention()
39:             (fused_attention): FusedAttention()
39:             (unfused_attention): UnfusedDotProductAttention(
39:               (scale_mask_softmax): FusedScaleMaskSoftmax()
39:               (attention_dropout): Dropout(p=0.0, inplace=False)
39:             )
39:           )
39:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
39:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
39:           (q_layernorm): IdentityOp()
39:           (k_layernorm): IdentityOp()
 0: GPTModel(
 0:   (embedding): LanguageModelEmbedding(
 0:     (word_embeddings): VocabParallelEmbedding()
 0:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 0:   )
 0:   (rotary_pos_emb): RotaryEmbedding()
 0:   (decoder): TransformerBlock(
 0:     (layers): ModuleList(
 0:       (0-31): 32 x TransformerLayer(
 0:         (input_layernorm): IdentityOp()
 0:         (self_attention): SelfAttention(
 0:           (core_attention): TEDotProductAttention(
 0:             (flash_attention): FlashAttention()
 0:             (fused_attention): FusedAttention()
 0:             (unfused_attention): UnfusedDotProductAttention(
 0:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 0:               (attention_dropout): Dropout(p=0.0, inplace=False)
 0:             )
 0:           )
 0:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 0:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 0:           (q_layernorm): IdentityOp()
 0:           (k_layernorm): IdentityOp()
29: GPTModel(
29:   (embedding): LanguageModelEmbedding(
29:     (word_embeddings): VocabParallelEmbedding()
29:     (embedding_dropout): Dropout(p=0.0, inplace=False)
29:   )
29:   (rotary_pos_emb): RotaryEmbedding()
29:   (decoder): TransformerBlock(
29:     (layers): ModuleList(
29:       (0-31): 32 x TransformerLayer(
29:         (input_layernorm): IdentityOp()
29:         (self_attention): SelfAttention(
29:           (core_attention): TEDotProductAttention(
29:             (flash_attention): FlashAttention()
29:             (fused_attention): FusedAttention()
29:             (unfused_attention): UnfusedDotProductAttention(
29:               (scale_mask_softmax): FusedScaleMaskSoftmax()
29:               (attention_dropout): Dropout(p=0.0, inplace=False)
29:             )
29:           )
29:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
29:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
29:           (q_layernorm): IdentityOp()
29:           (k_layernorm): IdentityOp()
61:         )
61:         (pre_cross_attn_layernorm): IdentityOp()
61:         (cross_attention): IdentityOp()
61:         (cross_attn_bda): IdentityFuncOp()
61:         (pre_mlp_layernorm): IdentityOp()
61:         (mlp): TEFusedMLP(
61:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
61:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
61:         )
61:       )
61:     )
61:     (final_layernorm): RMSNorm()
61:   )
61:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
13: )
23: GPTModel(
23:   (embedding): LanguageModelEmbedding(
23:     (word_embeddings): VocabParallelEmbedding()
23:     (embedding_dropout): Dropout(p=0.0, inplace=False)
23:   )
23:   (rotary_pos_emb): RotaryEmbedding()
23:   (decoder): TransformerBlock(
23:     (layers): ModuleList(
23:       (0-31): 32 x TransformerLayer(
23:         (input_layernorm): IdentityOp()
23:         (self_attention): SelfAttention(
23:           (core_attention): TEDotProductAttention(
23:             (flash_attention): FlashAttention()
23:             (fused_attention): FusedAttention()
23:             (unfused_attention): UnfusedDotProductAttention(
23:               (scale_mask_softmax): FusedScaleMaskSoftmax()
23:               (attention_dropout): Dropout(p=0.0, inplace=False)
23:             )
23:           )
23:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
23:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
23:           (q_layernorm): IdentityOp()
23:           (k_layernorm): IdentityOp()
55: GPTModel(
55:   (embedding): LanguageModelEmbedding(
55:     (word_embeddings): VocabParallelEmbedding()
55:     (embedding_dropout): Dropout(p=0.0, inplace=False)
55:   )
55:   (rotary_pos_emb): RotaryEmbedding()
55:   (decoder): TransformerBlock(
55:     (layers): ModuleList(
55:       (0-31): 32 x TransformerLayer(
55:         (input_layernorm): IdentityOp()
55:         (self_attention): SelfAttention(
55:           (core_attention): TEDotProductAttention(
55:             (flash_attention): FlashAttention()
55:             (fused_attention): FusedAttention()
55:             (unfused_attention): UnfusedDotProductAttention(
55:               (scale_mask_softmax): FusedScaleMaskSoftmax()
55:               (attention_dropout): Dropout(p=0.0, inplace=False)
55:             )
55:           )
55:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
55:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
55:           (q_layernorm): IdentityOp()
55:           (k_layernorm): IdentityOp()
47: GPTModel(
47:   (embedding): LanguageModelEmbedding(
47:     (word_embeddings): VocabParallelEmbedding()
47:     (embedding_dropout): Dropout(p=0.0, inplace=False)
47:   )
47:   (rotary_pos_emb): RotaryEmbedding()
47:   (decoder): TransformerBlock(
47:     (layers): ModuleList(
47:       (0-31): 32 x TransformerLayer(
47:         (input_layernorm): IdentityOp()
47:         (self_attention): SelfAttention(
47:           (core_attention): TEDotProductAttention(
47:             (flash_attention): FlashAttention()
47:             (fused_attention): FusedAttention()
47:             (unfused_attention): UnfusedDotProductAttention(
47:               (scale_mask_softmax): FusedScaleMaskSoftmax()
47:               (attention_dropout): Dropout(p=0.0, inplace=False)
47:             )
47:           )
47:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
47:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
47:           (q_layernorm): IdentityOp()
47:           (k_layernorm): IdentityOp()
39:         )
39:         (pre_cross_attn_layernorm): IdentityOp()
39:         (cross_attention): IdentityOp()
39:         (cross_attn_bda): IdentityFuncOp()
39:         (pre_mlp_layernorm): IdentityOp()
39:         (mlp): TEFusedMLP(
39:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
39:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
39:         )
39:       )
39:     )
39:     (final_layernorm): RMSNorm()
39:   )
39:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
39: )
 0:         )
 0:         (pre_cross_attn_layernorm): IdentityOp()
 0:         (cross_attention): IdentityOp()
 0:         (cross_attn_bda): IdentityFuncOp()
 0:         (pre_mlp_layernorm): IdentityOp()
 0:         (mlp): TEFusedMLP(
 0:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 0:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 0:         )
 0:       )
 0:     )
 0:     (final_layernorm): RMSNorm()
 0:   )
 0:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 0: )
29:         )
29:         (pre_cross_attn_layernorm): IdentityOp()
29:         (cross_attention): IdentityOp()
29:         (cross_attn_bda): IdentityFuncOp()
29:         (pre_mlp_layernorm): IdentityOp()
29:         (mlp): TEFusedMLP(
29:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
29:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
29:         )
29:       )
29:     )
29:     (final_layernorm): RMSNorm()
29:   )
29:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
61: )
14: GPTModel(
14:   (embedding): LanguageModelEmbedding(
14:     (word_embeddings): VocabParallelEmbedding()
14:     (embedding_dropout): Dropout(p=0.0, inplace=False)
14:   )
14:   (rotary_pos_emb): RotaryEmbedding()
14:   (decoder): TransformerBlock(
14:     (layers): ModuleList(
14:       (0-31): 32 x TransformerLayer(
14:         (input_layernorm): IdentityOp()
14:         (self_attention): SelfAttention(
14:           (core_attention): TEDotProductAttention(
14:             (flash_attention): FlashAttention()
14:             (fused_attention): FusedAttention()
14:             (unfused_attention): UnfusedDotProductAttention(
14:               (scale_mask_softmax): FusedScaleMaskSoftmax()
14:               (attention_dropout): Dropout(p=0.0, inplace=False)
14:             )
14:           )
14:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
14:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
14:           (q_layernorm): IdentityOp()
14:           (k_layernorm): IdentityOp()
23:         )
23:         (pre_cross_attn_layernorm): IdentityOp()
23:         (cross_attention): IdentityOp()
23:         (cross_attn_bda): IdentityFuncOp()
23:         (pre_mlp_layernorm): IdentityOp()
23:         (mlp): TEFusedMLP(
23:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
23:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
23:         )
23:       )
23:     )
23:     (final_layernorm): RMSNorm()
23:   )
23:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
55:         )
55:         (pre_cross_attn_layernorm): IdentityOp()
55:         (cross_attention): IdentityOp()
55:         (cross_attn_bda): IdentityFuncOp()
55:         (pre_mlp_layernorm): IdentityOp()
55:         (mlp): TEFusedMLP(
55:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
55:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
55:         )
55:       )
55:     )
55:     (final_layernorm): RMSNorm()
55:   )
55:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
47:         )
47:         (pre_cross_attn_layernorm): IdentityOp()
47:         (cross_attention): IdentityOp()
47:         (cross_attn_bda): IdentityFuncOp()
47:         (pre_mlp_layernorm): IdentityOp()
47:         (mlp): TEFusedMLP(
47:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
47:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
47:         )
47:       )
47:     )
47:     (final_layernorm): RMSNorm()
47:   )
47:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
47: )
 7: GPTModel(
 7:   (embedding): LanguageModelEmbedding(
 7:     (word_embeddings): VocabParallelEmbedding()
 7:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 7:   )
 7:   (rotary_pos_emb): RotaryEmbedding()
 7:   (decoder): TransformerBlock(
 7:     (layers): ModuleList(
 7:       (0-31): 32 x TransformerLayer(
 7:         (input_layernorm): IdentityOp()
 7:         (self_attention): SelfAttention(
 7:           (core_attention): TEDotProductAttention(
 7:             (flash_attention): FlashAttention()
 7:             (fused_attention): FusedAttention()
 7:             (unfused_attention): UnfusedDotProductAttention(
 7:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 7:               (attention_dropout): Dropout(p=0.0, inplace=False)
 7:             )
 7:           )
 7:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 7:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 7:           (q_layernorm): IdentityOp()
 7:           (k_layernorm): IdentityOp()
29: )
62: GPTModel(
62:   (embedding): LanguageModelEmbedding(
62:     (word_embeddings): VocabParallelEmbedding()
62:     (embedding_dropout): Dropout(p=0.0, inplace=False)
62:   )
62:   (rotary_pos_emb): RotaryEmbedding()
62:   (decoder): TransformerBlock(
62:     (layers): ModuleList(
62:       (0-31): 32 x TransformerLayer(
62:         (input_layernorm): IdentityOp()
62:         (self_attention): SelfAttention(
62:           (core_attention): TEDotProductAttention(
62:             (flash_attention): FlashAttention()
62:             (fused_attention): FusedAttention()
62:             (unfused_attention): UnfusedDotProductAttention(
62:               (scale_mask_softmax): FusedScaleMaskSoftmax()
62:               (attention_dropout): Dropout(p=0.0, inplace=False)
62:             )
62:           )
62:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
62:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
62:           (q_layernorm): IdentityOp()
62:           (k_layernorm): IdentityOp()
14:         )
14:         (pre_cross_attn_layernorm): IdentityOp()
14:         (cross_attention): IdentityOp()
14:         (cross_attn_bda): IdentityFuncOp()
14:         (pre_mlp_layernorm): IdentityOp()
14:         (mlp): TEFusedMLP(
14:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
14:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
14:         )
14:       )
14:     )
14:     (final_layernorm): RMSNorm()
14:   )
14:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
22: GPTModel(
22:   (embedding): LanguageModelEmbedding(
22:     (word_embeddings): VocabParallelEmbedding()
22:     (embedding_dropout): Dropout(p=0.0, inplace=False)
22:   )
22:   (rotary_pos_emb): RotaryEmbedding()
22:   (decoder): TransformerBlock(
22:     (layers): ModuleList(
22:       (0-31): 32 x TransformerLayer(
22:         (input_layernorm): IdentityOp()
22:         (self_attention): SelfAttention(
22:           (core_attention): TEDotProductAttention(
22:             (flash_attention): FlashAttention()
22:             (fused_attention): FusedAttention()
22:             (unfused_attention): UnfusedDotProductAttention(
22:               (scale_mask_softmax): FusedScaleMaskSoftmax()
22:               (attention_dropout): Dropout(p=0.0, inplace=False)
22:             )
22:           )
22:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
22:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
22:           (q_layernorm): IdentityOp()
22:           (k_layernorm): IdentityOp()
55: )
45: )
 7:         )
 7:         (pre_cross_attn_layernorm): IdentityOp()
 7:         (cross_attention): IdentityOp()
 7:         (cross_attn_bda): IdentityFuncOp()
 7:         (pre_mlp_layernorm): IdentityOp()
 7:         (mlp): TEFusedMLP(
 7:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 7:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 7:         )
 7:       )
 7:     )
 7:     (final_layernorm): RMSNorm()
 7:   )
 7:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 7: )
31: GPTModel(
31:   (embedding): LanguageModelEmbedding(
31:     (word_embeddings): VocabParallelEmbedding()
31:     (embedding_dropout): Dropout(p=0.0, inplace=False)
31:   )
31:   (rotary_pos_emb): RotaryEmbedding()
31:   (decoder): TransformerBlock(
31:     (layers): ModuleList(
31:       (0-31): 32 x TransformerLayer(
31:         (input_layernorm): IdentityOp()
31:         (self_attention): SelfAttention(
31:           (core_attention): TEDotProductAttention(
31:             (flash_attention): FlashAttention()
31:             (fused_attention): FusedAttention()
31:             (unfused_attention): UnfusedDotProductAttention(
31:               (scale_mask_softmax): FusedScaleMaskSoftmax()
31:               (attention_dropout): Dropout(p=0.0, inplace=False)
31:             )
31:           )
31:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
31:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
31:           (q_layernorm): IdentityOp()
31:           (k_layernorm): IdentityOp()
62:         )
62:         (pre_cross_attn_layernorm): IdentityOp()
62:         (cross_attention): IdentityOp()
62:         (cross_attn_bda): IdentityFuncOp()
62:         (pre_mlp_layernorm): IdentityOp()
62:         (mlp): TEFusedMLP(
62:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
62:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
62:         )
62:       )
62:     )
62:     (final_layernorm): RMSNorm()
62:   )
62:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
14: )
22:         )
22:         (pre_cross_attn_layernorm): IdentityOp()
22:         (cross_attention): IdentityOp()
22:         (cross_attn_bda): IdentityFuncOp()
22:         (pre_mlp_layernorm): IdentityOp()
22:         (mlp): TEFusedMLP(
22:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
22:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
22:         )
22:       )
22:     )
22:     (final_layernorm): RMSNorm()
22:   )
22:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
54: GPTModel(
54:   (embedding): LanguageModelEmbedding(
54:     (word_embeddings): VocabParallelEmbedding()
54:     (embedding_dropout): Dropout(p=0.0, inplace=False)
54:   )
54:   (rotary_pos_emb): RotaryEmbedding()
54:   (decoder): TransformerBlock(
54:     (layers): ModuleList(
54:       (0-31): 32 x TransformerLayer(
54:         (input_layernorm): IdentityOp()
54:         (self_attention): SelfAttention(
54:           (core_attention): TEDotProductAttention(
54:             (flash_attention): FlashAttention()
54:             (fused_attention): FusedAttention()
54:             (unfused_attention): UnfusedDotProductAttention(
54:               (scale_mask_softmax): FusedScaleMaskSoftmax()
54:               (attention_dropout): Dropout(p=0.0, inplace=False)
54:             )
54:           )
54:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
54:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
54:           (q_layernorm): IdentityOp()
54:           (k_layernorm): IdentityOp()
46: GPTModel(
46:   (embedding): LanguageModelEmbedding(
46:     (word_embeddings): VocabParallelEmbedding()
46:     (embedding_dropout): Dropout(p=0.0, inplace=False)
46:   )
46:   (rotary_pos_emb): RotaryEmbedding()
46:   (decoder): TransformerBlock(
46:     (layers): ModuleList(
46:       (0-31): 32 x TransformerLayer(
46:         (input_layernorm): IdentityOp()
46:         (self_attention): SelfAttention(
46:           (core_attention): TEDotProductAttention(
46:             (flash_attention): FlashAttention()
46:             (fused_attention): FusedAttention()
46:             (unfused_attention): UnfusedDotProductAttention(
46:               (scale_mask_softmax): FusedScaleMaskSoftmax()
46:               (attention_dropout): Dropout(p=0.0, inplace=False)
46:             )
46:           )
46:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
46:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
46:           (q_layernorm): IdentityOp()
46:           (k_layernorm): IdentityOp()
 5: )
31:         )
31:         (pre_cross_attn_layernorm): IdentityOp()
31:         (cross_attention): IdentityOp()
31:         (cross_attn_bda): IdentityFuncOp()
31:         (pre_mlp_layernorm): IdentityOp()
31:         (mlp): TEFusedMLP(
31:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
31:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
31:         )
31:       )
31:     )
31:     (final_layernorm): RMSNorm()
31:   )
31:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
62: )
23: )
54:         )
54:         (pre_cross_attn_layernorm): IdentityOp()
54:         (cross_attention): IdentityOp()
54:         (cross_attn_bda): IdentityFuncOp()
54:         (pre_mlp_layernorm): IdentityOp()
54:         (mlp): TEFusedMLP(
54:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
54:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
54:         )
54:       )
54:     )
54:     (final_layernorm): RMSNorm()
54:   )
54:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
46:         )
46:         (pre_cross_attn_layernorm): IdentityOp()
46:         (cross_attention): IdentityOp()
46:         (cross_attn_bda): IdentityFuncOp()
46:         (pre_mlp_layernorm): IdentityOp()
46:         (mlp): TEFusedMLP(
46:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
46:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
46:         )
46:       )
46:     )
46:     (final_layernorm): RMSNorm()
46:   )
46:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
46: )
 0: 
 0: MCore config:
30: GPTModel(
30:   (embedding): LanguageModelEmbedding(
30:     (word_embeddings): VocabParallelEmbedding()
30:     (embedding_dropout): Dropout(p=0.0, inplace=False)
30:   )
30:   (rotary_pos_emb): RotaryEmbedding()
30:   (decoder): TransformerBlock(
30:     (layers): ModuleList(
30:       (0-31): 32 x TransformerLayer(
30:         (input_layernorm): IdentityOp()
30:         (self_attention): SelfAttention(
30:           (core_attention): TEDotProductAttention(
30:             (flash_attention): FlashAttention()
30:             (fused_attention): FusedAttention()
30:             (unfused_attention): UnfusedDotProductAttention(
30:               (scale_mask_softmax): FusedScaleMaskSoftmax()
30:               (attention_dropout): Dropout(p=0.0, inplace=False)
30:             )
30:           )
30:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
30:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
30:           (q_layernorm): IdentityOp()
30:           (k_layernorm): IdentityOp()
63: GPTModel(
63:   (embedding): LanguageModelEmbedding(
63:     (word_embeddings): VocabParallelEmbedding()
63:     (embedding_dropout): Dropout(p=0.0, inplace=False)
63:   )
63:   (rotary_pos_emb): RotaryEmbedding()
63:   (decoder): TransformerBlock(
63:     (layers): ModuleList(
63:       (0-31): 32 x TransformerLayer(
63:         (input_layernorm): IdentityOp()
63:         (self_attention): SelfAttention(
63:           (core_attention): TEDotProductAttention(
63:             (flash_attention): FlashAttention()
63:             (fused_attention): FusedAttention()
63:             (unfused_attention): UnfusedDotProductAttention(
63:               (scale_mask_softmax): FusedScaleMaskSoftmax()
63:               (attention_dropout): Dropout(p=0.0, inplace=False)
63:             )
63:           )
63:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
63:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
63:           (q_layernorm): IdentityOp()
63:           (k_layernorm): IdentityOp()
22: )
54: )
30:         )
30:         (pre_cross_attn_layernorm): IdentityOp()
30:         (cross_attention): IdentityOp()
30:         (cross_attn_bda): IdentityFuncOp()
30:         (pre_mlp_layernorm): IdentityOp()
30:         (mlp): TEFusedMLP(
30:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
30:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
30:         )
30:       )
30:     )
30:     (final_layernorm): RMSNorm()
30:   )
30:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
63:         )
63:         (pre_cross_attn_layernorm): IdentityOp()
63:         (cross_attention): IdentityOp()
63:         (cross_attn_bda): IdentityFuncOp()
63:         (pre_mlp_layernorm): IdentityOp()
63:         (mlp): TEFusedMLP(
63:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
63:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
63:         )
63:       )
63:     )
63:     (final_layernorm): RMSNorm()
63:   )
63:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
30: )
63: )
31: )
 2: GPTModel(
 2:   (embedding): LanguageModelEmbedding(
 2:     (word_embeddings): VocabParallelEmbedding()
 2:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 2:   )
 2:   (rotary_pos_emb): RotaryEmbedding()
 2:   (decoder): TransformerBlock(
 2:     (layers): ModuleList(
 2:       (0-31): 32 x TransformerLayer(
 2:         (input_layernorm): IdentityOp()
 2:         (self_attention): SelfAttention(
 2:           (core_attention): TEDotProductAttention(
 2:             (flash_attention): FlashAttention()
 2:             (fused_attention): FusedAttention()
 2:             (unfused_attention): UnfusedDotProductAttention(
 2:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 2:               (attention_dropout): Dropout(p=0.0, inplace=False)
 2:             )
 2:           )
 2:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 2:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 2:           (q_layernorm): IdentityOp()
 2:           (k_layernorm): IdentityOp()
 2:         )
 2:         (pre_cross_attn_layernorm): IdentityOp()
 2:         (cross_attention): IdentityOp()
 2:         (cross_attn_bda): IdentityFuncOp()
 2:         (pre_mlp_layernorm): IdentityOp()
 2:         (mlp): TEFusedMLP(
 2:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 2:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 2:         )
 2:       )
 2:     )
 2:     (final_layernorm): RMSNorm()
 2:   )
 2:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 2: )
 6: GPTModel(
 6:   (embedding): LanguageModelEmbedding(
 6:     (word_embeddings): VocabParallelEmbedding()
 6:     (embedding_dropout): Dropout(p=0.0, inplace=False)
 6:   )
 6:   (rotary_pos_emb): RotaryEmbedding()
 6:   (decoder): TransformerBlock(
 6:     (layers): ModuleList(
 6:       (0-31): 32 x TransformerLayer(
 6:         (input_layernorm): IdentityOp()
 6:         (self_attention): SelfAttention(
 6:           (core_attention): TEDotProductAttention(
 6:             (flash_attention): FlashAttention()
 6:             (fused_attention): FusedAttention()
 6:             (unfused_attention): UnfusedDotProductAttention(
 6:               (scale_mask_softmax): FusedScaleMaskSoftmax()
 6:               (attention_dropout): Dropout(p=0.0, inplace=False)
 6:             )
 6:           )
 6:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 6:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 6:           (q_layernorm): IdentityOp()
 6:           (k_layernorm): IdentityOp()
 6:         )
 6:         (pre_cross_attn_layernorm): IdentityOp()
 6:         (cross_attention): IdentityOp()
 6:         (cross_attn_bda): IdentityFuncOp()
 6:         (pre_mlp_layernorm): IdentityOp()
 6:         (mlp): TEFusedMLP(
 6:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 6:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 6:         )
 6:       )
 6:     )
 6:     (final_layernorm): RMSNorm()
 6:   )
 6:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 6: )
37: GPTModel(
37:   (embedding): LanguageModelEmbedding(
37:     (word_embeddings): VocabParallelEmbedding()
37:     (embedding_dropout): Dropout(p=0.0, inplace=False)
37:   )
37:   (rotary_pos_emb): RotaryEmbedding()
37:   (decoder): TransformerBlock(
37:     (layers): ModuleList(
37:       (0-31): 32 x TransformerLayer(
37:         (input_layernorm): IdentityOp()
37:         (self_attention): SelfAttention(
37:           (core_attention): TEDotProductAttention(
37:             (flash_attention): FlashAttention()
37:             (fused_attention): FusedAttention()
37:             (unfused_attention): UnfusedDotProductAttention(
37:               (scale_mask_softmax): FusedScaleMaskSoftmax()
37:               (attention_dropout): Dropout(p=0.0, inplace=False)
37:             )
37:           )
37:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
37:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
37:           (q_layernorm): IdentityOp()
37:           (k_layernorm): IdentityOp()
37:         )
37:         (pre_cross_attn_layernorm): IdentityOp()
37:         (cross_attention): IdentityOp()
37:         (cross_attn_bda): IdentityFuncOp()
37:         (pre_mlp_layernorm): IdentityOp()
37:         (mlp): TEFusedMLP(
37:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
37:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
37:         )
37:       )
37:     )
37:     (final_layernorm): RMSNorm()
37:   )
37:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
37: )
38: GPTModel(
38:   (embedding): LanguageModelEmbedding(
38:     (word_embeddings): VocabParallelEmbedding()
38:     (embedding_dropout): Dropout(p=0.0, inplace=False)
38:   )
38:   (rotary_pos_emb): RotaryEmbedding()
38:   (decoder): TransformerBlock(
38:     (layers): ModuleList(
38:       (0-31): 32 x TransformerLayer(
38:         (input_layernorm): IdentityOp()
38:         (self_attention): SelfAttention(
38:           (core_attention): TEDotProductAttention(
38:             (flash_attention): FlashAttention()
38:             (fused_attention): FusedAttention()
38:             (unfused_attention): UnfusedDotProductAttention(
38:               (scale_mask_softmax): FusedScaleMaskSoftmax()
38:               (attention_dropout): Dropout(p=0.0, inplace=False)
38:             )
38:           )
38:           (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
38:           (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
38:           (q_layernorm): IdentityOp()
38:           (k_layernorm): IdentityOp()
38:         )
38:         (pre_cross_attn_layernorm): IdentityOp()
38:         (cross_attention): IdentityOp()
38:         (cross_attn_bda): IdentityFuncOp()
38:         (pre_mlp_layernorm): IdentityOp()
38:         (mlp): TEFusedMLP(
38:           (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
38:           (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
38:         )
38:       )
38:     )
38:     (final_layernorm): RMSNorm()
38:   )
38:   (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
38: )
 0: Llama31Config8B(tensor_model_parallel_size=1,
 0:                 pipeline_model_parallel_comm_backend=None,
 0:                 pipeline_model_parallel_size=1,
 0:                 virtual_pipeline_model_parallel_size=None,
 0:                 sequence_parallel=False,
 0:                 context_parallel_size=2,
 0:                 hierarchical_context_parallel_sizes=None,
 0:                 expert_model_parallel_size=1,
 0:                 expert_tensor_parallel_size=2,
 0:                 moe_extended_tp=False,
 0:                 perform_initialization=True,
 0:                 use_cpu_initialization=False,
 0:                 fp16=False,
 0:                 bf16=True,
 0:                 params_dtype=torch.bfloat16,
 0:                 timers=<megatron.core.timers.Timers object at 0x770205d6e930>,
 0:                 finalize_model_grads_func=<function MegatronOptimizerModule.on_fit_start.<locals>.finalize_model_grads_func at 0x7701f41de340>,
 0:                 grad_scale_func=None,
 0:                 no_sync_func=<bound method DistributedDataParallel.no_sync of DDP(
 0:   (module): Float16Module(
 0:     (module): GPTModel(
 0:       (embedding): LanguageModelEmbedding(
 0:         (word_embeddings): VocabParallelEmbedding()
 0:         (embedding_dropout): Dropout(p=0.0, inplace=False)
 0:       )
 0:       (rotary_pos_emb): RotaryEmbedding()
 0:       (decoder): TransformerBlock(
 0:         (layers): ModuleList(
 0:           (0-31): 32 x TransformerLayer(
 0:             (input_layernorm): IdentityOp()
 0:             (self_attention): SelfAttention(
 0:               (core_attention): TEDotProductAttention(
 0:                 (flash_attention): FlashAttention()
 0:                 (fused_attention): FusedAttention()
 0:                 (unfused_attention): UnfusedDotProductAttention(
 0:                   (scale_mask_softmax): FusedScaleMaskSoftmax()
 0:                   (attention_dropout): Dropout(p=0.0, inplace=False)
 0:                 )
 0:               )
 0:               (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
 0:               (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
 0:               (q_layernorm): IdentityOp()
 0:               (k_layernorm): IdentityOp()
 0:             )
 0:             (pre_cross_attn_layernorm): IdentityOp()
 0:             (cross_attention): IdentityOp()
 0:             (cross_attn_bda): IdentityFuncOp()
 0:             (pre_mlp_layernorm): IdentityOp()
 0:             (mlp): TEFusedMLP(
 0:               (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
 0:               (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
 0:             )
 0:           )
 0:         )
 0:         (final_layernorm): RMSNorm()
 0:       )
 0:       (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
 0:     )
 0:   )
 0: )>,
 0:                 grad_sync_func=None,
 0:                 param_sync_func=None,
 0:                 deterministic_mode=False,
 0:                 enable_autocast=False,
 0:                 autocast_dtype=torch.bfloat16,
 0:                 num_microbatches_with_partial_activation_checkpoints=None,
 0:                 gradient_accumulation_fusion=True,
 0:                 async_tensor_model_parallel_allreduce=False,
 0:                 use_te_rng_tracker=(True,),
 0:                 tp_comm_overlap=False,
 0:                 tp_comm_bulk_wgrad=True,
 0:                 tp_comm_bulk_dgrad=True,
 0:                 tp_comm_overlap_ag=True,
 0:                 tp_comm_overlap_rs=True,
 0:                 tp_comm_overlap_rs_dgrad=False,
 0:                 tp_comm_split_ag=True,
 0:                 tp_comm_atomic_ag=False,
 0:                 tp_comm_split_rs=True,
 0:                 tp_comm_atomic_rs=False,
 0:                 cross_entropy_loss_fusion=True,
 0:                 cross_entropy_fusion_impl='te',
 0:                 tp_comm_overlap_disable_qkv=False,
 0:                 tp_comm_overlap_disable_fc1=False,
 0:                 tp_comm_bootstrap_backend=None,
 0:                 overlap_moe_expert_parallel_comm=False,
 0:                 delay_wgrad_compute=False,
 0:                 pipeline_dtype=torch.bfloat16,
 0:                 variable_seq_lengths=False,
 0:                 overlap_p2p_comm=True,
 0:                 batch_p2p_comm=False,
 0:                 batch_p2p_sync=True,
 0:                 use_ring_exchange_p2p=False,
 0:                 deallocate_pipeline_outputs=True,
 0:                 defer_embedding_wgrad_compute=False,
 0:                 wgrad_deferral_limit=50,
 0:                 overlap_p2p_comm_warmup_flush=False,
 0:                 microbatch_group_size_per_vp_stage=1,
 0:                 cpu_offloading=False,
 0:                 cpu_offloading_num_layers=0,
 0:                 _cpu_offloading_context=None,
 0:                 cpu_offloading_activations=True,
 0:                 cpu_offloading_weights=False,
 0:                 cpu_offloading_double_buffering=False,
 0:                 barrier_with_L1_time=True,
 0:                 num_layers=32,
 0:                 mtp_num_layers=None,
 0:                 mtp_loss_scaling_factor=None,
 0:                 num_layers_in_first_pipeline_stage=None,
 0:                 num_layers_in_last_pipeline_stage=None,
 0:                 pipeline_model_parallel_layout=None,
 0:                 account_for_embedding_in_pipeline_split=False,
 0:                 account_for_loss_in_pipeline_split=False,
 0:                 hidden_size=4096,
 0:                 num_attention_heads=32,
 0:                 attention_backend=<AttnBackend.auto: 5>,
 0:                 softmax_scale=None,
 0:                 softmax_type='vanilla',
 0:                 num_query_groups=8,
 0:                 ffn_hidden_size=14336,
 0:                 kv_channels=128,
 0:                 hidden_dropout=0.0,
 0:                 attention_dropout=0.0,
 0:                 fp32_residual_connection=False,
 0:                 apply_residual_connection_post_layernorm=False,
 0:                 layernorm_epsilon=1e-05,
 0:                 layernorm_zero_centered_gamma=False,
 0:                 add_bias_linear=False,
 0:                 add_qkv_bias=False,
 0:                 gated_linear_unit=True,
 0:                 activation_func=<function silu at 0x77050c4b2520>,
 0:                 activation_func_fp8_input_store=False,
 0:                 glu_linear_offset=0.0,
 0:                 activation_func_clamp_value=None,
 0:                 num_moe_experts=None,
 0:                 rotary_interleaved=False,
 0:                 window_size=None,
 0:                 window_attn_skip_freq=None,
 0:                 normalization='RMSNorm',
 0:                 qk_layernorm=False,
 0:                 test_mode=False,
 0:                 calculate_per_token_loss=False,
 0:                 multi_latent_attention=False,
 0:                 no_rope_freq=None,
 0:                 moe_deepep_num_sms=20,
 0:                 init_method=functools.partial(<function normal_ at 0x77050c32ae80>, mean=0.0, std=0.02),
 0:                 output_layer_init_method=functools.partial(<function normal_ at 0x77050c32ae80>, mean=0.0, std=0.0025),
 0:                 init_method_std=0.02,
 0:                 embedding_init_method=functools.partial(<function normal_ at 0x77050c32ae80>, mean=0.0, std=0.02),
 0:                 embedding_init_method_std=0.02,
 0:                 init_model_with_meta_device=False,
 0:                 apply_query_key_layer_scaling=False,
 0:                 attention_softmax_in_fp32=False,
 0:                 disable_bf16_reduced_precision_matmul=False,
 0:                 bias_activation_fusion=True,
 0:                 masked_softmax_fusion=True,
 0:                 persist_layer_norm=True,
 0:                 memory_efficient_layer_norm=False,
 0:                 bias_dropout_fusion=True,
 0:                 apply_rope_fusion=True,
 0:                 use_fused_weighted_squared_relu=False,
 0:                 fused_single_qkv_rope=True,
 0:                 recompute_granularity=None,
 0:                 recompute_method=None,
 0:                 recompute_num_layers=None,
 0:                 distribute_saved_activations=None,
 0:                 recompute_modules=['core_attn'],
 0:                 fp8='hybrid',
 0:                 fp8_recipe='tensorwise',
 0:                 fp8_param=True,
 0:                 fp8_margin=0,
 0:                 fp8_interval=1,
 0:                 fp8_amax_history_len=1,
 0:                 fp8_amax_compute_algo='most_recent',
 0:                 fp8_wgrad=True,
 0:                 fp8_dot_product_attention=False,
 0:                 fp8_multi_head_attention=False,
 0:                 tp_only_amax_red=True,
 0:                 first_last_layers_bf16=False,
 0:                 num_layers_at_start_in_bf16=0,
 0:                 num_layers_at_end_in_bf16=0,
 0:                 use_kitchen=False,
 0:                 fp4=None,
 0:                 fp4_recipe='nvfp4',
 0:                 fp4_param=False,
 0:                 moe_shared_expert_intermediate_size=None,
 0:                 moe_shared_expert_overlap=False,
 0:                 moe_layer_freq=1,
 0:                 moe_ffn_hidden_size=None,
 0:                 moe_router_load_balancing_type='aux_loss',
 0:                 moe_router_topk=2,
 0:                 moe_router_topk_limited_devices=None,
 0:                 moe_router_padding_for_fp8=False,
 0:                 moe_router_num_groups=None,
 0:                 moe_router_group_topk=None,
 0:                 moe_router_pre_softmax=False,
 0:                 moe_router_topk_scaling_factor=None,
 0:                 moe_router_score_function='softmax',
 0:                 moe_router_dtype=None,
 0:                 moe_router_enable_expert_bias=False,
 0:                 moe_router_bias_update_rate=0.001,
 0:                 moe_router_force_load_balancing=False,
 0:                 moe_grouped_gemm=False,
 0:                 moe_use_legacy_grouped_gemm=False,
 0:                 moe_aux_loss_coeff=0.0,
 0:                 moe_z_loss_coeff=None,
 0:                 moe_input_jitter_eps=None,
 0:                 moe_token_dropping=False,
 0:                 moe_token_dispatcher_type='allgather',
 0:                 moe_enable_deepep=False,
 0:                 moe_per_layer_logging=False,
 0:                 moe_expert_capacity_factor=None,
 0:                 moe_pad_expert_input_to_capacity=False,
 0:                 moe_token_drop_policy='probs',
 0:                 moe_layer_recompute=False,
 0:                 moe_permute_fusion=False,
 0:                 moe_router_fusion=False,
 0:                 moe_apply_probs_on_input=False,
 0:                 cp_comm_type=None,
 0:                 enable_cuda_graph=1,
 0:                 cuda_graph_use_single_mempool=False,
 0:                 cuda_graph_retain_backward_graph=False,
 0:                 cuda_graph_warmup_steps=3,
 0:                 external_cuda_graph=False,
 0:                 cuda_graph_scope='full_iteration',
 0:                 clone_scatter_output_in_embedding=True,
 0:                 disable_parameter_transpose_cache=False,
 0:                 config_logger_dir='',
 0:                 flash_decode=False,
 0:                 use_te_activation_func=False,
 0:                 inference_rng_tracker=False,
 0:                 inference_sampling_seed=42,
 0:                 symmetric_ar_type=None,
 0:                 mrope_section=None,
 0:                 is_hybrid_model=False,
 0:                 mamba_state_dim=128,
 0:                 mamba_head_dim=64,
 0:                 mamba_num_groups=8,
 0:                 mamba_num_heads=None,
 0:                 use_mamba_mem_eff_path=True,
 0:                 mlp_chunks_for_prefill=1,
 0:                 heterogeneous_block_specs=False,
 0:                 hetereogenous_dist_checkpoint=False,
 0:                 quant_recipe=None,
 0:                 transformer_impl='transformer_engine',
 0:                 fp16_lm_cross_entropy=False,
 0:                 parallel_output=True,
 0:                 share_embeddings_and_output_weights=False,
 0:                 make_vocab_size_divisible_by=128,
 0:                 position_embedding_type='rope',
 0:                 rotary_base=500000,
 0:                 rotary_percent=1.0,
 0:                 seq_len_interpolation_factor=None,
 0:                 seq_length=8192,
 0:                 scatter_embedding_sequence_parallel=True,
 0:                 use_transformer_engine_full_layer_spec=False,
 0:                 transformer_layer_spec=<function default_layer_spec at 0x770206a1a700>,
 0:                 forward_step_fn=<function gpt_forward_step at 0x770206a19120>,
 0:                 data_step_fn=<function gpt_data_step at 0x770206a18cc0>,
 0:                 generation_config=None,
 0:                 vocab_size=None,
 0:                 tp_comm_overlap_cfg=None,
 0:                 use_transformer_engine_op_fuser=True,
 0:                 scale_factor=8.0,
 0:                 low_freq_factor=1.0,
 0:                 high_freq_factor=4.0,
 0:                 old_context_len=8192)
 0: [AUX I 2025-10-09 23:11:07 data:291] Instantiating MegatronPretrainingSampler with total_samples: 10000000 and consumed_samples: 0
 7: [rank7]:[W1009 23:11:07.359518112 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 188 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 7: NCCL version 2.27.7+cuda13.0
56: [rank56]:[W1009 23:11:07.664858344 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 335 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
63: [rank63]:[W1009 23:11:07.664911543 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 356 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
56: NCCL version 2.27.7+cuda13.0
63: NCCL version 2.27.7+cuda13.0
62: [rank62]:[W1009 23:11:07.665547697 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 353 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
62: NCCL version 2.27.7+cuda13.0
57: [rank57]:[W1009 23:11:07.666223047 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 338 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
57: NCCL version 2.27.7+cuda13.0
59: [rank59]:[W1009 23:11:07.667956821 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 344 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
61: [rank61]:[W1009 23:11:07.668305968 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 350 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
59: NCCL version 2.27.7+cuda13.0
39: [rank39]:[W1009 23:11:07.563342926 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 284 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
61: NCCL version 2.27.7+cuda13.0
39: NCCL version 2.27.7+cuda13.0
58: [rank58]:[W1009 23:11:07.669953499 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 341 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
60: [rank60]:[W1009 23:11:07.670067779 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 347 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
58: NCCL version 2.27.7+cuda13.0
60: NCCL version 2.27.7+cuda13.0
40: [rank40]:[W1009 23:11:07.927284604 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 287 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
55: [rank55]:[W1009 23:11:07.939059146 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 332 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
40: NCCL version 2.27.7+cuda13.0
55: NCCL version 2.27.7+cuda13.0
52: [rank52]:[W1009 23:11:07.940087595 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 323 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
52: NCCL version 2.27.7+cuda13.0
29: [rank29]:[W1009 23:11:07.144288159 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 254 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 4: [rank4]:[W1009 23:11:07.375294357 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 179 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
29: NCCL version 2.27.7+cuda13.0
 4: NCCL version 2.27.7+cuda13.0
36: [rank36]:[W1009 23:11:07.568477410 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 275 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
25: [rank25]:[W1009 23:11:07.145405545 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 242 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
23: [rank23]:[W1009 23:11:07.623671399 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 236 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
30: [rank30]:[W1009 23:11:07.145580080 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 257 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
25: NCCL version 2.27.7+cuda13.0
36: NCCL version 2.27.7+cuda13.0
23: NCCL version 2.27.7+cuda13.0
30: NCCL version 2.27.7+cuda13.0
31: [rank31]:[W1009 23:11:07.145956821 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 260 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
20: [rank20]:[W1009 23:11:07.624501524 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 227 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
31: NCCL version 2.27.7+cuda13.0
26: [rank26]:[W1009 23:11:07.146385680 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 245 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
10: [rank10]:[W1009 23:11:07.047234948 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 197 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 5: [rank5]:[W1009 23:11:07.377128755 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 182 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
35: [rank35]:[W1009 23:11:07.569568977 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 272 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 6: [rank6]:[W1009 23:11:07.377292571 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 185 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
20: NCCL version 2.27.7+cuda13.0
27: [rank27]:[W1009 23:11:07.146697139 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 248 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
26: NCCL version 2.27.7+cuda13.0
10: NCCL version 2.27.7+cuda13.0
 5: NCCL version 2.27.7+cuda13.0
35: NCCL version 2.27.7+cuda13.0
11: [rank11]:[W1009 23:11:07.047845598 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 200 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
27: NCCL version 2.27.7+cuda13.0
 6: NCCL version 2.27.7+cuda13.0
11: NCCL version 2.27.7+cuda13.0
12: [rank12]:[W1009 23:11:07.048170749 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 203 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
14: [rank14]:[W1009 23:11:07.048306841 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 209 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
21: [rank21]:[W1009 23:11:07.625904320 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 230 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
12: NCCL version 2.27.7+cuda13.0
14: NCCL version 2.27.7+cuda13.0
24: [rank24]:[W1009 23:11:07.147894778 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 239 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
24: NCCL version 2.27.7+cuda13.0
21: NCCL version 2.27.7+cuda13.0
44: [rank44]:[W1009 23:11:07.933401711 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 299 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
45: [rank45]:[W1009 23:11:07.933429957 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 302 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
22: [rank22]:[W1009 23:11:07.627070721 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 233 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
44: NCCL version 2.27.7+cuda13.0
45: NCCL version 2.27.7+cuda13.0
34: [rank34]:[W1009 23:11:07.572028724 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 269 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
33: [rank33]:[W1009 23:11:07.572110063 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 266 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
15: [rank15]:[W1009 23:11:07.049932300 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 212 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
53: [rank53]:[W1009 23:11:07.945434694 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 326 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
13: [rank13]:[W1009 23:11:07.050091796 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 206 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
34: NCCL version 2.27.7+cuda13.0
33: NCCL version 2.27.7+cuda13.0
22: NCCL version 2.27.7+cuda13.0
53: NCCL version 2.27.7+cuda13.0
15: NCCL version 2.27.7+cuda13.0
13: NCCL version 2.27.7+cuda13.0
28: [rank28]:[W1009 23:11:07.150569682 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 251 Rank 0]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
19: [rank19]:[W1009 23:11:07.628831388 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 224 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
41: [rank41]:[W1009 23:11:07.935617220 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 290 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
28: NCCL version 2.27.7+cuda13.0
19: NCCL version 2.27.7+cuda13.0
41: NCCL version 2.27.7+cuda13.0
 8: [rank8]:[W1009 23:11:07.052379484 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 191 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
47: [rank47]:[W1009 23:11:07.936380061 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 308 Rank 0]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 8: NCCL version 2.27.7+cuda13.0
 1: [rank1]:[W1009 23:11:07.382621493 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 170 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
47: NCCL version 2.27.7+cuda13.0
54: [rank54]:[W1009 23:11:07.948407469 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 329 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 1: NCCL version 2.27.7+cuda13.0
54: NCCL version 2.27.7+cuda13.0
32: [rank32]:[W1009 23:11:07.575773686 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 263 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
32: NCCL version 2.27.7+cuda13.0
46: [rank46]:[W1009 23:11:07.938115127 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 305 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 9: [rank9]:[W1009 23:11:07.054342780 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 194 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
16: [rank16]:[W1009 23:11:07.631653940 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 215 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
17: [rank17]:[W1009 23:11:07.631842299 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 218 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
16: NCCL version 2.27.7+cuda13.0
46: NCCL version 2.27.7+cuda13.0
 9: NCCL version 2.27.7+cuda13.0
42: [rank42]:[W1009 23:11:07.938626942 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 293 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
43: [rank43]:[W1009 23:11:07.938641404 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 296 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
17: NCCL version 2.27.7+cuda13.0
51: [rank51]:[W1009 23:11:07.950511507 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 320 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
43: NCCL version 2.27.7+cuda13.0
42: NCCL version 2.27.7+cuda13.0
51: NCCL version 2.27.7+cuda13.0
 3: [rank3]:[W1009 23:11:07.385658245 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 176 Rank 0]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 3: NCCL version 2.27.7+cuda13.0
49: [rank49]:[W1009 23:11:07.952245345 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 314 Rank 0]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
49: NCCL version 2.27.7+cuda13.0
18: [rank18]:[W1009 23:11:07.634907270 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 221 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
18: NCCL version 2.27.7+cuda13.0
48: [rank48]:[W1009 23:11:07.956114002 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 311 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
48: NCCL version 2.27.7+cuda13.0
50: [rank50]:[W1009 23:11:07.957854994 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 317 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
50: NCCL version 2.27.7+cuda13.0
 2: [rank2]:[W1009 23:11:07.394571746 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 173 Rank 0]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
 2: NCCL version 2.27.7+cuda13.0
 0: [rank0]:[W1009 23:11:07.405680801 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 167 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
37: [rank37]:[W1009 23:11:07.644572222 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 278 Rank 0]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
38: [rank38]:[W1009 23:11:07.644582036 ProcessGroupNCCL.cpp:5084] [PG ID 6 PG GUID 281 Rank 0]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
37: NCCL version 2.27.7+cuda13.0
38: NCCL version 2.27.7+cuda13.0
 0: [AUX I 2025-10-09 23:11:08 custom_callbacks:129] Starting training warmup
 0: [AUX I 2025-10-09 23:11:08 custom_callbacks:133]     Starting warmup step 0
23: [rank23]:[W1009 23:11:12.126259913 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
21: [rank21]:[W1009 23:11:12.128743019 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
19: [rank19]:[W1009 23:11:12.129701478 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
22: [rank22]:[W1009 23:11:12.152754155 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
16: [rank16]:[W1009 23:11:12.157765366 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
20: [rank20]:[W1009 23:11:12.176131948 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
18: [rank18]:[W1009 23:11:12.182273101 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
17: [rank17]:[W1009 23:11:12.207293982 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 7: [rank7]:[W1009 23:11:12.071686935 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 4: [rank4]:[W1009 23:11:12.078802609 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 5: [rank5]:[W1009 23:11:12.079649421 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
31: [rank31]:[W1009 23:11:12.850027363 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
30: [rank30]:[W1009 23:11:12.850255990 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
28: [rank28]:[W1009 23:11:12.851873514 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 6: [rank6]:[W1009 23:11:12.084028158 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 1: [rank1]:[W1009 23:11:12.087248447 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 3: [rank3]:[W1009 23:11:12.087915654 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 2: [rank2]:[W1009 23:11:12.092869191 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 0: [rank0]:[W1009 23:11:12.123998853 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 9: [rank9]:[W1009 23:11:12.828153008 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
32: [rank32]:[W1009 23:11:12.350762081 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
37: [rank37]:[W1009 23:11:12.359931761 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
38: [rank38]:[W1009 23:11:12.359958695 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
39: [rank39]:[W1009 23:11:12.367179575 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
10: [rank10]:[W1009 23:11:12.846565197 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
35: [rank35]:[W1009 23:11:12.368958751 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
27: [rank27]:[W1009 23:11:12.945928953 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
36: [rank36]:[W1009 23:11:12.370553905 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
29: [rank29]:[W1009 23:11:12.947741755 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
34: [rank34]:[W1009 23:11:12.371045846 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
33: [rank33]:[W1009 23:11:12.371256699 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
14: [rank14]:[W1009 23:11:12.849542687 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
12: [rank12]:[W1009 23:11:12.850058619 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 8: [rank8]:[W1009 23:11:12.850130534 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
13: [rank13]:[W1009 23:11:12.850178756 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
15: [rank15]:[W1009 23:11:12.850339222 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
25: [rank25]:[W1009 23:11:12.950451755 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
26: [rank26]:[W1009 23:11:12.950813006 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
11: [rank11]:[W1009 23:11:12.852848285 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
24: [rank24]:[W1009 23:11:12.952019296 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
46: [rank46]:[W1009 23:11:12.812524076 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
53: [rank53]:[W1009 23:11:12.844821544 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
55: [rank55]:[W1009 23:11:12.845177205 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
47: [rank47]:[W1009 23:11:12.833957223 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
52: [rank52]:[W1009 23:11:12.845623998 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
54: [rank54]:[W1009 23:11:12.847728372 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
42: [rank42]:[W1009 23:11:12.836963103 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
43: [rank43]:[W1009 23:11:12.837260945 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
51: [rank51]:[W1009 23:11:12.925629169 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
49: [rank49]:[W1009 23:11:12.927430573 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
48: [rank48]:[W1009 23:11:12.930497830 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
50: [rank50]:[W1009 23:11:12.930699613 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
63: [rank63]:[W1009 23:11:12.671894706 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
61: [rank61]:[W1009 23:11:12.672146693 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
62: [rank62]:[W1009 23:11:12.672237230 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
56: [rank56]:[W1009 23:11:12.672255552 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
60: [rank60]:[W1009 23:11:12.672451024 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
59: [rank59]:[W1009 23:11:12.672556810 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
57: [rank57]:[W1009 23:11:12.672817462 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
58: [rank58]:[W1009 23:11:12.672815982 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
44: [rank44]:[W1009 23:11:12.932821780 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
45: [rank45]:[W1009 23:11:12.932900999 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
40: [rank40]:[W1009 23:11:12.934255301 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
41: [rank41]:[W1009 23:11:12.934646675 ProcessGroupNCCL.cpp:571] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 is experimental for point-to-point collectives. To ensure safety, .wait() must be called on all returned handles before they fall out of scope, including for isend() calls. (function operator())
 0: [AUX I 2025-10-09 23:11:23 custom_callbacks:150]     Finished warmup step 0, takes 14.6521737575531 s
 0: [AUX I 2025-10-09 23:11:23 custom_callbacks:133]     Starting warmup step 1
 0: [NeMo I 2025-10-09 23:11:23 full_cuda_graph:164] Capture CUDA graph for training!!!
 0: [NeMo I 2025-10-09 23:11:23 full_cuda_graph:182] CUDA graph capture done!!!
 0: [AUX I 2025-10-09 23:11:24 custom_callbacks:150]     Finished warmup step 1, takes 0.639976978302002 s
 0: [AUX I 2025-10-09 23:11:24 custom_callbacks:157] Finished training warmup: 15.297500848770142 s. 
 0: [AUX I 2025-10-09 23:11:24 custom_callbacks:176] Starting validation warmups
 0: [NeMo I 2025-10-09 23:11:24 full_cuda_graph:164] Capture CUDA graph for validation!!!
 0: [NeMo I 2025-10-09 23:11:24 full_cuda_graph:182] CUDA graph capture done!!!
 0: [AUX I 2025-10-09 23:11:24 custom_callbacks:195] Finished validation warmup: 0.6258659362792969 s. 
 0: [AUX I 2025-10-09 23:11:24 custom_callbacks:202] Finished training warmup: 0.627021074295044 s. 
 0: [AUX I 2025-10-09 23:11:24 custom_callbacks:212] Time spent in run_training_warmup: 0.630786657333374s
 0: :::MLLOG {"namespace": "", "time_ms": 1760051484798, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"warmup_time": 77.87777555303182}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051484856, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"init_finished": 0.05897117801941931}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051484857, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051484858, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 83}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051484859, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051490742, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.17969179153442383, "reduced_train_loss": 8.380121231079102}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 31.0, "samples_count": 1024.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051496622, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18599343299865723, "reduced_train_loss": 7.530817985534668}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 63.0, "samples_count": 2048.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051502578, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1863877773284912, "reduced_train_loss": 7.083868980407715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 95.0, "samples_count": 3072.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051508554, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1847221851348877, "reduced_train_loss": 6.830201625823975}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 127.0, "samples_count": 4096.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051514557, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19189739227294922, "reduced_train_loss": 6.770097255706787}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 159.0, "samples_count": 5120.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051520558, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1910266876220703, "reduced_train_loss": 6.678834438323975}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 191.0, "samples_count": 6144.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051526580, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18935751914978027, "reduced_train_loss": 6.468847751617432}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 223.0, "samples_count": 7168.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051532619, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18732714653015137, "reduced_train_loss": 6.382495880126953}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 255.0, "samples_count": 8192.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051538665, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1903994083404541, "reduced_train_loss": 6.147599220275879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 287.0, "samples_count": 9216.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051544714, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19244623184204102, "reduced_train_loss": 5.971988677978516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 319.0, "samples_count": 10240.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051550773, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19024085998535156, "reduced_train_loss": 5.870059490203857}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 351.0, "samples_count": 11264.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051556844, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1887950897216797, "reduced_train_loss": 5.756568431854248}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 383.0, "samples_count": 12288.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557069, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1880536124452495}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557070, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557070, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 12288, "step": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.39632272720336914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 0, "samples_count": 32}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04602313041687012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1, "samples_count": 64}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557332, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04596424102783203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2, "samples_count": 96}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046393394470214844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3, "samples_count": 128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557424, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.045725345611572266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4, "samples_count": 160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557471, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04713797569274902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5, "samples_count": 192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557518, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04705500602722168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 6, "samples_count": 224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557565, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04678463935852051}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 7, "samples_count": 256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557611, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04620718955993652}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 8, "samples_count": 288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557658, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04705095291137695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 9, "samples_count": 320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557707, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0494844913482666}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 10, "samples_count": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557758, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05104970932006836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 11, "samples_count": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557807, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048848628997802734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 12, "samples_count": 416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04796123504638672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 13, "samples_count": 448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557905, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04981350898742676}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 14, "samples_count": 480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051557954, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04862618446350098}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 15, "samples_count": 512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558002, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04835104942321777}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 16, "samples_count": 544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04942941665649414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 17, "samples_count": 576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558100, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048021554946899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 18, "samples_count": 608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04884505271911621}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 19, "samples_count": 640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048532962799072266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 20, "samples_count": 672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558245, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047702789306640625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 21, "samples_count": 704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558293, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04806852340698242}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 22, "samples_count": 736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558341, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04871082305908203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 23, "samples_count": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558391, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049820661544799805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 24, "samples_count": 800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04822969436645508}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 25, "samples_count": 832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558487, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047286272048950195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 26, "samples_count": 864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558536, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049097299575805664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 27, "samples_count": 896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04984307289123535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 28, "samples_count": 928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558633, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047475337982177734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 29, "samples_count": 960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558683, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04952216148376465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 30, "samples_count": 992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558734, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.051229238510131836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 31, "samples_count": 1024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558737, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 5.67690896987915, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558737, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.6683209600159898}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558737, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 12288, "step": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051558738, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051564834, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19281363487243652, "reduced_train_loss": 5.497893333435059}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 415.0, "samples_count": 13312.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051570924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1945638656616211, "reduced_train_loss": 5.443127155303955}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 447.0, "samples_count": 14336.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051577042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19249296188354492, "reduced_train_loss": 5.324615478515625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 479.0, "samples_count": 15360.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051583176, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19498467445373535, "reduced_train_loss": 5.193290710449219}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 511.0, "samples_count": 16384.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051589305, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19378900527954102, "reduced_train_loss": 5.0795111656188965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 543.0, "samples_count": 17408.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051595409, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1933581829071045, "reduced_train_loss": 4.913524150848389}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 575.0, "samples_count": 18432.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051601557, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1942148208618164, "reduced_train_loss": 4.893833637237549}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 607.0, "samples_count": 19456.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051607659, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18793487548828125, "reduced_train_loss": 4.714492321014404}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 639.0, "samples_count": 20480.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051613756, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19144153594970703, "reduced_train_loss": 4.658041477203369}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 671.0, "samples_count": 21504.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051619855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19100499153137207, "reduced_train_loss": 4.553436279296875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 703.0, "samples_count": 22528.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051625957, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19167208671569824, "reduced_train_loss": 4.940775394439697}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 735.0, "samples_count": 23552.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632067, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1908855438232422, "reduced_train_loss": 4.598836898803711}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 767.0, "samples_count": 24576.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632360, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19172484589580563}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632360, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632361, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 24576, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632407, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.33942294120788574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 32, "samples_count": 1056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632471, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.06464624404907227}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 33, "samples_count": 1088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632518, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04683732986450195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 34, "samples_count": 1120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632564, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046475887298583984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 35, "samples_count": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632613, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04885411262512207}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 36, "samples_count": 1184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632662, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048381805419921875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 37, "samples_count": 1216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632711, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049086570739746094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 38, "samples_count": 1248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632760, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04968762397766113}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 39, "samples_count": 1280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632807, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04706931114196777}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 40, "samples_count": 1312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04709029197692871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 41, "samples_count": 1344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632904, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04913330078125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 42, "samples_count": 1376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051632952, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047969818115234375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 43, "samples_count": 1408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04846668243408203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 44, "samples_count": 1440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633048, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048140764236450195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 45, "samples_count": 1472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633097, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049167633056640625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 46, "samples_count": 1504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633147, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04918932914733887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 47, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633194, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047600746154785156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 48, "samples_count": 1568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633242, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04797005653381348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 49, "samples_count": 1600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633292, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04948139190673828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 50, "samples_count": 1632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633340, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04833793640136719}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 51, "samples_count": 1664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633389, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0489811897277832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 52, "samples_count": 1696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633438, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04862332344055176}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 53, "samples_count": 1728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633486, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04817938804626465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 54, "samples_count": 1760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633534, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04845738410949707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 55, "samples_count": 1792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633583, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04883265495300293}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 56, "samples_count": 1824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633631, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04789996147155762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 57, "samples_count": 1856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633680, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04915189743041992}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 58, "samples_count": 1888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633728, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04790949821472168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 59, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633776, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048171281814575195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 60, "samples_count": 1952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633826, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05027270317077637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 61, "samples_count": 1984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633876, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04965686798095703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 62, "samples_count": 2016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633925, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04885721206665039}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 63, "samples_count": 2048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633927, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 4.58836030960083, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 24576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633927, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5673489639884792}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633927, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 24576, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051633927, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051640043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19156455993652344, "reduced_train_loss": 4.481053352355957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 799.0, "samples_count": 25600.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051646157, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19046854972839355, "reduced_train_loss": 4.4028239250183105}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 831.0, "samples_count": 26624.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051652281, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1917126178741455, "reduced_train_loss": 4.360746383666992}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 863.0, "samples_count": 27648.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051658419, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19263672828674316, "reduced_train_loss": 4.319784641265869}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 895.0, "samples_count": 28672.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051664571, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19366073608398438, "reduced_train_loss": 4.218443393707275}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 927.0, "samples_count": 29696.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051670697, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1915757656097412, "reduced_train_loss": 4.286581516265869}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 959.0, "samples_count": 30720.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051676813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1889486312866211, "reduced_train_loss": 4.236823558807373}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 991.0, "samples_count": 31744.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051682907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18951892852783203, "reduced_train_loss": 4.114451885223389}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1023.0, "samples_count": 32768.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051689024, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19138097763061523, "reduced_train_loss": 4.028246879577637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1055.0, "samples_count": 33792.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051695124, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18998932838439941, "reduced_train_loss": 4.175816535949707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1087.0, "samples_count": 34816.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051701237, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1896076202392578, "reduced_train_loss": 4.094986438751221}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1119.0, "samples_count": 35840.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707357, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19526457786560059, "reduced_train_loss": 4.138094902038574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1151.0, "samples_count": 36864.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707655, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19199910661715572}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707655, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707655, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 36864, "step": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707702, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3451395034790039}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 64, "samples_count": 2080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707757, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.054839372634887695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 65, "samples_count": 2112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707804, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04683184623718262}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 66, "samples_count": 2144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707851, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047879934310913086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 67, "samples_count": 2176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04811811447143555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 68, "samples_count": 2208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707947, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047273874282836914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 69, "samples_count": 2240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051707994, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04753375053405762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 70, "samples_count": 2272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708041, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04715681076049805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 71, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708088, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04671931266784668}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 72, "samples_count": 2336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708138, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05011153221130371}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 73, "samples_count": 2368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708190, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0511927604675293}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 74, "samples_count": 2400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708242, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.052701711654663086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 75, "samples_count": 2432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708291, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04868340492248535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 76, "samples_count": 2464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708339, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04778409004211426}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 77, "samples_count": 2496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708387, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04824686050415039}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 78, "samples_count": 2528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04929471015930176}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 79, "samples_count": 2560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708487, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050618886947631836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 80, "samples_count": 2592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708537, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050368309020996094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 81, "samples_count": 2624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0489199161529541}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 82, "samples_count": 2656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708633, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04714202880859375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 83, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708681, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047785043716430664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 84, "samples_count": 2720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708731, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05015444755554199}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 85, "samples_count": 2752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708778, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04714226722717285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 86, "samples_count": 2784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049211978912353516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 87, "samples_count": 2816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708876, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04849696159362793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 88, "samples_count": 2848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04819917678833008}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 89, "samples_count": 2880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051708973, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04883861541748047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 90, "samples_count": 2912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709021, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04792022705078125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 91, "samples_count": 2944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709071, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05006146430969238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 92, "samples_count": 2976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709122, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05075693130493164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 93, "samples_count": 3008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709170, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04801583290100098}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 94, "samples_count": 3040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709218, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04849100112915039}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 95, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709221, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 4.09398078918457, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 36864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709222, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5667851170292124}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709222, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 36864, "step": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051709222, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 1152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051715346, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19124579429626465, "reduced_train_loss": 4.096192836761475}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1183.0, "samples_count": 37888.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051721459, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1942598819732666, "reduced_train_loss": 3.906984329223633}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1215.0, "samples_count": 38912.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051727564, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18906450271606445, "reduced_train_loss": 4.00026273727417}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1247.0, "samples_count": 39936.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051733674, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1929316520690918, "reduced_train_loss": 4.018397808074951}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1279.0, "samples_count": 40960.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051739791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18883180618286133, "reduced_train_loss": 3.9611802101135254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1311.0, "samples_count": 41984.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051745931, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19468474388122559, "reduced_train_loss": 3.8756163120269775}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1343.0, "samples_count": 43008.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051752073, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1934506893157959, "reduced_train_loss": 3.944240093231201}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1375.0, "samples_count": 44032.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051758224, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19004058837890625, "reduced_train_loss": 3.9425504207611084}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1407.0, "samples_count": 45056.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051764370, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1933274269104004, "reduced_train_loss": 3.895122766494751}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1439.0, "samples_count": 46080.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051770490, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18971872329711914, "reduced_train_loss": 3.9455156326293945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1471.0, "samples_count": 47104.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051776594, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19206523895263672, "reduced_train_loss": 3.87290620803833}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1503.0, "samples_count": 48128.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051782717, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19359827041625977, "reduced_train_loss": 3.952477216720581}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1535.0, "samples_count": 49152.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783013, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19216389913284124}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783013, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783013, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 49152, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783059, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.34218811988830566}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 96, "samples_count": 3104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783126, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.06707119941711426}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 97, "samples_count": 3136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783173, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0467832088470459}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 98, "samples_count": 3168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783222, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04922080039978027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 99, "samples_count": 3200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783269, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04669308662414551}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 100, "samples_count": 3232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783318, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04874467849731445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 101, "samples_count": 3264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783364, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04669451713562012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 102, "samples_count": 3296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783414, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049910783767700195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 103, "samples_count": 3328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783461, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046792030334472656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 104, "samples_count": 3360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783512, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050713300704956055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 105, "samples_count": 3392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783559, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04767560958862305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 106, "samples_count": 3424}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783607, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04733610153198242}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 107, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783656, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04873180389404297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 108, "samples_count": 3488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783703, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04793548583984375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 109, "samples_count": 3520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783753, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04905819892883301}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 110, "samples_count": 3552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783802, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04907536506652832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 111, "samples_count": 3584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783850, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0488893985748291}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 112, "samples_count": 3616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783899, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048960208892822266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 113, "samples_count": 3648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783949, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049411773681640625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 114, "samples_count": 3680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051783996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046987295150756836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 115, "samples_count": 3712}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784044, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04815363883972168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 116, "samples_count": 3744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784094, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04969334602355957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 117, "samples_count": 3776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784142, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04876279830932617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 118, "samples_count": 3808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784190, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04755353927612305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 119, "samples_count": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784241, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05117058753967285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 120, "samples_count": 3872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04684805870056152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 121, "samples_count": 3904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784338, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04961705207824707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 122, "samples_count": 3936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784384, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04668855667114258}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 123, "samples_count": 3968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784433, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04859018325805664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 124, "samples_count": 4000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784483, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050025224685668945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 125, "samples_count": 4032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784533, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04994058609008789}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 126, "samples_count": 4064}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784582, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048921823501586914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 127, "samples_count": 4096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784585, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.8967947959899902, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 49152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784585, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5722163469763473}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784585, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 49152, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051784585, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051790704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19043254852294922, "reduced_train_loss": 3.8327996730804443}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1567.0, "samples_count": 50176.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051796839, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18876194953918457, "reduced_train_loss": 3.827180862426758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1599.0, "samples_count": 51200.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051802958, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19159150123596191, "reduced_train_loss": 3.8387937545776367}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1631.0, "samples_count": 52224.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051809103, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19310665130615234, "reduced_train_loss": 3.8960933685302734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1663.0, "samples_count": 53248.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051815239, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.2003343105316162, "reduced_train_loss": 3.8014161586761475}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1695.0, "samples_count": 54272.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051821384, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19066524505615234, "reduced_train_loss": 3.825993776321411}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1727.0, "samples_count": 55296.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051827487, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19054245948791504, "reduced_train_loss": 3.8203837871551514}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1759.0, "samples_count": 56320.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051833603, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1904594898223877, "reduced_train_loss": 3.818631410598755}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1791.0, "samples_count": 57344.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051839737, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.192718505859375, "reduced_train_loss": 3.7970104217529297}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1823.0, "samples_count": 58368.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051845868, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18958449363708496, "reduced_train_loss": 3.763040542602539}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1855.0, "samples_count": 59392.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051851998, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19588136672973633, "reduced_train_loss": 3.704625129699707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1887.0, "samples_count": 60416.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858135, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19035601615905762, "reduced_train_loss": 3.727191925048828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1919.0, "samples_count": 61440.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858428, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19229898666662848}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858428, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858428, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 61440, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858474, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.339733362197876}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 128, "samples_count": 4128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858533, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.059179067611694336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 129, "samples_count": 4160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858579, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.045694589614868164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 130, "samples_count": 4192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858627, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04781508445739746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 131, "samples_count": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858675, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04830431938171387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 132, "samples_count": 4256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858724, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04887866973876953}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 133, "samples_count": 4288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858771, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047338247299194336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 134, "samples_count": 4320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858819, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04732084274291992}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 135, "samples_count": 4352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858866, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04723381996154785}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 136, "samples_count": 4384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858916, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050614118576049805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 137, "samples_count": 4416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051858967, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05034899711608887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 138, "samples_count": 4448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859015, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048404693603515625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 139, "samples_count": 4480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859063, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0475921630859375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 140, "samples_count": 4512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05028510093688965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 141, "samples_count": 4544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859162, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048717498779296875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 142, "samples_count": 4576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859209, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047478437423706055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 143, "samples_count": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859257, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04738497734069824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 144, "samples_count": 4640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859305, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04854607582092285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 145, "samples_count": 4672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859352, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0469973087310791}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 146, "samples_count": 4704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0519406795501709}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 147, "samples_count": 4736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859452, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0474400520324707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 148, "samples_count": 4768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859499, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047440290451049805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 149, "samples_count": 4800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859547, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04824042320251465}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 150, "samples_count": 4832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859596, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04907512664794922}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 151, "samples_count": 4864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859645, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04904437065124512}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 152, "samples_count": 4896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859693, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04752016067504883}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 153, "samples_count": 4928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859741, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04821324348449707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 154, "samples_count": 4960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859789, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04819774627685547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 155, "samples_count": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859840, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05021786689758301}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 156, "samples_count": 5024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859887, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04747152328491211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 157, "samples_count": 5056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859936, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04941296577453613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 158, "samples_count": 5088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859986, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049323081970214844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 159, "samples_count": 5120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859987, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.762219190597534, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 61440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5597302989917807}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859987, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 61440, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051859987, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051866124, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18993544578552246, "reduced_train_loss": 3.7881581783294678}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1951.0, "samples_count": 62464.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051872256, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1896517276763916, "reduced_train_loss": 3.810117483139038}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 1983.0, "samples_count": 63488.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051878376, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19097566604614258, "reduced_train_loss": 3.7484917640686035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2015.0, "samples_count": 64512.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051884491, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18941307067871094, "reduced_train_loss": 3.7326385974884033}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2047.0, "samples_count": 65536.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051890606, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19243884086608887, "reduced_train_loss": 3.637631416320801}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2079.0, "samples_count": 66560.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051896718, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19044995307922363, "reduced_train_loss": 3.741576671600342}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2111.0, "samples_count": 67584.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051902861, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19499564170837402, "reduced_train_loss": 3.701242446899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2143.0, "samples_count": 68608.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051908987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19204497337341309, "reduced_train_loss": 3.685822010040283}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2175.0, "samples_count": 69632.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051915146, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1893906593322754, "reduced_train_loss": 3.6613197326660156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2207.0, "samples_count": 70656.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051921301, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1925792694091797, "reduced_train_loss": 3.640822410583496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2239.0, "samples_count": 71680.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051927456, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19092440605163574, "reduced_train_loss": 3.684407949447632}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2271.0, "samples_count": 72704.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051933588, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19298720359802246, "reduced_train_loss": 3.626579761505127}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2303.0, "samples_count": 73728.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051933880, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1924277146302605}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051933880, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051933880, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 73728, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051933927, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.33916640281677246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 160, "samples_count": 5152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051933996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.06932711601257324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 161, "samples_count": 5184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.045862436294555664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 162, "samples_count": 5216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934090, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04782891273498535}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 163, "samples_count": 5248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934138, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04876255989074707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 164, "samples_count": 5280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934185, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04694724082946777}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 165, "samples_count": 5312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934233, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04756951332092285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 166, "samples_count": 5344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934281, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047769784927368164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 167, "samples_count": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934329, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0477902889251709}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 168, "samples_count": 5408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934377, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048209190368652344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 169, "samples_count": 5440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934424, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047534942626953125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 170, "samples_count": 5472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934474, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050022125244140625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 171, "samples_count": 5504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934524, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04955267906188965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 172, "samples_count": 5536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934573, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04875445365905762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 173, "samples_count": 5568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934621, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048876285552978516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 174, "samples_count": 5600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934671, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04966449737548828}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 175, "samples_count": 5632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934721, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04960370063781738}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 176, "samples_count": 5664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934769, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048284053802490234}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 177, "samples_count": 5696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934819, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050133466720581055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 178, "samples_count": 5728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934869, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04937553405761719}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 179, "samples_count": 5760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934916, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0476994514465332}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 180, "samples_count": 5792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051934965, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04912114143371582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 181, "samples_count": 5824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935013, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04793548583984375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 182, "samples_count": 5856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935061, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04753732681274414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 183, "samples_count": 5888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935111, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05006241798400879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 184, "samples_count": 5920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935158, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047507286071777344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 185, "samples_count": 5952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935207, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049056291580200195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 186, "samples_count": 5984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935256, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04801750183105469}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 187, "samples_count": 6016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935305, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04936099052429199}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 188, "samples_count": 6048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935356, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05080008506774902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 189, "samples_count": 6080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935402, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046759843826293945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 190, "samples_count": 6112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935451, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04899096488952637}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 191, "samples_count": 6144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935455, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.6610522270202637, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 73728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935455, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5752398240147159}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935455, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 73728, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051935455, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051941589, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19012451171875, "reduced_train_loss": 3.681457996368408}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2335.0, "samples_count": 74752.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051947717, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1908581256866455, "reduced_train_loss": 3.552063465118408}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2367.0, "samples_count": 75776.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051953836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.192549467086792, "reduced_train_loss": 3.5813148021698}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2399.0, "samples_count": 76800.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051959977, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19408774375915527, "reduced_train_loss": 3.6225531101226807}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2431.0, "samples_count": 77824.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051966136, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1908416748046875, "reduced_train_loss": 3.6109375953674316}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2463.0, "samples_count": 78848.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051972278, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1934678554534912, "reduced_train_loss": 3.542436122894287}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2495.0, "samples_count": 79872.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051978432, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1945793628692627, "reduced_train_loss": 3.5817017555236816}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2527.0, "samples_count": 80896.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051984569, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19232940673828125, "reduced_train_loss": 3.68654465675354}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2559.0, "samples_count": 81920.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051990683, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19043350219726562, "reduced_train_loss": 3.5732343196868896}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2591.0, "samples_count": 82944.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760051996798, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1894989013671875, "reduced_train_loss": 3.600154161453247}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2623.0, "samples_count": 83968.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052002913, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1915266513824463, "reduced_train_loss": 3.6030733585357666}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2655.0, "samples_count": 84992.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009028, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19010329246520996, "reduced_train_loss": 3.5662732124328613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2687.0, "samples_count": 86016.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009330, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19238300961448354}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009330, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009330, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 86016, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009377, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.349076509475708}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 192, "samples_count": 6176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009428, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0512845516204834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 193, "samples_count": 6208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009474, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04669046401977539}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 194, "samples_count": 6240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009522, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048096656799316406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 195, "samples_count": 6272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009571, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04827523231506348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 196, "samples_count": 6304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009619, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047879695892333984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 197, "samples_count": 6336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009666, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047293663024902344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 198, "samples_count": 6368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009713, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047071218490600586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 199, "samples_count": 6400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009761, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04784440994262695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 200, "samples_count": 6432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009809, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048149824142456055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 201, "samples_count": 6464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009857, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04804253578186035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 202, "samples_count": 6496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048686981201171875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 203, "samples_count": 6528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052009955, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049027204513549805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 204, "samples_count": 6560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010003, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04870247840881348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 205, "samples_count": 6592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04836106300354004}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 206, "samples_count": 6624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010100, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04807686805725098}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 207, "samples_count": 6656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04807782173156738}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 208, "samples_count": 6688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010198, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05003857612609863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 209, "samples_count": 6720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010248, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04952692985534668}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 210, "samples_count": 6752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010294, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04669666290283203}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 211, "samples_count": 6784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010342, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04804205894470215}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 212, "samples_count": 6816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010391, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048966407775878906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 213, "samples_count": 6848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048055410385131836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 214, "samples_count": 6880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010488, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0487666130065918}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 215, "samples_count": 6912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010536, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047724246978759766}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 216, "samples_count": 6944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010584, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04826641082763672}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 217, "samples_count": 6976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010632, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04753756523132324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 218, "samples_count": 7008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010682, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05003976821899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 219, "samples_count": 7040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010731, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04955005645751953}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 220, "samples_count": 7072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010778, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04702496528625488}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 221, "samples_count": 7104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04934072494506836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 222, "samples_count": 7136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010875, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047242164611816406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 223, "samples_count": 7168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010879, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.582024335861206, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 86016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010879, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.549523706024047}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010879, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 86016, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052010879, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052017033, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19323348999023438, "reduced_train_loss": 3.5423455238342285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2719.0, "samples_count": 87040.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052023169, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19872736930847168, "reduced_train_loss": 3.5414087772369385}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2751.0, "samples_count": 88064.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052029306, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1968545913696289, "reduced_train_loss": 3.53617000579834}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2783.0, "samples_count": 89088.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052035447, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1911909580230713, "reduced_train_loss": 3.598881959915161}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2815.0, "samples_count": 90112.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052041587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19066381454467773, "reduced_train_loss": 3.5414199829101562}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2847.0, "samples_count": 91136.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052047713, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1916966438293457, "reduced_train_loss": 3.4609694480895996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2879.0, "samples_count": 92160.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052053859, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19070053100585938, "reduced_train_loss": 3.5401461124420166}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2911.0, "samples_count": 93184.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052060013, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19358372688293457, "reduced_train_loss": 3.6646087169647217}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2943.0, "samples_count": 94208.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052066165, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19177722930908203, "reduced_train_loss": 3.429098129272461}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 2975.0, "samples_count": 95232.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052072307, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1908252239227295, "reduced_train_loss": 3.5066397190093994}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3007.0, "samples_count": 96256.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052078436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18956875801086426, "reduced_train_loss": 3.5052149295806885}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3039.0, "samples_count": 97280.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052084561, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19025039672851562, "reduced_train_loss": 3.5032618045806885}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3071.0, "samples_count": 98304.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052084860, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19265895786202236}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052084861, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052084861, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 98304, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052084907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.346386194229126}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 224, "samples_count": 7200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052084975, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.06761813163757324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 225, "samples_count": 7232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085022, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047766685485839844}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 226, "samples_count": 7264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085070, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047464609146118164}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 227, "samples_count": 7296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085117, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04727625846862793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 228, "samples_count": 7328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085166, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04842066764831543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 229, "samples_count": 7360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085213, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04738783836364746}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 230, "samples_count": 7392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085260, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04738616943359375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 231, "samples_count": 7424}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085308, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04774284362792969}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 232, "samples_count": 7456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085358, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05033278465270996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 233, "samples_count": 7488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085406, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047746896743774414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 234, "samples_count": 7520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085456, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050278663635253906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 235, "samples_count": 7552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049970388412475586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 236, "samples_count": 7584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085555, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048561811447143555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 237, "samples_count": 7616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0486760139465332}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 238, "samples_count": 7648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085654, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050499677658081055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 239, "samples_count": 7680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04949951171875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 240, "samples_count": 7712}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085750, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04657387733459473}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 241, "samples_count": 7744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085797, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04725480079650879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 242, "samples_count": 7776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085848, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05009865760803223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 243, "samples_count": 7808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085897, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0495913028717041}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 244, "samples_count": 7840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085946, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04867148399353027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 245, "samples_count": 7872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052085994, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0484468936920166}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 246, "samples_count": 7904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086043, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04861807823181152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 247, "samples_count": 7936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0501248836517334}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 248, "samples_count": 7968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086143, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05043148994445801}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 249, "samples_count": 8000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04809117317199707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 250, "samples_count": 8032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04813385009765625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 251, "samples_count": 8064}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086289, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049050092697143555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 252, "samples_count": 8096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086339, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050179481506347656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 253, "samples_count": 8128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086389, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049822092056274414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 254, "samples_count": 8160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050261497497558594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 255, "samples_count": 8192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086442, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.5221056938171387, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 98304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086442, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.581723586015869}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086442, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 98304, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052086442, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052092577, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19657015800476074, "reduced_train_loss": 3.4856202602386475}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3103.0, "samples_count": 99328.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052098705, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1906900405883789, "reduced_train_loss": 3.4567437171936035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3135.0, "samples_count": 100352.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052104840, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18936729431152344, "reduced_train_loss": 3.5041584968566895}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3167.0, "samples_count": 101376.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052110978, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18930387496948242, "reduced_train_loss": 3.4787089824676514}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3199.0, "samples_count": 102400.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052117112, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18765020370483398, "reduced_train_loss": 3.460639238357544}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3231.0, "samples_count": 103424.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052123244, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18976521492004395, "reduced_train_loss": 3.438758611679077}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3263.0, "samples_count": 104448.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052129347, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18882322311401367, "reduced_train_loss": 3.419455051422119}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3295.0, "samples_count": 105472.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052135469, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1892566680908203, "reduced_train_loss": 3.443655252456665}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3327.0, "samples_count": 106496.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052141581, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19232916831970215, "reduced_train_loss": 3.5086452960968018}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3359.0, "samples_count": 107520.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052147707, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19121003150939941, "reduced_train_loss": 3.4867477416992188}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3391.0, "samples_count": 108544.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052153839, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19114208221435547, "reduced_train_loss": 3.4740891456604004}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3423.0, "samples_count": 109568.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052159964, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1929781436920166, "reduced_train_loss": 3.4258103370666504}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3455.0, "samples_count": 110592.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160259, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19223089898423495}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160259, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160259, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 110592, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160305, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3419196605682373}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 256, "samples_count": 8224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160374, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.06871652603149414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 257, "samples_count": 8256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160420, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04615211486816406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 258, "samples_count": 8288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160467, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0468144416809082}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 259, "samples_count": 8320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160517, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04972362518310547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 260, "samples_count": 8352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160565, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04827618598937988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 261, "samples_count": 8384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160613, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04791569709777832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 262, "samples_count": 8416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160661, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04790186882019043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 263, "samples_count": 8448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160708, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04693174362182617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 264, "samples_count": 8480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160756, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04810333251953125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 265, "samples_count": 8512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160804, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048726558685302734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 266, "samples_count": 8544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05019521713256836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 267, "samples_count": 8576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160905, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05032777786254883}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 268, "samples_count": 8608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052160951, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04628109931945801}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 269, "samples_count": 8640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161002, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05097079277038574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 270, "samples_count": 8672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161049, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046971797943115234}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 271, "samples_count": 8704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161104, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.054673194885253906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 272, "samples_count": 8736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161153, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04961276054382324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 273, "samples_count": 8768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04767870903015137}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 274, "samples_count": 8800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161254, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05249285697937012}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 275, "samples_count": 8832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161302, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04884529113769531}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 276, "samples_count": 8864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04646039009094238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 277, "samples_count": 8896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161396, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047101736068725586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 278, "samples_count": 8928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161444, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04825282096862793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 279, "samples_count": 8960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161493, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048407793045043945}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 280, "samples_count": 8992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161541, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04857802391052246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 281, "samples_count": 9024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161591, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04965949058532715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 282, "samples_count": 9056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161638, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047393083572387695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 283, "samples_count": 9088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161688, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049401044845581055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 284, "samples_count": 9120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161736, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04851675033569336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 285, "samples_count": 9152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161785, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04908490180969238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 286, "samples_count": 9184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161834, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0484156608581543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 287, "samples_count": 9216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161835, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.4674720764160156, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 110592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161835, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5761552360490896}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161835, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 110592, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052161835, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052167974, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1902599334716797, "reduced_train_loss": 3.4237921237945557}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3487.0, "samples_count": 111616.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052174086, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1901869773864746, "reduced_train_loss": 3.4689881801605225}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3519.0, "samples_count": 112640.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052180201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18938469886779785, "reduced_train_loss": 3.412323236465454}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3551.0, "samples_count": 113664.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052186328, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1926710605621338, "reduced_train_loss": 3.485935688018799}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3583.0, "samples_count": 114688.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052192456, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1891462802886963, "reduced_train_loss": 3.522657871246338}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3615.0, "samples_count": 115712.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052198604, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18829131126403809, "reduced_train_loss": 3.452331066131592}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3647.0, "samples_count": 116736.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052204744, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19210004806518555, "reduced_train_loss": 3.3659963607788086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3679.0, "samples_count": 117760.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052210874, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19158339500427246, "reduced_train_loss": 3.4259424209594727}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3711.0, "samples_count": 118784.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052216989, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19091224670410156, "reduced_train_loss": 3.4663350582122803}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3743.0, "samples_count": 119808.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052223127, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19916248321533203, "reduced_train_loss": 3.4225940704345703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3775.0, "samples_count": 120832.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052229270, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18971776962280273, "reduced_train_loss": 3.4193739891052246}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3807.0, "samples_count": 121856.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235392, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19457364082336426, "reduced_train_loss": 3.4279379844665527}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3839.0, "samples_count": 122880.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235686, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19231929578897203}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235686, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235686, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 122880, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235732, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3404088020324707}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 288, "samples_count": 9248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235799, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.06725931167602539}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 289, "samples_count": 9280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235846, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0469975471496582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 290, "samples_count": 9312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235894, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04745650291442871}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 291, "samples_count": 9344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235941, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047502994537353516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 292, "samples_count": 9376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052235988, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04648709297180176}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 293, "samples_count": 9408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236036, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048656463623046875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 294, "samples_count": 9440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236083, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0465092658996582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 295, "samples_count": 9472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236132, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049223899841308594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 296, "samples_count": 9504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236181, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04935050010681152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 297, "samples_count": 9536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236232, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05019855499267578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 298, "samples_count": 9568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236280, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04823136329650879}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 299, "samples_count": 9600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236328, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0478823184967041}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 300, "samples_count": 9632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05068635940551758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 301, "samples_count": 9664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236429, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05046272277832031}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 302, "samples_count": 9696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236476, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04758119583129883}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 303, "samples_count": 9728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.051172494888305664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 304, "samples_count": 9760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236575, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047403812408447266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 305, "samples_count": 9792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236624, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04892158508300781}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 306, "samples_count": 9824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236673, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04933500289916992}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 307, "samples_count": 9856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236720, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04650402069091797}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 308, "samples_count": 9888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048431396484375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 309, "samples_count": 9920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236817, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048676490783691406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 310, "samples_count": 9952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236867, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050015926361083984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 311, "samples_count": 9984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236916, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04890584945678711}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 312, "samples_count": 10016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052236966, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049993276596069336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 313, "samples_count": 10048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237016, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04998898506164551}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 314, "samples_count": 10080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237064, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047753334045410156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 315, "samples_count": 10112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04910016059875488}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 316, "samples_count": 10144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237160, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04722905158996582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 317, "samples_count": 10176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237210, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05008530616760254}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 318, "samples_count": 10208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237258, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04852938652038574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 319, "samples_count": 10240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237260, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.4248571395874023, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 122880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237260, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5744854169897735}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237260, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 122880, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052237260, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052243411, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19390463829040527, "reduced_train_loss": 3.357372283935547}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3871.0, "samples_count": 123904.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052249535, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19232773780822754, "reduced_train_loss": 3.4033074378967285}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3903.0, "samples_count": 124928.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052255661, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1918795108795166, "reduced_train_loss": 3.430006265640259}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3935.0, "samples_count": 125952.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052261791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19195556640625, "reduced_train_loss": 3.393465995788574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3967.0, "samples_count": 126976.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052267924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1949145793914795, "reduced_train_loss": 3.465059280395508}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 3999.0, "samples_count": 128000.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052274055, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19468021392822266, "reduced_train_loss": 3.436986207962036}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4031.0, "samples_count": 129024.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052280201, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19086790084838867, "reduced_train_loss": 3.4269466400146484}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4063.0, "samples_count": 130048.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052286345, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19333648681640625, "reduced_train_loss": 3.3977010250091553}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4095.0, "samples_count": 131072.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052292465, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19377756118774414, "reduced_train_loss": 3.442880868911743}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4127.0, "samples_count": 132096.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052298588, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18892455101013184, "reduced_train_loss": 3.4084901809692383}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4159.0, "samples_count": 133120.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052304726, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19050049781799316, "reduced_train_loss": 3.2185721397399902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4191.0, "samples_count": 134144.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052310876, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19077348709106445, "reduced_train_loss": 3.4295711517333984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4223.0, "samples_count": 135168.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311176, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19248857868751656}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311176, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311176, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 135168, "step": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311223, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3469054698944092}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 320, "samples_count": 10272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311293, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.07022595405578613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 321, "samples_count": 10304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311341, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047775983810424805}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 322, "samples_count": 10336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311387, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04619288444519043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 323, "samples_count": 10368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311434, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04688453674316406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 324, "samples_count": 10400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311481, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04767465591430664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 325, "samples_count": 10432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046679019927978516}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 326, "samples_count": 10464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311575, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04727745056152344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 327, "samples_count": 10496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311622, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04695773124694824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 328, "samples_count": 10528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311671, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04882550239562988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 329, "samples_count": 10560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047420501708984375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 330, "samples_count": 10592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311766, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04706406593322754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 331, "samples_count": 10624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311814, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04845547676086426}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 332, "samples_count": 10656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311866, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0515894889831543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 333, "samples_count": 10688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311914, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04804873466491699}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 334, "samples_count": 10720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052311961, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04732370376586914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 335, "samples_count": 10752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312009, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04797768592834473}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 336, "samples_count": 10784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312059, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04977083206176758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 337, "samples_count": 10816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312106, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04766058921813965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 338, "samples_count": 10848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047289133071899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 339, "samples_count": 10880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312208, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05432915687561035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 340, "samples_count": 10912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312255, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04669070243835449}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 341, "samples_count": 10944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312302, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047396183013916016}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 342, "samples_count": 10976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312353, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05092930793762207}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 343, "samples_count": 11008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312403, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04988908767700195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 344, "samples_count": 11040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312450, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04733920097351074}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 345, "samples_count": 11072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312501, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05102062225341797}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 346, "samples_count": 11104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312551, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04972982406616211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 347, "samples_count": 11136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312600, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04909205436706543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 348, "samples_count": 11168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04706978797912598}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 349, "samples_count": 11200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312695, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04733848571777344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 350, "samples_count": 11232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312743, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0484006404876709}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 351, "samples_count": 11264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312747, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3879706859588623, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 135168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312747, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5717388530028984}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312747, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 135168, "step": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052312748, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 4224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052318865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1924734115600586, "reduced_train_loss": 3.3152966499328613}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4255.0, "samples_count": 136192.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052324978, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18873047828674316, "reduced_train_loss": 3.343823194503784}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4287.0, "samples_count": 137216.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052331092, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19117164611816406, "reduced_train_loss": 3.362738847732544}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4319.0, "samples_count": 138240.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052337198, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19142508506774902, "reduced_train_loss": 3.3253026008605957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4351.0, "samples_count": 139264.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052343301, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18885087966918945, "reduced_train_loss": 3.4133877754211426}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4383.0, "samples_count": 140288.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052349411, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19071102142333984, "reduced_train_loss": 3.458125352859497}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4415.0, "samples_count": 141312.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052355521, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19119596481323242, "reduced_train_loss": 3.423029899597168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4447.0, "samples_count": 142336.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052361644, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19381213188171387, "reduced_train_loss": 3.273336410522461}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4479.0, "samples_count": 143360.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052367749, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18867039680480957, "reduced_train_loss": 3.348106622695923}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4511.0, "samples_count": 144384.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052373856, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19172239303588867, "reduced_train_loss": 3.4037468433380127}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4543.0, "samples_count": 145408.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052379968, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19404363632202148, "reduced_train_loss": 3.319075107574463}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4575.0, "samples_count": 146432.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386081, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19270062446594238, "reduced_train_loss": 3.2126803398132324}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4607.0, "samples_count": 147456.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19173959041927446}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386376, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386376, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 147456, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386422, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3411433696746826}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 352, "samples_count": 11296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386492, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0702829360961914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 353, "samples_count": 11328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386539, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04680299758911133}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 354, "samples_count": 11360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04753303527832031}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 355, "samples_count": 11392}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386634, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04766654968261719}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 356, "samples_count": 11424}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386683, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0482478141784668}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 357, "samples_count": 11456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386729, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.045798301696777344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 358, "samples_count": 11488}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386776, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047826528549194336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 359, "samples_count": 11520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386824, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047353506088256836}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 360, "samples_count": 11552}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386871, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04777789115905762}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 361, "samples_count": 11584}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386921, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049938201904296875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 362, "samples_count": 11616}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052386972, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05016326904296875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 363, "samples_count": 11648}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387021, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04974198341369629}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 364, "samples_count": 11680}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387069, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04815363883972168}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 365, "samples_count": 11712}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387120, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050951242446899414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 366, "samples_count": 11744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387171, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05091094970703125}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 367, "samples_count": 11776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387221, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049314260482788086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 368, "samples_count": 11808}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387269, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048483848571777344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 369, "samples_count": 11840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387319, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049683332443237305}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 370, "samples_count": 11872}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387367, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0477292537689209}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 371, "samples_count": 11904}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387414, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04711651802062988}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 372, "samples_count": 11936}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387462, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04834294319152832}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 373, "samples_count": 11968}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387512, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049663543701171875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 374, "samples_count": 12000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387559, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04730844497680664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 375, "samples_count": 12032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387608, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04879260063171387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 376, "samples_count": 12064}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387656, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04794764518737793}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 377, "samples_count": 12096}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387705, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04899024963378906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 378, "samples_count": 12128}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387753, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04868745803833008}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 379, "samples_count": 12160}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387802, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04900097846984863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 380, "samples_count": 12192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387851, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04907822608947754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 381, "samples_count": 12224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048520565032958984}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 382, "samples_count": 12256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387949, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04930520057678223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 383, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387952, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3655242919921875, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 147456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387952, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5767139199888334}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387952, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 147456, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052387952, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 4608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052394072, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1906871795654297, "reduced_train_loss": 3.460649013519287}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4639.0, "samples_count": 148480.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052400182, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1899411678314209, "reduced_train_loss": 3.4143121242523193}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4671.0, "samples_count": 149504.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052406286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18718957901000977, "reduced_train_loss": 3.303523063659668}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4703.0, "samples_count": 150528.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052412398, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19187140464782715, "reduced_train_loss": 3.294527292251587}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4735.0, "samples_count": 151552.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052418532, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18927311897277832, "reduced_train_loss": 3.2944979667663574}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4767.0, "samples_count": 152576.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052424641, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19025874137878418, "reduced_train_loss": 3.3624515533447266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4799.0, "samples_count": 153600.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052430762, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19244837760925293, "reduced_train_loss": 3.377755641937256}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4831.0, "samples_count": 154624.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052436879, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18825912475585938, "reduced_train_loss": 3.360426902770996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4863.0, "samples_count": 155648.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052442981, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19347715377807617, "reduced_train_loss": 3.34854793548584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4895.0, "samples_count": 156672.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052449101, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19111132621765137, "reduced_train_loss": 3.367037057876587}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4927.0, "samples_count": 157696.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052455212, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1893470287322998, "reduced_train_loss": 3.340181827545166}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4959.0, "samples_count": 158720.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461350, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19247817993164062, "reduced_train_loss": 3.2541348934173584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 4991.0, "samples_count": 159744.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461655, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19193394030207855}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461655, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461655, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 159744, "step": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461702, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3516659736633301}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 384, "samples_count": 12320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461753, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05145549774169922}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 385, "samples_count": 12352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461801, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04822707176208496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 386, "samples_count": 12384}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461850, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04821443557739258}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 387, "samples_count": 12416}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461896, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046233415603637695}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 388, "samples_count": 12448}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461943, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04741382598876953}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 389, "samples_count": 12480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052461991, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04766035079956055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 390, "samples_count": 12512}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462040, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048792123794555664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 391, "samples_count": 12544}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462088, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04799342155456543}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 392, "samples_count": 12576}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462136, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04849815368652344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 393, "samples_count": 12608}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462184, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04826498031616211}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 394, "samples_count": 12640}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462233, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04843616485595703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 395, "samples_count": 12672}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462283, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05038809776306152}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 396, "samples_count": 12704}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462333, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0497744083404541}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 397, "samples_count": 12736}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462381, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04817557334899902}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 398, "samples_count": 12768}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462429, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04815530776977539}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 399, "samples_count": 12800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462477, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0480043888092041}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 400, "samples_count": 12832}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462526, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04819631576538086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 401, "samples_count": 12864}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462576, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050701141357421875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 402, "samples_count": 12896}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462624, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047324419021606445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 403, "samples_count": 12928}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462671, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047532081604003906}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 404, "samples_count": 12960}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462722, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05111885070800781}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 405, "samples_count": 12992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462772, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05001330375671387}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 406, "samples_count": 13024}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462822, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04931020736694336}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 407, "samples_count": 13056}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462869, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047629356384277344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 408, "samples_count": 13088}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462920, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05095410346984863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 409, "samples_count": 13120}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052462969, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04880499839782715}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 410, "samples_count": 13152}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463017, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048213958740234375}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 411, "samples_count": 13184}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463064, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046448707580566406}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 412, "samples_count": 13216}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463112, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04881477355957031}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 413, "samples_count": 13248}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463162, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04999136924743652}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 414, "samples_count": 13280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463210, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04786324501037598}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 415, "samples_count": 13312}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463213, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.3328123092651367, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 159744}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463213, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5581886740401387}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463213, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 159744, "step": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052463213, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 4992}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052469329, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19104957580566406, "reduced_train_loss": 3.246968984603882}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5023.0, "samples_count": 160768.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052475441, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1911458969116211, "reduced_train_loss": 3.340287208557129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5055.0, "samples_count": 161792.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052481578, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19106483459472656, "reduced_train_loss": 3.2492527961730957}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5087.0, "samples_count": 162816.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052487719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1941230297088623, "reduced_train_loss": 3.2322640419006348}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5119.0, "samples_count": 163840.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052493832, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18956232070922852, "reduced_train_loss": 3.2596595287323}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5151.0, "samples_count": 164864.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052499940, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.191359281539917, "reduced_train_loss": 3.2605700492858887}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5183.0, "samples_count": 165888.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052506045, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19091057777404785, "reduced_train_loss": 3.4291601181030273}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5215.0, "samples_count": 166912.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052512154, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19040417671203613, "reduced_train_loss": 3.255521535873413}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5247.0, "samples_count": 167936.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052518266, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19112348556518555, "reduced_train_loss": 3.269066572189331}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5279.0, "samples_count": 168960.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052524392, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19215703010559082, "reduced_train_loss": 3.2716989517211914}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5311.0, "samples_count": 169984.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052530530, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1942577362060547, "reduced_train_loss": 3.2961134910583496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5343.0, "samples_count": 171008.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052536646, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19072961807250977, "reduced_train_loss": 3.309663772583008}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5375.0, "samples_count": 172032.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052536940, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19199631498167946}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052536940, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052536940, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 172032, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052536987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3405435085296631}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 416, "samples_count": 13344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537061, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.07503867149353027}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 417, "samples_count": 13376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537109, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04758501052856445}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 418, "samples_count": 13408}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537159, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049571990966796875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 419, "samples_count": 13440}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537205, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046736717224121094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 420, "samples_count": 13472}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537252, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046671390533447266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 421, "samples_count": 13504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537298, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04646039009094238}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 422, "samples_count": 13536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537347, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04908156394958496}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 423, "samples_count": 13568}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537397, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04984855651855469}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 424, "samples_count": 13600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537446, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0488436222076416}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 425, "samples_count": 13632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537496, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04932737350463867}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 426, "samples_count": 13664}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537544, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048107147216796875}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 427, "samples_count": 13696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537593, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04977130889892578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 428, "samples_count": 13728}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537643, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049263954162597656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 429, "samples_count": 13760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537692, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04934358596801758}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 430, "samples_count": 13792}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537741, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04896068572998047}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 431, "samples_count": 13824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537789, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04845070838928223}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 432, "samples_count": 13856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537840, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05011773109436035}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 433, "samples_count": 13888}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537886, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046617746353149414}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 434, "samples_count": 13920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537934, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048354148864746094}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 435, "samples_count": 13952}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052537984, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04921913146972656}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 436, "samples_count": 13984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538031, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04751420021057129}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 437, "samples_count": 14016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538081, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04999113082885742}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 438, "samples_count": 14048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538128, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047239065170288086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 439, "samples_count": 14080}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538178, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04914450645446777}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 440, "samples_count": 14112}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538226, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048354387283325195}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 441, "samples_count": 14144}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538277, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0507352352142334}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 442, "samples_count": 14176}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538325, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04848957061767578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 443, "samples_count": 14208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538372, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046827077865600586}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 444, "samples_count": 14240}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538422, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04992103576660156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 445, "samples_count": 14272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538471, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0489506721496582}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 446, "samples_count": 14304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538520, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04937934875488281}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 447, "samples_count": 14336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538523, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.303412675857544, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 172032}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538523, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5839292660239153}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538523, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 172032, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052538524, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 185, "samples_count": 12288, "step": 5376}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052544646, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1913301944732666, "reduced_train_loss": 3.321638822555542}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5407.0, "samples_count": 173056.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052550781, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18987011909484863, "reduced_train_loss": 3.3281328678131104}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5439.0, "samples_count": 174080.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052556911, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19439339637756348, "reduced_train_loss": 3.358372926712036}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5471.0, "samples_count": 175104.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052563032, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19090747833251953, "reduced_train_loss": 3.3296902179718018}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5503.0, "samples_count": 176128.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052569150, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.1905817985534668, "reduced_train_loss": 3.3116562366485596}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5535.0, "samples_count": 177152.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052575282, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19754362106323242, "reduced_train_loss": 3.2714908123016357}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5567.0, "samples_count": 178176.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052581404, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19292354583740234, "reduced_train_loss": 3.31168532371521}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5599.0, "samples_count": 179200.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052587551, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19168400764465332, "reduced_train_loss": 3.2746059894561768}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5631.0, "samples_count": 180224.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052593677, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19013047218322754, "reduced_train_loss": 3.2617671489715576}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5663.0, "samples_count": 181248.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052599779, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19041943550109863, "reduced_train_loss": 3.2281153202056885}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5695.0, "samples_count": 182272.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052605893, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.18804025650024414, "reduced_train_loss": 3.290989875793457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5727.0, "samples_count": 183296.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612004, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19312310218811035, "reduced_train_loss": 3.317030668258667}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 5759.0, "samples_count": 184320.0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612303, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"train_step_time": 0.19213461039316826}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 210, "samples_count": 12288}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612304, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 194, "samples_count": 12288, "step": 5760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612304, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 131, "samples_count": 184320, "step": 5760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612350, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.3464057445526123}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 448, "samples_count": 14368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612412, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.062158823013305664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 449, "samples_count": 14400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612459, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04693031311035156}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 450, "samples_count": 14432}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612505, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04644346237182617}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 451, "samples_count": 14464}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612554, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04846024513244629}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 452, "samples_count": 14496}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612600, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04588818550109863}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 453, "samples_count": 14528}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04742836952209473}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 454, "samples_count": 14560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612695, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048124074935913086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 455, "samples_count": 14592}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612743, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04769611358642578}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 456, "samples_count": 14624}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048430442810058594}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 457, "samples_count": 14656}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612842, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05092930793762207}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 458, "samples_count": 14688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612893, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.050542354583740234}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 459, "samples_count": 14720}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612942, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04923558235168457}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 460, "samples_count": 14752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052612991, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04918646812438965}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 461, "samples_count": 14784}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613040, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04831409454345703}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 462, "samples_count": 14816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613088, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04840588569641113}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 463, "samples_count": 14848}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613136, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048063039779663086}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 464, "samples_count": 14880}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613184, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048136234283447266}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 465, "samples_count": 14912}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613232, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04816007614135742}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 466, "samples_count": 14944}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613281, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04905104637145996}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 467, "samples_count": 14976}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613330, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04848361015319824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 468, "samples_count": 15008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613378, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0477442741394043}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 469, "samples_count": 15040}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613427, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049416303634643555}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 470, "samples_count": 15072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613478, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.05106353759765625}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 471, "samples_count": 15104}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613525, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.046895503997802734}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 472, "samples_count": 15136}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613573, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.047814130783081055}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 473, "samples_count": 15168}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613622, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.049368858337402344}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 474, "samples_count": 15200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613672, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04964184761047363}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 475, "samples_count": 15232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04724717140197754}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 476, "samples_count": 15264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.048975229263305664}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 477, "samples_count": 15296}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613817, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.0489346981048584}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 478, "samples_count": 15328}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613865, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_step_time": 0.04799532890319824}, "metadata": {"file": "/workspace/llm/custom_callbacks.py", "lineno": 480, "step": 479, "samples_count": 15360}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613868, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 3.28406023979187, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 272, "samples_count": 184320}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613868, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"validation_time": 1.5652077739941888}, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 163, "step": 5760}}
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613868, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 148, "samples_count": 184320, "step": 5760}}
 0: Average train_step_time 0.19116001485122575
 0: :::MLLOG {"namespace": "", "time_ms": 1760052613897, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.12/dist-packages/mlperf_common/callbacks/logging.py", "lineno": 106, "step": 5760, "samples_count": 184320, "status": "success"}}
++ date +%s
+ echo 'RUNANDTIME_STOP 1760052664'
RUNANDTIME_STOP 1760052664
+ set -e
